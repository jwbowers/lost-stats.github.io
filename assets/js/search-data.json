{"0": {
    "doc": "ANOVA",
    "title": "ANOVA",
    "content": "ANOVA, meaning Analysis of Variance, is a statistical test that tells us about the differences between the means of two or more independent groups. ANOVA tests are most often used when looking at interaction effects between categorical independent variables and continuous dependent variables. The null and alternative hypothesis of an ANOVA test are: . \\[H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_n\\] \\[H_1: \\exists i, j : \\mu_i \\neq \\mu_j\\] The null hypothesis (\\(H_0\\)) states that the means of the different levels are all equal, while the alternative hypothesis (\\(H_1\\)) states that at least one mean is different than that of the other variables. Common Variations of ANOVA . | One-way ANOVA . | Used to determine relationship between a single variable and the response variable | Example: Looking at student scores on standardized test for all students in a school grouped by grade | . | Two-way ANOVA . | Used when examining two categorical independent variables and their relationship with the dependent variable | Categorized as a type of factorial ANOVA | . | Factorial (N-way) ANOVA . | Test two or more categorical independent variables and their relationships to the dependent variable | Terms N-way and Factorial are often used synonymous to describe testing that involves multiple independent variables and a single dependent variable | Example: Measuring growth rate of plants based on the manipulation of watering frequency and exposure to sunlight | . | . For more information, see Wikipedia: Analysis of Variance and Statology: One-Way vs. Two-Way ANOVA . ",
    "url": "/Model_Estimation/OLS/ANOVA.html",
    
    "relUrl": "/Model_Estimation/OLS/ANOVA.html"
  },"1": {
    "doc": "ANOVA",
    "title": "Keep in Mind",
    "content": "Assumptions . There are strict assumptions made about data and variables with ANOVA testing methods: . | Normal distribution . | Observations that are taken have been randomly selected from the population are independent from each other | More likely to be a potential concern with data sets and sample sizes | . | Independence of variables . | Observations that are taken have been randomly selected from the population are are independent of each other | . | Homoscedasticity . | Variation around mean is similar for variables being tested | Uses F-statistic to test homogeneity of variance in means | . | Equal Sample Sizes . | If multiple groups are being tested, the number of observations included in the sample must be equal for each group | . | . For more information and examples, see BeST: Checking Assumptions of One-Way ANOVA . Advantages . | Can be superior to other tests (such as z-test) since ANOVA allows us to make comparisons of multiple variables | Reduces Type I error rate . | Very important to consider depending on the context | . | Provides robust statistical inference if assumptions of data are met | . Disadvantages . | Adding additional variables will increase test complexity and difficulty interpreting results | ANOVA test results only provide limited insight into potential interactions, post hoc tests are often required to gain more robust insight into statistically significant ANOVA results | If the test tells us to reject the null hypothesis of equal means, it may not be clear which variable is having the explanatory effect if comparing multiple groups | Must check assumptions prior to using ANOVA to test data . | Data might not have all characteristics meeting the strict assumptions of ANOVA | . | . ",
    "url": "/Model_Estimation/OLS/ANOVA.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/OLS/ANOVA.html#keep-in-mind"
  },"2": {
    "doc": "ANOVA",
    "title": "Also Consider",
    "content": "Different variations of ANOVA testing that can be used to better suit the context of the analysis. A few examples include: . | ANCOVA . | Stands for Analysis of Covariance | Used to test interaction effects between independent categorical variables in addition to relationship with continuous dependent variable | Example: Looking at relationship between hours spent studying and grade level (the independent variables) and standardizes test score (dependent variable), where it is suspected that there may be an interaction between independent variables; time spent studying would be a covariate | . | MANOVA . | Stands for Multivariate Analysis of Variance | Looks at effects of multiple independent categorical variables and multiple continuous dependent variables and potential interacting effects | Can be further classified into one-way and two-way tests | Example: Testing the relationship between level of education (the independent variable) with standardized test scores and annual income (the dependent variables) | . | Within-Subjects ANOVA . | Also referred to as Repeated Measures ANOVA | Can be used to examine differences between two or more time periods | Frequently used when looking at test results for pre-treatment and post-treatment variable interaction | Example: Looking at dependent variable of median income based on independent variable of level of education, measured in multiple different time periods over several years; the Panel Study of Income Dynamics (PSID), more on that found here | . | . For more examples, see Statistic Solutions: The Various Forms of ANOVA . ",
    "url": "/Model_Estimation/OLS/ANOVA.html#also-consider",
    
    "relUrl": "/Model_Estimation/OLS/ANOVA.html#also-consider"
  },"3": {
    "doc": "ANOVA",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/ANOVA.html#implementations",
    
    "relUrl": "/Model_Estimation/OLS/ANOVA.html#implementations"
  },"4": {
    "doc": "ANOVA",
    "title": "R",
    "content": "We will be using the mtcars data set included in the base program . Prior to running test, check underlying assumptions for the data . # Selecting variables of interest cars = mtcars[, c(\"mpg\", # Dependent/response variable \"wt\")] # Independent variable # 1. Creating histograms to check for uniform distribution hist(cars$mpg) hist(cars$wt) # 2. Using Chi Square test to check for variable independence chisq.test(cars$mpg, cars$wt) # 3. Creating model to check for variation around mean mod = lm(formula = mpg~wt, data = cars) plot(mod) . If we can verify these assumptions, then we can be confident that the information obtained from the ANOVA test will be an accurate measurement of the true relationship between the variables. # ANOVA test and summary anova(mod) #&gt; Analysis of Variance Table #&gt; #&gt; Response: mpg #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; wt 1 847.73 847.73 91.375 1.294e-10 *** #&gt; Residuals 30 278.32 9.28 #&gt; --- #&gt; Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 . In this example, we can see that wt is significant on all levels. Therefore, we can reject our null hypothesis that both means being tested are equal and accept our alternative hypothesis. ",
    "url": "/Model_Estimation/OLS/ANOVA.html#r",
    
    "relUrl": "/Model_Estimation/OLS/ANOVA.html#r"
  },"5": {
    "doc": "AR Models",
    "title": "Autoregressive (AR) Models",
    "content": "Autoregressive (AR) models are fundamental to time series analysis. They are estimated via regressing a variable on one or more of its lagged values. That is, AR models take the form: \\(Y_t = c + \\sum_{i = 1}^{p} \\beta_i Y_{t-i} + \\epsilon_t\\) Where we say p is the order of our auto regression. Their estimation in statistical software packages is generally straightforward. For additional information, see Wikipedia: Autoregressive model. ",
    "url": "/Time_Series/AR-models.html#autoregressive-ar-models",
    
    "relUrl": "/Time_Series/AR-models.html#autoregressive-ar-models"
  },"6": {
    "doc": "AR Models",
    "title": "Keep In Mind",
    "content": ". | An AR model can be univariate (scalar) or multivariate (vector). This may be important to implementing an AR model in your statisical package of choice. | Data should be properly formatted before estimation. If not, non-time series objects (e.g., a date column) may be interpereted by software as a time series variable, leading to erroneous output. | . ",
    "url": "/Time_Series/AR-models.html#keep-in-mind",
    
    "relUrl": "/Time_Series/AR-models.html#keep-in-mind"
  },"7": {
    "doc": "AR Models",
    "title": "Implementations",
    "content": "Following the instructions for creating and formatting Time Series Data, we will use quaterly GDP data downloaded from FRED as an example. ",
    "url": "/Time_Series/AR-models.html#implementations",
    
    "relUrl": "/Time_Series/AR-models.html#implementations"
  },"8": {
    "doc": "AR Models",
    "title": "Julia",
    "content": "AR(p) models in Julia can be estimated using the StateSpaceModels.jl package, which also allows for the estimation of a variety of time series models that have linear state-space representations. Begin by importing and loading necessary packages into your work environment. # Load necessary packages using StateSpaceModels, CSV, Dates, DataFrames, LinearAlgebra . You can then download the GDPC1.csv dataset using the CSV.jl package, and store it as a DataFrame object. # Import (download) data data = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\"), DataFrame) . The data can then be assigned a general ARIMA(p,d,q) representation, where if d and q are set to zero, the model specification becomes an AR(p). The d=q= 0 constraint can be applied by inputting order = (p,0,0), where p&gt;0. # Specify GDPC1 series as an AR(2) model model = SARIMA(data.GDPC1, order = (2,0,0)) . Lastly, the above-specified model can be estimated using the fit! function, and the estimation results printed using the results function. The sole input for both of these functions is the model object that contains the chosen data series and its assigned ARIMA structure. # Fit (estimate) the model fit!(model) # Print estimates results(model) . ",
    "url": "/Time_Series/AR-models.html#julia",
    
    "relUrl": "/Time_Series/AR-models.html#julia"
  },"9": {
    "doc": "AR Models",
    "title": "Python",
    "content": "In Python, the statsmodels package provides a range of tools to fit models using maximum likelihood estimation. In the example below, we will use the AutoReg function. This can fit models of the form: . \\[y_t = \\delta_0 + \\delta_1 t + \\phi_1 y_{t-1} + \\ldots + \\phi_p y_{t-p} + \\sum_{i=1}^{s-1} \\gamma_i d_i + \\sum_{j=1}^{m} \\kappa_j x_{t,j} + \\epsilon_t.\\] where \\(d_i\\) are seasonal dummies, \\(x_{t,j}\\) are exogenous regressors, and the \\(\\phi_p\\) are the coefficients of the auto-regressive components of the model. Using GDP data, let’s fit an auto-regressive model of order 1, an AR(1), with AutoReg: . # Install pandas and statsmodels using 'pip install' or 'conda install' on the command line import pandas as pd from statsmodels.tsa.ar_model import AutoReg, ar_select_order gdp = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", index_col=0) ar1_model = AutoReg(gdp, 1) results = ar1_model.fit() print(results.summary()) . ## AutoReg Model Results ## ============================================================================== ## Dep. Variable: GDPC1 No. Observations: 292 ## Model: AutoReg(1) Log Likelihood -1625.980 ## Method: Conditional MLE S.D. of innovations 64.626 ## Date: 0000000000000000 AIC 8.358 ## Time: 00:00:00 BIC 8.396 ## Sample: 04-01-1947 HQIC 8.373 ## - 10-01-2019 ## ============================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------ ## intercept 21.8083 7.439 2.931 0.003 7.227 36.389 ## GDPC1.L1 1.0043 0.001 1356.492 0.000 1.003 1.006 ## Roots ## ============================================================================= ## Real Imaginary Modulus Frequency ## ----------------------------------------------------------------------------- ## AR.1 0.9957 +0.0000j 0.9957 0.0000 ## ----------------------------------------------------------------------------- . Now let’s use the automatic option to choose how many lags to include (this uses the BIC criterion to choose, though over criteria are available): . select_model = ar_select_order(gdp, maxlag=10) print(select_model.ar_lags) . ## [1 2 3] . This tells us to include lags up to 3. We can pass the list of lags right back to the Auto_Reg function: . arp_model = AutoReg(gdp, select_model.ar_lags) results_p = arp_model.fit() print(results_p.summary()) . ## AutoReg Model Results ## ============================================================================== ## Dep. Variable: GDPC1 No. Observations: 292 ## Model: AutoReg(3) Log Likelihood -1593.285 ## Method: Conditional MLE S.D. of innovations 59.989 ## Date: 0000000000000000 AIC 8.223 ## Time: 00:00:00 BIC 8.286 ## Sample: 10-01-1947 HQIC 8.248 ## - 10-01-2019 ## ============================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------ ## intercept 13.3707 7.098 1.884 0.060 -0.541 27.282 ## GDPC1.L1 1.2921 0.058 22.273 0.000 1.178 1.406 ## GDPC1.L2 -0.1253 0.095 -1.315 0.189 -0.312 0.062 ## GDPC1.L3 -0.1646 0.058 -2.826 0.005 -0.279 -0.050 ## Roots ## ============================================================================= ## Real Imaginary Modulus Frequency ## ----------------------------------------------------------------------------- ## AR.1 0.9960 +0.0000j 0.9960 0.0000 ## AR.2 1.7430 +0.0000j 1.7430 0.0000 ## AR.3 -3.5005 +0.0000j 3.5005 0.5000 ## ----------------------------------------------------------------------------- . ",
    "url": "/Time_Series/AR-models.html#python",
    
    "relUrl": "/Time_Series/AR-models.html#python"
  },"10": {
    "doc": "AR Models",
    "title": "R",
    "content": "#load data gdp = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") #estimation via ols: pay attention to the selection of the 'GDPC1' column. #if the column is not specified, the function call also interprets the date column as a time series variable! ar_gdp = ar.ols(gdp$GDPC1) ar_gdp #lag order is automatically selected by minimizing AIC #disable this feature with the optional command 'aic = F'. Note: you will also likely wish to specify the argument 'order.max'. #ar.ols() defaults to demeaning the data automatically. Also consider taking logs and first differencing for statistically meaningful results. ",
    "url": "/Time_Series/AR-models.html#r",
    
    "relUrl": "/Time_Series/AR-models.html#r"
  },"11": {
    "doc": "AR Models",
    "title": "STATA",
    "content": "*load data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", clear *Generate the new date variable *To generalize to a different set of data, replace '1947q1' with your own series' start date. generate date_index = tq(1947q1) + _n-1 *Index the new variable format as quarter format date_index %tq *Convert a variable into time-series data tsset date_index *Specifiy and Run AR regression: this STATA method will not automatically select a lag order. *The 'L.' operator indicates the lagged value of a variable in STATA, 'L2.' its second lag, and so on. reg gdpc1 L.gdpc1 L2.gdpc1 *variables are not demeaned automatically by STATA. Also consider taking logs and first differencing for statistically meaningful results. ",
    "url": "/Time_Series/AR-models.html#stata",
    
    "relUrl": "/Time_Series/AR-models.html#stata"
  },"12": {
    "doc": "AR Models",
    "title": "AR Models",
    "content": " ",
    "url": "/Time_Series/AR-models.html",
    
    "relUrl": "/Time_Series/AR-models.html"
  },"13": {
    "doc": "ARCH Model",
    "title": "Autoregressive Conditional Heteroscedasticity (ARCH) model",
    "content": "The autoregressive conditional heteroscedasticity (ARCH) model is a statistical model for time series data that models the variance of the current error as a function of the actual sizes of the previous time periods’ errors. The ARCH model is appropriate when the error variance in a time series follows an autoregressive (AR) model. An ARCH(q) process can be written as \\(y_{t} = a_{0} + \\sum_{i=1}^{q}y_{t-q}+\\epsilon_{t}\\) where \\(\\epsilon_{t}\\) denote the error terms. These \\(\\epsilon_{t}\\) are split into a stochastic piece \\(z_{t}\\) and a time-dependent standard deviation \\(\\sigma_{t}\\) characterizing the typical size of the terms so that \\(\\epsilon_{t}=\\sigma_{t}z_{t}\\). The random variable \\(z_{t}\\) is a strong white noise process. The series \\(\\sigma_{t}^{2}\\) is modeled by . \\[\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\dots + \\alpha_{q}\\epsilon_{t-1}^{2} = \\alpha_{0} + \\sum_{i=1}^{q}\\alpha_{i}\\epsilon_{t-i}^{2} \\, ,\\] where \\(\\alpha_{0} &gt; 0\\) and \\(\\alpha_{i} \\geq 0\\), \\(i &gt; 0\\). An ARCH(q) model can be estimated using ordinary least squares. This procedure is as follows: . | Estimate the best fitting autoregressive model AR(q) \\(y_{t}=a_{0}+a_{1}y_{t-1}+\\dots +a_{q}y_{t-q}+\\epsilon_{t} =a_{0}+\\sum_{i=1}^{q}a_{i}y_{t-i}+\\epsilon_{t}\\). | Obtain the squares of the error \\(\\hat{\\epsilon}^{2}\\) and regress them on a constant and q lagged values: \\(\\hat{\\epsilon}^{2}=\\hat {\\alpha}_{0}+\\sum_{i=1}^{q}\\hat{\\alpha}_{i}\\hat{\\epsilon}_{t-i}^{2}\\) where q is the length of ARCH lags. | Null Hypothesis: \\(\\alpha_{i}=0\\) for all \\(i=1,\\dots ,q\\). Alternative hypothesis: At least one of the estimated \\(\\alpha_{i}\\) coefficients must be significant. Under the null hypothesis of no ARCH errors, the test statistic \\(T'R²\\) follows \\(\\chi^{2}\\) distribution with q degrees of freedom, where \\(T'\\) is the number of equations in the model which fits the residuals vs the lags (i.e. \\(T'=T-q\\)). If \\(T'R²\\) is greater than the Chi-square table value, we reject the null hypothesis and conclude there is an ARCH effect, otherwise we do not reject the null hypothesis. | . For additional information, see Wikipedia: Autoregressive conditional heteroskedasticity. ",
    "url": "/Time_Series/ARCH_Model.html#autoregressive-conditional-heteroscedasticity-arch-model",
    
    "relUrl": "/Time_Series/ARCH_Model.html#autoregressive-conditional-heteroscedasticity-arch-model"
  },"14": {
    "doc": "ARCH Model",
    "title": "Keep in Mind",
    "content": ". | Data should be properly formatted for estimation as a time-series. See creating a time series data set. If not, you may fail to execute or receive erroneous output. | ARCH can be used to model time-varying conditional variance. | . ",
    "url": "/Time_Series/ARCH_Model.html#keep-in-mind",
    
    "relUrl": "/Time_Series/ARCH_Model.html#keep-in-mind"
  },"15": {
    "doc": "ARCH Model",
    "title": "Also Consider",
    "content": ". | ARCH models can be univariate (scalar) or multivariate (vector). | ARCH models are commonly employed in modeling financial time series that exhibit time-varying volatility and volatility clustering, i.e. periods of swings interspersed with periods of relative calm. | If an autoregressive moving average (ARMA) model is assumed for the error variance, the model is a generalized autoregressive conditional heteroskedasticity (GARCH) model. For more information on GARCH models, see Wikipedia: GARCH. For information about estimating an GARCH models, see LOST: GARCH models. | . ",
    "url": "/Time_Series/ARCH_Model.html#also-consider",
    
    "relUrl": "/Time_Series/ARCH_Model.html#also-consider"
  },"16": {
    "doc": "ARCH Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/ARCH_Model.html#implementations",
    
    "relUrl": "/Time_Series/ARCH_Model.html#implementations"
  },"17": {
    "doc": "ARCH Model",
    "title": "Julia",
    "content": "The ARCHModels.jl package offers a variety of ARCH-model simulation, estimation, and forecasting tools. We start by loading the package. # Load necessary packages using ARCHModels . Next, we use the simulate function to specify an ARCH{1} model with coefficient parameters a0 and a1, and then simulate a realization of the specified data-generating process with 1000 observations. # Simulate an ARCH(1) process a0 = 1.0 a1 = 0.5 arch1sim = simulate(ARCH{1}([a0, a1]), 1000) . Lastly, we use the fit function to fit an ARCH{1} model to the generated series contained in the data attribute of the UnivariateARCHModel object we named arch1sim in the above code chunk. # Fit ARCH(1) model to simulated data fit(ARCH{1}, arch1sim.data) . ",
    "url": "/Time_Series/ARCH_Model.html#julia",
    
    "relUrl": "/Time_Series/ARCH_Model.html#julia"
  },"18": {
    "doc": "ARCH Model",
    "title": "Python",
    "content": "from random import gauss from random import seed from matplotlib import pyplot from arch import arch_model import numpy as np # seed the process np.random.seed(1) # Simulating a ARCH(1) process a0 = 1 a1 = .5 w = np.random.normal(size=1000) e = np.random.normal(size=1000) Y = np.empty_like(w) for t in range(1, len(w)): Y[t] = w[t] * np.sqrt((a0 + a1*Y[t-1]**2)) # fit model model = arch_model(Y, vol = \"ARCH\", rescale = \"FALSE\") model_fit = model.fit() print(model_fit.summary) . ",
    "url": "/Time_Series/ARCH_Model.html#python",
    
    "relUrl": "/Time_Series/ARCH_Model.html#python"
  },"19": {
    "doc": "ARCH Model",
    "title": "R",
    "content": "# setup library(fGarch) # seed pseudorandom number generator set.seed(1) # Simulating a ARCH(1) process e &lt;- NULL obs &lt;- 1000 e[1] &lt;- rnorm(1) for (i in 2:obs) {e[i] &lt;- rnorm(1)*(1+0.5*(e[i-1])^2)^0.5} # fit the model arch.fit &lt;- garchFit(~garch(1,0), data = e, trace = F) summary(arch.fit) . ",
    "url": "/Time_Series/ARCH_Model.html#r",
    
    "relUrl": "/Time_Series/ARCH_Model.html#r"
  },"20": {
    "doc": "ARCH Model",
    "title": "Stata",
    "content": "* seed pseudorandom number generator set seed 1 * Simulating a ARCH(1) process set obs 1000 gen time=_n tsset time gen e=. replace e=rnormal() if time==1 replace e=rnormal()*(1 + .5*(e[_n-1])^2)^.5 if time&gt;=2 &amp; time&lt;=2000 * Estimate arch parameters.. arch e, arch(1) . ",
    "url": "/Time_Series/ARCH_Model.html#stata",
    
    "relUrl": "/Time_Series/ARCH_Model.html#stata"
  },"21": {
    "doc": "ARCH Model",
    "title": "ARCH Model",
    "content": " ",
    "url": "/Time_Series/ARCH_Model.html",
    
    "relUrl": "/Time_Series/ARCH_Model.html"
  },"22": {
    "doc": "ARIMA Models",
    "title": "ARIMA Models",
    "content": " ",
    "url": "/Time_Series/ARIMA-models.html",
    
    "relUrl": "/Time_Series/ARIMA-models.html"
  },"23": {
    "doc": "ARIMA Models",
    "title": "Introduction",
    "content": "ARIMA, which stands for Autoregressive Integrated Moving-Average, is a time series model specification which combines typical Autoregressive (AR) and Moving Average (MA), while also allowing for unit roots. An ARIMA thus has three parameters: \\(p\\), which denotes the AR parameters, \\(q\\), which denotes the MA parameters, and \\(d\\), which represents the number of times an ARIMA model must be differenced in order to get an ARMA model. A univariate \\(ARIMA(p, 1, q)\\) model can be specified by . \\[y_{t}=\\alpha + \\delta t +u_{t}\\] where \\(u_{t}\\) is an \\(ARMA(p+1,q)\\). Particularly, . \\[\\rho(L)u_{t}=\\theta(L)\\varepsilon_{t}\\] where \\(\\varepsilon_{t}\\sim WN(0,\\sigma^{2})\\) and . \\[\\begin{align} \\rho(L)&amp;=(1-\\rho_{1}L-\\dots-\\rho_{p+1}L^{p+1})\\\\ \\theta(L)&amp;=1+\\theta_{1}L+\\dots+\\theta_{q}L^{q} \\end{align}\\] Recall that \\(L\\) is the lag operator and \\(\\theta(L)\\) must be invertible. If we factor \\(\\rho(L)=(1-\\lambda_{1}L)\\cdots(1-\\lambda_{p+1}L)\\), where \\(\\{\\lambda\\}\\) are the eigenvalues of the \\(F\\) matrix (see LOST: State-Space Models), then define\\(\\phi(L)=(1-\\lambda_{1}L)\\cdots(1-\\lambda_{p}L)\\). It follows that . \\[\\begin{align*} \\phi(L)(1-L)u_{t}&amp;=\\theta(L)\\varepsilon_{t} \\implies \\phi(L)\\Delta u_{t}&amp;=\\theta(L)\\varepsilon_{t} \\end{align*}\\] Since \\(\\Delta u_{t}\\) is now a stationary \\(ARMA(p,q)\\), it has a Wold form \\(\\Delta u_{t}=\\phi^{-1}(L)\\theta(L)\\varepsilon_{t}\\), and so we can write In the general case of an \\(ARIMA(p,d,q)\\), a unit root of multiplicity \\(d\\) leads to . \\[\\phi(L)(1-L)^{d}y_{t}=\\theta(L)\\varepsilon_{t}\\] which leads to \\(\\Delta^{d} y_{t}\\) being an \\(ARMA(p,q)\\) process. ",
    "url": "/Time_Series/ARIMA-models.html#introduction",
    
    "relUrl": "/Time_Series/ARIMA-models.html#introduction"
  },"24": {
    "doc": "ARIMA Models",
    "title": "Keep in Mind",
    "content": ". | Error terms are generally assumed to be from a white noise process with 0 mean and constant variance | A non-zero intercept or mean in \\(\\Delta y_{t}\\) is reffered to as drift, and can be speciied in functions below | If your model has no unit roots, it may be best to consider an ARMA, AR, or MA model | You can always test the presence of a unit root afer fitting your model using a unit root test, such as the Augmented Dickey-Fuller test | . ",
    "url": "/Time_Series/ARIMA-models.html#keep-in-mind",
    
    "relUrl": "/Time_Series/ARIMA-models.html#keep-in-mind"
  },"25": {
    "doc": "ARIMA Models",
    "title": "Also Consider",
    "content": ". | AR Models (LOST: AR models) | MA Models (LOST: MA models) | ARMA Models (LOST: ARMA models) | Seasonal ARIMA models, if you suspect the time series data you are trying to fit with is subject to seasonality | If you are working with State-Space models, you may be interested in trend-cycle decomposition with ARIMA. This involves breaking down the ARIMA into a “trend” component, which encapsulates permanent effects (stochastic and deterministic), and a “cyclical” effect, which encapsulates transitory, non-permanent variation in the model. One extension of this is the Unobserved Components ARIMA, or UC-ARIMA | . ",
    "url": "/Time_Series/ARIMA-models.html#also-consider",
    
    "relUrl": "/Time_Series/ARIMA-models.html#also-consider"
  },"26": {
    "doc": "ARIMA Models",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/ARIMA-models.html#implementations",
    
    "relUrl": "/Time_Series/ARIMA-models.html#implementations"
  },"27": {
    "doc": "ARIMA Models",
    "title": "Julia",
    "content": "ARIMA(p,d,q) models in Julia can be estimated using the StateSpaceModels.jl package, which also allows for the estimation of a variety of time series models that have linear state-space representations. Begin by importing and loading necessary packages into your work environment. # Load necessary packages using StateSpaceModels, CSV, Dates, DataFrames, LinearAlgebra . You can then download the GDPC1.csv dataset using the CSV.jl package, and store it as a DataFrame object. # Import (download) data data = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\"), DataFrame) . The data can then be assigned a general ARIMA(p,d,q) representation. # Specify GDPC1 series as an ARIMA(2,2,2) model model = SARIMA(data.GDPC1, order = (2,2,2)) . Lastly, the above-specified model can be estimated using the fit! function, and the estimation results printed using the results function. The sole input for both of these functions is the model object that contains the chosen data series and its assigned ARIMA structure. # Fit (estimate) the model fit!(model) # Print estimates results(model) . ",
    "url": "/Time_Series/ARIMA-models.html#julia",
    
    "relUrl": "/Time_Series/ARIMA-models.html#julia"
  },"28": {
    "doc": "ARIMA Models",
    "title": "R",
    "content": "The stats package, which comes standard-loaded on an RStudio workspace, includes the function arima, which allows one to estimate an arima model, if they know \\(p,d,\\) and \\(q\\) already. #load data gdp = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") gdp_ts = ts(gdp[ ,2], frequency = 4, start = c(1947, 01), end = c(2019, 04)) y = log(gdp_ts)*100 . The output for arima() is a list. Use $coef to get only the AR and MA estimates. Use $model to get the entire estimated model. If you want to see the maximized log-likelihood value, \\(sigma^{2}\\), and AIC, simply run the function on the data: . #estimate an ARIMA(2,1,2) model lgdp_arima = arima(y, c(2,1,2)) #To see maximized log-likelihood value, $sigma^{2}$, and AIC: lgdp_arima #To get only the AR and MA parameter estimates: lgdp_arima$coef #To see the estimated model: lgdp_arima$model . In the example above, we are using an d = 1 ARIMA model, or a ARIMA model with one unit root. As noted above, if we want to manually test a time series for one or more unit roots, we run an Augmented Dicky-Fuller test through the tseries::adf.test() function. Note that the null hypothesis for a Augmented Dicky-Fuller tests is that there is a unit root. # If necessary # install.packages(\"tseries\") # perform an augmented Dicky Fuller test adf.test(y) . In this case, we fail to reject the null hypothesis, meaning that there is at least one unit root in the data. To test for a second unit root, we can run an Augmented Dicky-Fuller test on the first difference of the time series. # ADF test on first difference of the data adf.test(y |&gt; diff()) . We can reject the null hypothesis that there is a unit root in the first difference of the data at the 1% level. This implies that there is only one unit root in the series, and a d = 1 model is best suited for this data. The forecast package includes the ability to auto-select ARIMA models. This is of particular use when one would like to automate the selection of \\(p,q\\), and \\(d\\), without writing their own function. According to David Childers, forecast::auto.arima() takes the following steps: - Use the KPSS to test for unit roots, differencing the series unit stationary - Create likelihood functions at various orders of \\(p,q\\) - Use AIC to choose \\(p,q\\), then estimate via Maxmium Likelihood to select \\(p,q\\) . library(forecast) #Finding optimal parameters for an ARIMA using the previous data lgdp_auto = auto.arima(y) #A seasonal model was selected, with non-seasonal components (p,d,q)=(1,2,1), and seasonal components (P,D,Q)=(2,0,1) . After specifying an ARIMA model, it is often prudent to check the model’s residuals for time dependency. Ideally, there should be no autocorrelation in the residuals. The forecast::checkresiduals() function runs a Ljung-Box test which checks for autocorrelation in a time series (in this case the residuals from our ARIMA model). It also generates a visualization of the residuals . # check the residuals of the autogenerated ARIMA model checkresiduals(lgdp_auto) . The Ljung-Box test and the plot show that there is autocorrelation in the residuals. We can reject the null-hypothesis that there is no autocorrelation at the 1% level. This illustrates that, while auto.arima() is efficient, it is always a good idea to review the model it selects. auto.arima() contains a lot of flexibility. If one knows the value of \\(d\\), it can be passed to the function. Maximum and starting values for \\(p,q,\\) and \\(d\\) can be specified in the seasonal- and non-seasonal cases. If one would like to restrict themselves to a non-seasonal model, or use a different test, these can also be done. Some of these features are demonstrated below. The method for testing unit roots can also be specified. See ?auto.arima or the package documentation for more. # Auto-estimate y, specifying: ## non-seasonal ## Using Augmented Dickey-Fuller rather than KPSS ## d=1 ## p starts at 1 and does not exceed 4 # no drift lgdp_ns &lt;- auto.arima(y, seasonal = F, test = \"adf\", start.p = 1, max.p = 4, allowdrift = F) #An ARIMA(3,1,0) was specified lgdp_ns . The forecast package also contains the ability to simulate ARIMA data given an ARIMA model. Note that the input here should come from either forecast::auto.arima() or forecast::Arima(), rather than stats::arima(). #Simulate data using a non-seasonal ARIMA() arima_222 &lt;- Arima(y, c(2,2,2)) sim_arima &lt;- forecast:::simulate.Arima(arima_222) tail(sim_arima, 20) . ",
    "url": "/Time_Series/ARIMA-models.html#r",
    
    "relUrl": "/Time_Series/ARIMA-models.html#r"
  },"29": {
    "doc": "ARMA Models",
    "title": "Autoregressive Moving-Average (ARMA) Models",
    "content": "Auto regressive moving average (ARMA) models are a combination of two commonly used time series processes, the autoregressive (AR) process and the moving-average (MA) process. As such, ARMA models have the form . \\[Y_t = c + \\sum_{i = 1}^{p} \\beta_i Y_{t-i} + \\sum_{j = 1}^{q} \\theta_j \\varepsilon_{t-j} + \\varepsilon_t\\] If an ARMA model has an AR component of order \\(p\\) and an MA component of order \\(q\\), then the model is commonly refered to as an \\(ARMA(p,q)\\) . For additional information, see Wikipedia: Autoregressive Moving-Average model. ",
    "url": "/Time_Series/ARMA-models.html#autoregressive-moving-average-arma-models",
    
    "relUrl": "/Time_Series/ARMA-models.html#autoregressive-moving-average-arma-models"
  },"30": {
    "doc": "ARMA Models",
    "title": "Keep In Mind",
    "content": ". | Data must be properly formatted for estimation as a time-series. See creating a time series data set. If this is not done, then depending on your statistical package of choice, either your estimation will fail to execute or you will receive erroneous output. | ARMA models include some number of lagged error terms from the MA component, which are inherently unobservable. Consequently these models cannot be estimated using OLS alone, unlike AR models. | ARMA models are most commonly estimated using maximum likelihood estimation (MLE). One consequence of this is that, given some time series and some specified order \\((p,q)\\), the estimates obtained from the estimated \\(ARMA(p,q)\\) model will vary depending on the type of MLE estimation used. | As is the case in many situations where one is trying to estimate a time-series process, model selection is important. For ARMA models, model selection meaning chosing the number of AR and MA parameters, the \\(p\\) and \\(q\\), for which a coefficient will be estimated. In practice, it is common to estimate several different potential models, then use some criterion to determine which model best fits the time-series. Common criteria used to evaluate ARMA models are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), also referred to as the Schwarz Information Criterion (SIC). For more information on these and other model selection criteria, see Wikipedia: Model Selection. | Estimating a time series using an ARMA model relies on two assumptions. The first is the standard assumption that we have selected the correct functional form for the time series. In this case, that means assuming that we have selected the correct \\(p\\) and \\(q\\). Second, we also have to assume that our time series is stationary. For a discussion of the stationarity assumption and what constraints this assumption imposes on our model, again see Wikipedia: Autoregressive Moving-Average model. | . ",
    "url": "/Time_Series/ARMA-models.html#keep-in-mind",
    
    "relUrl": "/Time_Series/ARMA-models.html#keep-in-mind"
  },"31": {
    "doc": "ARMA Models",
    "title": "Also Consider",
    "content": ". | ARMA models can only be estimated for univariate time series. If you are interested in estimating a time series process using multiple time series on the right hand side of your model, consider using a vector AR (VAR) model or a VARMA model. | Before estimating an ARMA model, it is standard practice to try to determine whether or not the time series appears to be stationary. See LOST: Stationarity and Weak Dependence for more details. | If the time series you are trying to estimate does not appear to be stationary, then using an ARMA model to estimate the series is innappropriate. For simpler forms of nonstationarity, an ARIMA model may be useful. An \\(ARIMA(p,d,q)\\) model is a more general model for a time-series than an \\(ARMA(p,q)\\). In these models, \\(p\\) still signifies an \\(AR(p)\\) component, and \\(q\\) an \\(MA(q)\\) component. For more information on ARIMA models, see Wikipedia: ARIMA. For information about estimating an ARIMA model, see LOST: ARIMA models | . ",
    "url": "/Time_Series/ARMA-models.html#also-consider",
    
    "relUrl": "/Time_Series/ARMA-models.html#also-consider"
  },"32": {
    "doc": "ARMA Models",
    "title": "Implementations",
    "content": "First, follow the instructions for creating and formatting time-series data using your software of choice. We will again use quarterly US GDP data downloaded from FRED as an example. This time, though, we will try to estimate the quarterly log change in GDP with an \\(ARMA(3,1)\\) process. Note that an \\(ARMA(3,1)\\) model is almost certainly not the best way to estimate this time series, and is used here solely as an example. ",
    "url": "/Time_Series/ARMA-models.html#implementations",
    
    "relUrl": "/Time_Series/ARMA-models.html#implementations"
  },"33": {
    "doc": "ARMA Models",
    "title": "Julia",
    "content": "ARMA(p,q) models in Julia can be estimated using the StateSpaceModels.jl package, which also allows for the estimation of a variety of time series models that have linear state-space representations. Begin by importing and loading necessary packages into your work environment. # Load necessary packages using StateSpaceModels, CSV, Dates, DataFrames, LinearAlgebra . You can then download the GDPC1.csv dataset using the CSV.jl package, and store it as a DataFrame object. # Import (download) data data = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\"), DataFrame) . The data can then be assigned a general ARIMA(p,d,q) representation, where if d is set to zero, the model specification becomes an ARMA(p,q). The d= 0 constraint can be applied by inputting order = (p,0,q), where p&gt;0 and q&gt;0. # Specify GDPC1 series as an ARMA(3,1) model model = SARIMA(data.GDPC1, order = (3,0,1)) . Lastly, the above-specified model can be estimated using the fit! function, and the estimation results printed using the results function. The sole input for both of these functions is the model object that contains the chosen data series and its assigned ARIMA structure. # Fit (estimate) the model fit!(model) # Print estimates results(model) . ",
    "url": "/Time_Series/ARMA-models.html#julia",
    
    "relUrl": "/Time_Series/ARMA-models.html#julia"
  },"34": {
    "doc": "ARMA Models",
    "title": "Python",
    "content": "The statsmodels library offers a way to fit ARIMA(p, d, q) models, with its ARIMA function. To get an ARMA model, just set \\(d\\) to zero. In the example below, we’ll take the first difference of the log of the data, then fit a model with \\(p=3\\) auto-regressive terms and \\(q=1\\) moving average terms. import numpy as np import pandas as pd from statsmodels.tsa.arima.model import ARIMA gdp = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", index_col=0) # Take 1st diff of log of gdp d_ln_gdp = np.log(gdp).diff() print(d_ln_gdp.head()) . GDPC1 DATE 1947-01-01 NaN 1947-04-01 -0.002670 1947-07-01 -0.002067 1947-10-01 0.015521 1948-01-01 0.014931 . You can see that the first value is NaN. That’s because, for the first value, there is no previous value to do the differencing with. Let’s fit the model: . p = 3 d = 0 q = 1 mod = ARIMA(d_ln_gdp, order=(p, d, q)) res = mod.fit() print(res.summary()) . SARIMAX Results ============================================================================== Dep. Variable: GDPC1 No. Observations: 292 Model: ARIMA(3, 0, 1) Log Likelihood 972.763 Date: 00:00:00 AIC -1933.526 Time: 00:00:00 BIC -1911.466 Sample: 01-01-1947 HQIC -1924.690 - 10-01-2019 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0077 0.001 9.163 0.000 0.006 0.009 ar.L1 0.1918 0.417 0.461 0.645 -0.625 1.008 ar.L2 0.1980 0.145 1.368 0.171 -0.086 0.482 ar.L3 -0.0961 0.072 -1.332 0.183 -0.237 0.045 ma.L1 0.1403 0.410 0.342 0.732 -0.663 0.944 sigma2 7.301e-05 4.29e-06 17.006 0.000 6.46e-05 8.14e-05 =================================================================================== Ljung-Box (L1) (Q): 0.02 Jarque-Bera (JB): 59.78 Prob(Q): 0.90 Prob(JB): 0.00 Heteroskedasticity (H): 0.26 Skew: 0.15 Prob(H) (two-sided): 0.00 Kurtosis: 5.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). ",
    "url": "/Time_Series/ARMA-models.html#python",
    
    "relUrl": "/Time_Series/ARMA-models.html#python"
  },"35": {
    "doc": "ARMA Models",
    "title": "R",
    "content": "There are numerous packages to estimate ARMA models in R. For this tutorial, we will use the arima() function, which comes preloaded into R from the stats package. For our purposes, it is sufficient to note that estimating an \\(ARIMA(p,0,q)\\) model is largely equivalent to estimating an \\(ARMA(p,q)\\). For more information about estimating a true ARIMA process (where \\(d&gt;0\\)), see the Also Consider section above. Additionally, the tsibble package can also be used to easily construct our quarterly log change in GDP variable. The arima() function does require that we specify the order of the model (ie, pick the values of \\(p\\) and \\(q\\)). For an alternative function that will evaluate multiple models and select the best performing, see the auto.arima function available through the forecast package. ## Load and install time series packages if (!require(\"tsibble\")) install.packages(\"tsibble\") library(tsibble) #load data gdp = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") #set our data up as a time-series gdp$DATE &lt;- as.Date(gdp$DATE) gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) #construct our first difference of log gdp variable gdp_ts$lgdp=log(gdp_ts$GDPC1) gdp_ts$ldiffgdp=difference(gdp_ts$lgdp, lag=1, difference=1) #Estimate our ARMA(3,1) ##Note that because we are modeling for the first difference of log GDP, we cannot use our first observation of ##log GDP to estimate our model. arma_gdp = arima(gdp_ts$lgdp[2:292], order=c(3,0,1)) arma_gdp . ",
    "url": "/Time_Series/ARMA-models.html#r",
    
    "relUrl": "/Time_Series/ARMA-models.html#r"
  },"36": {
    "doc": "ARMA Models",
    "title": "Stata",
    "content": "In Stata we will again estimate an \\(ARMA(p,q)\\) by estimating an \\(ARIMA(p,0,q)\\) using the Stata command arima. This command works similarly to Stata’s reg command. For information about the specific estimation procedure used by this function, optional arguments, etc, see Stata: ARIMA manual . *load data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\" *Generate the new date variable generate date_index = tq(1947q1) + _n-1 *Index the date variable as quarterly format date_index %tq *Convert a variable into time-series data tsset date_index *construct our first difference of log gdp variable gen lgdp = ln(gdpc1) gen dlgdp = D.lgdp *Specify the ARMA model using the arima command *Stata will automatically drop the first entry, since we do not have a value for the first difference of GDP *for this entry. arima dlgdp, arima(3,0,1) . ",
    "url": "/Time_Series/ARMA-models.html#stata",
    
    "relUrl": "/Time_Series/ARMA-models.html#stata"
  },"37": {
    "doc": "ARMA Models",
    "title": "ARMA Models",
    "content": " ",
    "url": "/Time_Series/ARMA-models.html",
    
    "relUrl": "/Time_Series/ARMA-models.html"
  },"38": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Adding and Labeling a Reference Line",
    "content": "With a lot of graph types, you may want to add a reference line so that the data can be compared to it. For example, perhaps you have a graph that shows growth over time, and want to have a reference line for “no growth” so you can easily see how far things have come. Or perhaps an event happens at a particular time and you want to mark when the event is. Or maybe you just want it to be easy to compare different categories to a mean. ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html"
  },"39": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Keep in Mind",
    "content": ". | Make sure that it’s clear what your reference line is. A reader might not guess that it represents a mean, or a particular event, or something else. In some cases, the line extending to a particular value on the x- or y-axis does the job. Other times you might want a direct label. | . ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#keep-in-mind"
  },"40": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Also Consider",
    "content": ". | This page will show how to place a line but not how to style it. You may want your line to be dashed, or bold, or a different color. In most cases the stylistic controls for your reference line will be the exact same as those for a regular line graph. See styling line graphs | . ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#also-consider",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#also-consider"
  },"41": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Implementations",
    "content": "These implementations will add a line indicating the mean area to the bar graphs found in line graph. They will also show how to place a vertical line, this time at a particular value between the bars, showing how reference lines can be placed on discrete axes as well. ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#implementations",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#implementations"
  },"42": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Python",
    "content": "# Load packages import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/Manitoba.lakes.csv\", index_col=0) # Calculate the value where we want the reference line to be, the mean # Note we could pick any other value here that we wanted mean_area = np.mean(df['area']) # This uses pandas' built-in bar plot function, but this uses # matplotlib under the hood; any other matplotlib bar graph works the same plt.style.use('seaborn') ax = df.plot.bar(y='area', legend=False, ylabel='Area', rot=15) ax.set_title('Area of lakes in Manitoba', loc='left') # Place the line plt.axhline(mean_area) # Look at our result and figure out the appropriate x/y location # We can figure out visually that it should be a bit above 5000, where # the line is. But how about x? We can set x by trial and error, or # note that there are 9 bars and get the x-coordinate of the last one # using ax.patches.get_x(), and adjust from there plt.annotate('Mean Area', xy = (ax.patches[8].get_x() - .5, 5500)) # Similarly if we want to position a vertical line, we can use plt.axvline. # How to position it on a discrete non-numeric x axis? # place it after the third bar using get_x to find the third bar # and get_width to move over to the right side of the bar, then a bit more to adjust plt.axvline(ax.patches[2].get_x() + ax.patches[2].get_width() + .25) . ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#python",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#python"
  },"43": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "R",
    "content": "In this example, we will place the line’s label using the ggplot2 function annotate(), which will require us to figure out the annotation’s coordinates ourselves. However, if you prefer, you can use the point-and-click annotation tool ggannotate. library(tidyverse) df = read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/Manitoba.lakes.csv\") %&gt;% rename(Location = ...1) # Get the reference value we want to mark mean_area = df %&gt;% pull(area) %&gt;% mean() # Make the bar plot, and order the bars by height, why not p &lt;- ggplot(df, aes(x = reorder(Location, -area), y = area)) + geom_col() + labs(x = 'Location', y = 'Area') + # add a horizontal line using geom_hline, specifying its y intercept geom_hline(yintercept = mean_area) # Look at our result so far and figure out the appropriate x/y location # for our annotation. p # We can figure out visually that it should be a bit above 5000, where # the line is. But how about x? We want it over the 9th bar, so we start with 9 # and can adjust from there by changing x or the horizontal justification (hjust) p &lt;- p + annotate(geom = 'text', label = 'Mean Area', x = 9, y = 5500) + # Now we can add a vertical reference line with geom_vline # If we want it between bars 3 and 4, that puts the line at x = 3.5 geom_vline(xintercept = 3.5) p . ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#r",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#r"
  },"44": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Stata",
    "content": "import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/Manitoba.lakes.csv\", clear rename v1 Location * Get the reference value we want to mark summ area local mean_area = r(mean) * Make the bar plot, and order the bars by height, why not * And add a horizontal reference line with yline graph bar area, over(Location, sort(1)) yti(Area) /// yline(`mean_area') * From here, you need to use the Graph Editor * Right-click the graph, do \"Start Graph Editor\", and add the annotation * And you can also add a line object to place a vertical reference line * If we want to do things ourselves, we need to switch to twoway * (which we should do here anyway for a method that works with non-bar graphs) * Unfortunately, twoway bar doesn't like categorical x axes so we need to do some work there * Put the bars in order and create a labeled numeric variable * that's in the order we want the bars gsort -area g Location_n = _n * Make sure labutil is installed with ssc install labutil labmask Location_n, values(Location) * Get the reference value we want to mark summ area local mean_area = r(mean) * NOW A DILEMMA: * we can easily add vertical and horizontal lines in twoway with yline and xline (or tline for time series graphs) * BUT these go BEHIND the graph, not in the foreground * that's okay for our vertical line between bars 3 and 4 (at 3.5) so let's do that * but for our horizontal line we'll need to draw it ourselves with function * which is annoying since we'll have to specify its range by hand * Start with our basic graph that mimics the graph bar we started with twoway (bar area Location_n, xti(\"Location\") yti(\"Area\") xlab(1/9, valuelabel) legend(off) /// xline(3.5)) /// Our vertical line goes between bars 3 and 4, i.e. at 3.5 (function y = `mean_area', range(.5 9.5) /// Now our horizontal line at the mean ) * Now we can look at our result and see where we think the annotation * should go. For our x-axis value we have 9 bars so should aim somewhere around 9 twoway (bar area Location_n, xti(\"Location\") yti(\"Area\") xlab(1/9, valuelabel) legend(off) /// xline(3.5) text(5500 9 \"Mean Area\")) /// Our vertical line goes between bars 3 and 4, i.e. at 3.5 (function y = `mean_area', range(.5 9.5)) . ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#stata",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#stata"
  },"45": {
    "doc": "Adding and Labeling a Reference Line",
    "title": "Tableau",
    "content": "To add a reference line in Tableau is easy when it’s a continuous variable. Start by making your graph. Then, go to the analytics pane and drag a reference line to your graph. Select the calculation (or value) you want . And the result will be automatically labeled. For a reference line on a discrete variable, the process is much more involved. See this guide. ",
    "url": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#tableau",
    
    "relUrl": "/Presentation/Figures/Adding_and_Labeling_a_Reference_Line.html#tableau"
  },"46": {
    "doc": "Animated Graphs",
    "title": "Animated graphs",
    "content": "This page provides a brief explanation on how to make animated graphs. Animated graphs help present information that changes over a time period (such as years) or space (such as by state). They are one straightforward way of presenting three-dimensional data in a two-dimensional space. ",
    "url": "/Presentation/Figures/Animated_graphs.html#animated-graphs",
    
    "relUrl": "/Presentation/Figures/Animated_graphs.html#animated-graphs"
  },"47": {
    "doc": "Animated Graphs",
    "title": "Keep in Mind",
    "content": ". | Animated graphs can be in any shape: line, plot, bar, etc. | Because the graph will be moving, it is often a good idea to start with a relatively simple graph. Otherwise the viewer can get overwhelmed. | Carefully think about the duration of your animation and the number of frames to determine the speed and fluidity at which it moves. | The height and width dimensions are important for presenting the animated details | . ",
    "url": "/Presentation/Figures/Animated_graphs.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/Animated_graphs.html#keep-in-mind"
  },"48": {
    "doc": "Animated Graphs",
    "title": "Also Consider",
    "content": ". | You may need to do some data manipulation to create the right animated graph. | Check the presentation Figures, there are many charts we can animate. | . ",
    "url": "/Presentation/Figures/Animated_graphs.html#also-consider",
    
    "relUrl": "/Presentation/Figures/Animated_graphs.html#also-consider"
  },"49": {
    "doc": "Animated Graphs",
    "title": "Implementations",
    "content": "Notes on implementations: . ",
    "url": "/Presentation/Figures/Animated_graphs.html#implementations",
    
    "relUrl": "/Presentation/Figures/Animated_graphs.html#implementations"
  },"50": {
    "doc": "Animated Graphs",
    "title": "R",
    "content": "There are many great packages to do animated graphs such as gganimate and plotly. Here we use gganimate and ggplot2 to do animated bar graphs, and gifski to save the results. gganimate has the benefit of being able to animate any graph made in ggplot2. This means that all the other graphs in the Figures section, which use ggplot2 for the R implementations, can be animated using gganimate. # Load in necessary packages library(ggplot2) library(gganimate) library(gifski) library(dplyr) # Load in desired data (gapminder) library(gapminder) . gganimate works by taking standard ggplot2 input and adding a transition_ function which specifies what should change from frame to frame, and how. transition_states() is fairly easy to work with, as it simply takes an ordered variable and transitions from value to value of that variable. Here we use year, and so it will remake the graph for each year, and transition between them. ease_aes() specifies how to smoothly transition between states. graph_data1 &lt;- gapminder %&gt;% filter(continent==\"Oceania\") # first use ggplot for the graph ggplot(graph_data1, aes(x=country, y=gdpPercap, fill=country)) + geom_bar(stat='identity') + theme_bw() + # then set the gganimate, transition_states( # to specify the transition, here we specify year, transition_length = 2, # How long to spend on each transition state_length = 1) + # How long to spend in each state (these are relative values, not numbers of frames) ease_aes('sine-in-out') + # to control easing of aesthetics labs(title = 'GDP per Capita in Year {closest_state}', # title with the timestamp period subtitle = 'Oceania Countries (1952 - 2007)') . anim_save(\"graph1.gif\") # to save the graph as gif . Animation can be a good way to show changes over many groups at once. graph_data &lt;- gapminder %&gt;% filter(continent==\"Americas\") # Plot bar graphs by country, we have 25 countries in the Americas, that hard to interpretation using one graph. # this example shows how gganimate can create a nice animated graph even with that high number of countries graph_2 &lt;- ggplot(graph_data, aes(x=country, y=gdpPercap, fill=country)) + geom_bar(stat='identity') + theme_bw() + # gganimate specific bits: transition_states( year, transition_length = 2, state_length = 1) + ease_aes('sine-in-out')+ theme(axis.text=element_blank(), axis.title.x=element_blank(), legend.position=\"center\", panel.border=element_blank(), axis.title.y=element_text(size=20), axis.text.y = element_text(hjust=1, size=16), axis.text.x = element_text(angle = 45, hjust=1, size=16), plot.title=element_text(size=25, hjust=0.5, face=\"bold\", colour=\"black\", vjust=1), plot.subtitle=element_text(size=24, hjust=0.5, face=\"italic\", color=\"grey\"), plot.margin = margin(2, 2, 4, 4, \"cm\"))+ labs(title = 'GDP per Capita for Year {closest_state}', # title with the timestamp period subtitle = 'Countries in North and South America (1952 - 2007)') . Here, we’ve saved the graph as an object, so that we can use the animate() function to render it and save it as a GIF. This allows us to adjust animation settings like the number of frames or the frames per second. # The animated sitting, create an image per frame, in this example, we used year so it creates an image for each year animate(graph_2, 100, fps = 20, end_pause=30, width = 1400, height = 1000, renderer = gifski_renderer(\"gganim1.gif\")) . There are many other transition_ functions. One useful option for line plots is transition_reveal(), which reveals only a part of the graph at a time until the entire graph is visible. # Plot North America, here we try line graph by only graph North America countries graph_data3 &lt;- graph_data %&gt;% filter(country %in% c(\"United States\", \"Canada\", \"Mexico\")) # Plot line graph graph_3 &lt;- ggplot(data = graph_data3) + geom_line(mapping = aes(x=year, y=lifeExp, color=country)) + theme_bw() + theme(legend.position = \"right\") + labs(title = 'Life Expectancy in North America (1952 - 2007)')+ transition_reveal(year) + # Reveal data along a given dimension ease_aes('linear') # The values change linearly during tweening graph_3 . anim_save(\"graph_3.gif\") # to save the graph as gif . ",
    "url": "/Presentation/Figures/Animated_graphs.html#r",
    
    "relUrl": "/Presentation/Figures/Animated_graphs.html#r"
  },"51": {
    "doc": "Animated Graphs",
    "title": "Animated Graphs",
    "content": " ",
    "url": "/Presentation/Figures/Animated_graphs.html",
    
    "relUrl": "/Presentation/Figures/Animated_graphs.html"
  },"52": {
    "doc": "Autocorrelation Function",
    "title": "Autocorrelation Function",
    "content": "Serial correlation, also known as autocorrelation, is the relationship between a variable and lagged versions of itself. Error terms from different time periods of a time series variable being correlated is what we call “serial correlation”. Time series data that exhibits autocorrelation is easier to predict and forecast than data that does not. The process for detecting the presence of serial correlation is fairly straightforward. In effect, you simply check whether a variable is correlated with lagged versions of itself. There are details in that process, but that’s the main idea. The “autocorrelation” function checks the correlation between a variable and different lags of itself, typically presented as a graph. It is common to then look at the correlations (or a plot of them) and see which lag lengths have nonzero correlations. There is also the “partial autocorrelation” function, which in the context of a single varaible looks at the correlation between \\(Y_t\\) and \\(Y_{t-j}\\) while controlling for each of the intermediate lags \\(Y_{t-1}, ..., Y_{t-j+1}\\). ",
    "url": "/Time_Series/Autocorrelation_Function.html",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html"
  },"53": {
    "doc": "Autocorrelation Function",
    "title": "Keep in Mind",
    "content": ". | In most languages, there is a dedicated data format for time series data that must be used with time series commands. See Creating a Time Series Dataset. | . ",
    "url": "/Time_Series/Autocorrelation_Function.html#keep-in-mind",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#keep-in-mind"
  },"54": {
    "doc": "Autocorrelation Function",
    "title": "Also Consider",
    "content": ". | The number of nonzero autocorrelations you find can help inform your choice of time-series model, for example how many autoregressive terms to include in an AR Model. | For further reading on serial correlation, see here | . ",
    "url": "/Time_Series/Autocorrelation_Function.html#also-consider",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#also-consider"
  },"55": {
    "doc": "Autocorrelation Function",
    "title": "Implementations",
    "content": "The data we use comes from the Federal Reserve’s Economic Database, using the series on U.S. national defense expenditures and gross investment. ",
    "url": "/Time_Series/Autocorrelation_Function.html#implementations",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#implementations"
  },"56": {
    "doc": "Autocorrelation Function",
    "title": "Julia",
    "content": "# Load necessary packages using StatsBase, CSV, DataFrames # Import (download) data data = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/FDEFX.csv\"), DataFrame) # Estimate autocorrelation function (lags 1-12) acfx = autocor(data.FDEFX, 1:12) # Estimate partial autocorrelation function (lags 1-12) pacfx = pacf(data.FDEFX, 1:12) . ",
    "url": "/Time_Series/Autocorrelation_Function.html#julia",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#julia"
  },"57": {
    "doc": "Autocorrelation Function",
    "title": "Python",
    "content": "import pandas as pd import statsmodels.api as sm d = pd.read_csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/FDEFX.csv') # Plot the ACF sm.graphics.tsa.plot_acf(d['FDEFX']) # Plot the PACF sm.graphics.tsa.plot_pacf(d['FDEFX']) # The PACF shows that, while defense expenditures are correlated with defense expenditures # many periods ago (as shown in the ACF), all of that is explained by a one-period lag. # The rest of the effect is *through* that one-period lag. ",
    "url": "/Time_Series/Autocorrelation_Function.html#python",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#python"
  },"58": {
    "doc": "Autocorrelation Function",
    "title": "R",
    "content": "Base-R has a well-rounded set of time series functions, including acf() and pacf(). # load in data, pick a variable, and make sure it is in time series format gov &lt;- read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/FDEFX.csv\") ts_def &lt;- ts(gov$FDEFX, start=c(1959), end=c(2020), frequency = 1) # now run the time series data through the autocorrelation function acf() # which will also produce a plot acf(ts_def, plot=TRUE, main=\"Defense Expenditures and Investment, Autocorrelation\") acf(ts_def, plot=FALSE, main=\"Defense Expenditures and Investment, Autocorrelation\") # or pacf() for a partial autocorrelation pacf(ts_def, plot=TRUE, main=\"Defense Expenditures and Investment, Autocorrelation\") # The PACF shows that, while defense expenditures are correlated with defense expenditures # many periods ago (as shown in the ACF), all of that is explained by a one-period lag. # The rest of the effect is *through* that one-period lag. ",
    "url": "/Time_Series/Autocorrelation_Function.html#r",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#r"
  },"59": {
    "doc": "Autocorrelation Function",
    "title": "Stata",
    "content": "import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/FDEFX.csv\", clear * Set the date variable to be quarterly date data * with qofd picking the quarter from the actual date g quarter = qofd(date(date, \"YMD\")) * tell Stata this is time series data * Specifically, quarterly data tsset quarter, quarterly * Get both the autocorrelation and partial autocorrelation * (with a text-based graph) corrgram fdefx * The PACF shows that, while defense expenditures are correlated with defense expenditures * many periods ago (as shown in the ACF), all of that is explained by a one-period lag. * The rest of the effect is *through* that one-period lag. ",
    "url": "/Time_Series/Autocorrelation_Function.html#stata",
    
    "relUrl": "/Time_Series/Autocorrelation_Function.html#stata"
  },"60": {
    "doc": "Balance Tables",
    "title": "Balance Tables",
    "content": "Balance Tables are a method by which you can statistically compare differences in characteristics between a treatment and control group. Common in experimental work and when using matching estimators, balance tables show if the treatment and control group are ‘balanced’ and can be seen as similarly ‘identical’ for comparison of a causal effect. ",
    "url": "/Presentation/Tables/Balance_Tables.html",
    
    "relUrl": "/Presentation/Tables/Balance_Tables.html"
  },"61": {
    "doc": "Balance Tables",
    "title": "Keep in Mind",
    "content": ". | When a characteristic is statistically different between control and treatment, your study is unbalanced in respect to that attribute. | When a characteristic is unbalanced in your study, you may want to consider controlling for that attribute as a variable in your model specification. | Balance tables can only report numeric differences and are not suitable for string value comparisions | . ",
    "url": "/Presentation/Tables/Balance_Tables.html#keep-in-mind",
    
    "relUrl": "/Presentation/Tables/Balance_Tables.html#keep-in-mind"
  },"62": {
    "doc": "Balance Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Balance_Tables.html#implementations",
    
    "relUrl": "/Presentation/Tables/Balance_Tables.html#implementations"
  },"63": {
    "doc": "Balance Tables",
    "title": "R",
    "content": "# Import Dependency library(\"cobalt\") # Load Data data(mtcars) # Create Balance Table bal.tab(am ~ mpg + hp, data = mtcars) . Another approach provides an omnibus summary of overall balance on many covariates and also allows for stratification like that arising from block-randomized experiments (for example those designed using the blockTools package) or matched designs (for example using the optmatch package). library(RItools) options(show.signif.stars=FALSE,digits=3) xb_res &lt;- xBalance(am~mpg+hp+cyl+wt,strata=list(nostrat=NULL,vsstrat=~vs),data=mtcars,report=\"all\") xb_res$overall xb_res$results[,c(1:3,6:7),] . ",
    "url": "/Presentation/Tables/Balance_Tables.html#r",
    
    "relUrl": "/Presentation/Tables/Balance_Tables.html#r"
  },"64": {
    "doc": "Balance Tables",
    "title": "Stata",
    "content": "* Import Dependency: 'ssc install table1' * Load Data sysuse auto, clear * Create Balance Table * You need to declare the kind of variable for each, as well as the variable by which you define treatment and control. * Adding test gives the statistical difference between the two groups. The ending saves your output as an .xls file table1, by(foreign) vars(price conts \\ mpg conts \\ weight contn \\ length conts) test saving(bal_tab.xls, replace) . Also Consider . The World Bank’s very useful ietoolkit for Stata has a very flexible command for creating balance tables, iebaltab. You can learn more about how to use it on their Wiki page on the command. ",
    "url": "/Presentation/Tables/Balance_Tables.html#stata",
    
    "relUrl": "/Presentation/Tables/Balance_Tables.html#stata"
  },"65": {
    "doc": "Contributing",
    "title": "HOW TO CONTRIBUTE",
    "content": ". | Get a GitHub account. You do not need to know Git to contribute to LOST, but you do need a GitHub account. | Read the Guide to GitHub Markdown which will show the syntax that is used on LOST pages. | Read the below LOST Writing Guide, which shows what a good LOST page looks like from top to bottom. Even if you are just adding another language to an existing page, be sure to read the Implementations section at the bottom. | Explore LOST using the navigation bar on the left, find a page that needs to be expanded, and add more content. Or find one that doesn’t exist but should (perhaps on the Desired Nonexistent Pages list, and write it yourself! Go to the GitHub Repository for LOST to find the appropriate file to edit or folder to create your new file in. | If you are a “Contributor” to the project, you can make your edits and changes directly to the repository. If not, you will need to issue a pull request to get your work on LOST. We will add you as a contributor after your first accepted pull request. If you don’t know Git or how to do a pull request, please post in Issues asking to be added as a contributor so you can edit LOST directly. | If you’ve made a new page, make sure it’s saved as a .md file, put it in the appropriate folder, and add Navigation Information at the top (see below). If you’ve written a Desired Nonexistent Page, be sure to remove it from the list. Or, if your page links to some new nonexistent pages, add those to the list! Also, try to see if other pages have attempted to link to the page you’re working on, and update their links so they go to the right place. | . ",
    "url": "/Contributing/Contributing.html#how-to-contribute",
    
    "relUrl": "/Contributing/Contributing.html#how-to-contribute"
  },"66": {
    "doc": "Contributing",
    "title": "LOST WRITING GUIDE",
    "content": "A LOST page is intended to be a set of instructions for performing a statistical technique, where “statistical technique” is broadly defined as “the things you do in statistical software”, which includes everything from loading data to estimating models to cleaning data to visualization to reproducible notebooks. After someone reads a LOST page, they should have a decent idea of: . | How to implement at least a basic version of what they want to do in the language/software they’re using | What common pitfalls there might be in what they’re doing | What are the pros and cons of meaningfully-different ways of doing the same thing, if relevant | Given what they’re doing, what else should they consider doing (for example, if they’re running a regression discontinuity design, you might suggest they also run a test for bunching on either side of the cutoff) | . Things to remember while writing: . | Be as clear as possible. You’re writing a set of instructions, in effect. People should be able to follow them. | The technical ability of the reader may vary by page. People reading a LOST page about how to calculate a mean probably have little experience with their software and will need a lot of hand-holding. You can assume that people reading a LOST page about Markov-Chain Monte Carlo methods probably already have a fairly solid background. | . ",
    "url": "/Contributing/Contributing.html#lost-writing-guide",
    
    "relUrl": "/Contributing/Contributing.html#lost-writing-guide"
  },"67": {
    "doc": "Contributing",
    "title": "Markdown",
    "content": "LOST pages are written in Markdown. Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for . Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text ![Image](src) . Note that links in LOST are relative links - when linking to another LOST page, don’t use the full URL. Instead of regular-markdown . [Page](url) . instead do, for example: . /Category/page.html . For more details see GitHub Flavored Markdown. ",
    "url": "/Contributing/Contributing.html#markdown",
    
    "relUrl": "/Contributing/Contributing.html#markdown"
  },"68": {
    "doc": "Contributing",
    "title": "Math",
    "content": "Math is rendered with MathJax, which provides support for \\(\\LaTeX\\) math formatting. To use on a specific page, make sure that the YAML at the top on the underlying Markdown (i.e.md) file includes a line saying mathjax: true. This should already be the default on most existing pages, but it is worth emphasising. For example, here is a screenshot of the “Contributing.md” file that you are reading right now. After that, equations and other math sections can be delimited with two dollar symbol pairs. For example, $$x = \\frac{1}{2}$$ is rendered inline as \\(x = \\frac{1}{2}\\). Similarly, we can render math in display mode (i.e. as a distinct block) by wrapping the dollar symbol pairs on separate lines. For example, . $$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon $$ . is rendered as display math: . \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\\] While we don’t include such examples here, note that standard \\(\\LaTeX\\) math environments such as \\begin{equation} ... \\end{equation} (for numbered equations) and \\begin{align} ... \\end{align} (for aligned equation systems) are all supported. Just remember to wrap them between a pair of dollar symbols. More information about MathJax can be found here. ",
    "url": "/Contributing/Contributing.html#math",
    
    "relUrl": "/Contributing/Contributing.html#math"
  },"69": {
    "doc": "Contributing",
    "title": "STRUCTURE OF A LOST PAGE",
    "content": "When starting a LOST page, you should copy the New Page Template. There are four main sections of a LOST page: . ",
    "url": "/Contributing/Contributing.html#structure-of-a-lost-page",
    
    "relUrl": "/Contributing/Contributing.html#structure-of-a-lost-page"
  },"70": {
    "doc": "Contributing",
    "title": "Navigation Information",
    "content": "Your page will begin with what’s known as YAML, i.e. something that looks like this: . --- title: Observation level parent: Data Manipulation has_children: false nav_order: 1 mathjax: true --- . You don’t need to worry too much about YAML syntax (here’s the Wikipedia entry for those interested). The important thing is that the YAML provides a set of very basic instructions for the website navigation and page structure. Make sure to fill in the title with a relevant and brief title. Also be sure to put the appropriate name for the parent — this will ensure that your page shows up in the appropriate spot in the navigation structure. Options for parent include: . | Data Manipulation | Geo-Spatial | Machine Learning | Model Estimation | Presentation | Summary Statistics | Time Series | Other | . For the most part, you should generally ignore has_children. (An exception is if you are creating a new section that does have new child pages, but then you are probably better off filing an issue with us to make sure this is done correctly.) You can also ignore nav_order — leaving this at 1 for everything will put everything in alphabetical order. ",
    "url": "/Contributing/Contributing.html#navigation-information",
    
    "relUrl": "/Contributing/Contributing.html#navigation-information"
  },"71": {
    "doc": "Contributing",
    "title": "Introduction",
    "content": "This is an introduction to the technique. Most of the time this will be just a few sentences about what it is and does, and perhaps why it is used. However, in cases of more niche or complex material, there may be a reason to include more detailed information or general non-language-specific instructions here. In general, however, for more detailed explanations or discussions of statistical properties, you can always just link to an outside trusted source like Wikipedia or a (non-paywalled) academic paper. ",
    "url": "/Contributing/Contributing.html#introduction",
    
    "relUrl": "/Contributing/Contributing.html#introduction"
  },"72": {
    "doc": "Contributing",
    "title": "Keep in Mind",
    "content": "This is a list of details and reminders for people using the method, especially if they are not yet an expert at it or if the detail is not well-known. This may include: . | Important assumptions that an estimation method makes. | Notes about interpreting the results. | Settings where the technique seems like it might be a good idea, but actually isn’t. | Features of the technique that might surprise users or be unexpected. | Rules of thumb for use (“you will want to set the number of bootstrap samples to at least 1,000 (citation)”) | . ",
    "url": "/Contributing/Contributing.html#keep-in-mind",
    
    "relUrl": "/Contributing/Contributing.html#keep-in-mind"
  },"73": {
    "doc": "Contributing",
    "title": "Also Consider",
    "content": "This is a list of other techniques that are commonly used in addition to this page’s technique, or as an alternative to this page’s technique. If not obvious, include a very brief explanation of why you might want to use that other technique in addition to/instead of the current one. Note that you can link to another LOST page even if that page doesn’t exist yet. Maybe it will inspite someone to write it! . For example, pages about estimation techniques might list standard robustness tests to be used in addition to the technique, or adjustments to standard errors they might want to use. A page about a data visualization technique might include a link to a page about setting color palettes to be used in addition. Or, they might list an alternative technique that might be used if a certain assumption fails (“This technique requires continuous variables. So if your data is discrete, use this other method.”). To link to other LOST pages (even if they don’t exist yet — don’t forget to add these to Desired Nonexistent Pages!), we ask that you spell the url in a way our markdown renderer understands. Specifically, please write the url as {{ \"/Category_name/page.html\" | relative_url }}. For example, if you’d like to link to this page, please write [Guide to Contributing]({{ \"/Contributing/Contributing.html\" | relative_url}}) which will then be rendered as Guide to Contributing. ",
    "url": "/Contributing/Contributing.html#also-consider",
    
    "relUrl": "/Contributing/Contributing.html#also-consider"
  },"74": {
    "doc": "Contributing",
    "title": "Implementations",
    "content": "Implementations contains multiple subsections, one for each statistical software environment/programming language that the technique can be implemented in. | Implementations should be listed in alphabetical order of software/language. eViews, then Python, then R, then SAS, then Stata, etc. | For each language, include well-commented and as-brief-as-reasonably-possible example code that provides an example of performing the technique. Readers should be able to copy the code and have it run the technique from beginning to end, including steps like loading in data if necessary. See existing pages for examples. | If someone else on the internet has already written a clear, thorough, and general implementation example, then linking to it is perfectly fine! This includes StackOverflow/StackExchange answers, which you can link to using the share button. Extremely long demonstrations for super-complex methods may be better left as links only (perhaps with a tiny example pulled out and put on LOST). If the example is short enough, though, including the example directly in the LOST page is preferable, with link attribution of the source, so readers don’t have to go elsewhere. | Avoid creating a long list of examples showing every variant or optional setting of the technique. Instead, focus on one main example, with variants included only if they are especially important. If you like, you can mention in comments additional useful options/settings the reader might want to look into and what they do. | If the technique requires that a package or library be installed, include the code for installing the package in a comment (or if you are using a language where libraries cannot be installed inside the code, include a comment directing the user to install the library). | If a given language has multiple ways of performing the same technique, ideally report only one “best” method, whatever that might be. If other methods are only different in trivial ways, then you can describe them as being alternatives, but avoid providing examples for them. If other methods are different in important ways, then include an example for each, with text explanations of what is different about them. If two contributors seriously disagree about which way is best, then they’re probably different in some meaningful way so you can include both as long as you can explain what that difference is. | It is fine to add implementations for software that only has a graphical interface rather than code (such as Excel) using screenshots. Be sure to keep images well-cropped and small so they don’t crowd the page. If your graphical instructions are necessarily very long, consider posting them as a blog post somewhere and just put a link to that post in Implementations. | . Images . Images can be added to Implementation sections if relevant, for example if you’re working with GUI-only software, or demonstrating the output of a data visualization technique. How can you add these images? You can upload the images somewhere on the internet that allows image linking, and include the image in your instructions with ![Image](src). Ideally, upload the image directly to the Images/name-of-your-page/ subfolder of whatever directory you’re working in, and link to the images there. Please be sure to add alt text to images for sight-impaired users. Image filenames should make reference to the language used to make them, i.e. python_scatterplot.png. Data . Ideally, the same data set will be uploaded to LOST directly in a format accessible by many languages (like CSV) in the Data/name-of-your-page/ subfolder of whatever directory you’re working in, and then that data can be used for implementation in all languages on the page. This is not required, but is encouraged. ",
    "url": "/Contributing/Contributing.html#implementations",
    
    "relUrl": "/Contributing/Contributing.html#implementations"
  },"75": {
    "doc": "Contributing",
    "title": "FREQUENTLY ASKED QUESTIONS",
    "content": ". | What techniques are important enough to be their own page? This is a little subjective, but if you’re writing about X, which is a minor option/variant of Y, then you can just include it on the Y page. If X is a different technique or a variant of Y that is used in different circumstances or produces meaningfully different output, then give X its own page. | How should I title my page? Pick a single, concise description of the technique you’re talking about. If there are multiple ways to refer to the technique you’re doing, pick one. You will also need to select a file name, which should be in lower_case_with_underscores.md and you might want to make a bit shorter. So Ordinary Least Squares (Linear Regression) might be the title and H1 heading, and ordinary_least_squares.md might be the file name. | What languages can I include in Implementations? Any language is valid as long as it’s something people actually do statistical analysis in. Don’t include something just because you can (I mean, you can technically do OLS in assembly but is that useful for anyone?), but because you think someone will find it useful. | Should I include the output of my code? For data visualization, yes! Just keep the images relatively small so they don’t crowd the page. See the Implementations section above for how to add images. If your output is not visual, there’s probably no need to include output unless you think that it is especially important for some technique. | How can I discuss what I’m doing with other contributors? Head to the Issues page and find (or post) a thread with the title of the page you’re talking about. | How can I [add an image/link to another LOST page/add an external link/bold text] in the LOST wiki? See the Markdown section above. | I want to contribute but I do not like all the rules and structure on this page. I don’t even want my FAQ entry to be a question. Just let me write what I want. If you have valuable knowledge about statistical techniques to share with people and are able to explain things clearly, I don’t want to stop you. So go for it. Maybe post something in Issues when you’re done and perhaps someone else will help make your page more consistent with the rest of the Wiki. I mean, it would be nicer if you did that yourself, but hey, we all have different strengths, right? | . ",
    "url": "/Contributing/Contributing.html#frequently-asked-questions",
    
    "relUrl": "/Contributing/Contributing.html#frequently-asked-questions"
  },"76": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": " ",
    "url": "/Contributing/Contributing.html",
    
    "relUrl": "/Contributing/Contributing.html"
  },"77": {
    "doc": "Correlation Matrix",
    "title": "Correlation Matrices",
    "content": "A correlation matrix is a table used to present the results of correlation tests between multiple variables at a time. A correlation test is a statistical method used to define the correlation between two (and sometimes more) variables. The correlation coefficients from these tests are what a correlation matrix is composed of. They are often used as a method to summarize large amounts of data with the intention of identifying patterns of highly correlated variables. They can be a key component to checking the viability of other analyses. ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#correlation-matrices",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#correlation-matrices"
  },"78": {
    "doc": "Correlation Matrix",
    "title": "Keep in Mind",
    "content": ". | When creating a correlation matrix it is important to consider the which type of method for correlation analysis will best meet your needs. While the most common is the Pearson Correlation Coefficient, the Spearman and the Kendall methods are also viable possibilities. The Pearson measures the linear dependence between two variables while Spearman and the Kendall methods use a rank-based, non-parametric correlation test. | Correlation only measures a linear relationship between two variables. Correlation will not pick up on non-linearity, and sometimes the correlation may be zero even when the two variables are clearly related. | As with most kinds of statistical analysis it is important to choose how to deal with missing values in your dataset when creating a correlation matrix. The most common method is to only include complete observations in the correlation test. This is a form of pairwise missing values. Pairwise missing values has an underlying assumption that all missing values are random, which is not necessarily the case. Multiple imputation is in some cases a often a better choice for dealing with missing values. It is just not as straightforward to do. | Thinking more about missing values, there are two approaches to dropping missing values (if that’s what you’re doing). One approach, the complete observations approach, is to drop an observation from all calculations for the entire matrix if it contains any missing values. So in a correlation matrix for the variables \\(A\\), \\(B\\), and \\(C\\), and row 1 of the data was missing its \\(C\\) value, it would also be dropped from the calculation of \\(Corr(A,B)\\). This first approach is preferred if you want to understand the correlation structure of an analysis that will drop observations in a similar way, like a covariance matrix or a linear regression. The second approach, the pairwise complete observations approach, is to only drop observations from correlation calculations involving missing data, so row 1 with a missing \\(C\\) value would be dropped from \\(Corr(B,C)\\) and \\(Corr(A,C)\\), but not from \\(Corr(A,B)\\). This sceond approach is preferred if you want to understand the overall relationships between the variables. | . ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#keep-in-mind",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#keep-in-mind"
  },"79": {
    "doc": "Correlation Matrix",
    "title": "Also Consider",
    "content": ". | Correlations measure linear relationships, and so are effectively rescaled versions of results from Ordinary Least Squares regression | Correlation matrices can be visualized as Heatmap Correlation Matrices | . ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#also-consider",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#also-consider"
  },"80": {
    "doc": "Correlation Matrix",
    "title": "Implementation",
    "content": " ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#implementation",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#implementation"
  },"81": {
    "doc": "Correlation Matrix",
    "title": "Python",
    "content": "import pandas as pd import numpy as np d = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\") # Get the correlation matrix d.corr() # Default is Pearson but we can do Kendall or Spearman with method d.corr(method = 'spearman') # This approach uses pairwise-complete correlations by default, but we can # drop non-complete rows first to do a complete-cases correlation matrix d.dropna().corr() # If we want significance we'll have to do it one at a time # (although we could build a loop to get it for all pairs of columns) # We could get a p-value by just running OLS, or we can use Pearson directly from scipy import stats pvalue = stats.pearsonr(d['mpg'],d['disp'])[1] . ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#python",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#python"
  },"82": {
    "doc": "Correlation Matrix",
    "title": "R",
    "content": "data(mtcars) # computing the correlation test using the pearson method # changing the method to \"spearman\" or \"kendall\" will change the type of correlation test used # \"complete.obs\" is a casewise deletion method for datasets with missing values cor_test &lt;- cor(mtcars, method = \"pearson\", use = \"complete.obs\") # Creating the correlation matrix for the previous correlation test round(cor_test) # To do a pairwise-complete approach, where observations with missing data in some variables # are still included in correlations for others, do use = \"pairwise.complete.obs\" # (no difference in this case since there's no missing data) cor(mtcars, method = \"pearson\", use = \"pairwise.complete.obs\") . The cor() function only creates correlation matrices with correlation coefficients. If you want to also include the p-values you will need to use the rcorr() function from the Hmisc package. This will only work for correlation tests using the Pearson or Spearman Methods. library(Hmisc) data(mtcars) #computing the correlation matrix using the pearson method (which is the default for rcorr()) cor_test2 &lt;- rcorr(as.matrix(mtcars)) cor_test2 . ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#r",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#r"
  },"83": {
    "doc": "Correlation Matrix",
    "title": "Stata",
    "content": "sysuse auto.dta, clear * For a basic correlation matrix using Pearson, use cor * This uses a complete cases method cor * * To do a pairwise-complete method, use pwcorr pwcorr * * You can also perform significance tests on the * individual correlations using the sig option pwcorr *, sig * p-values are below each correlation . ",
    "url": "/Presentation/Tables/Correlation_Matrix.html#stata",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html#stata"
  },"84": {
    "doc": "Correlation Matrix",
    "title": "Correlation Matrix",
    "content": " ",
    "url": "/Presentation/Tables/Correlation_Matrix.html",
    
    "relUrl": "/Presentation/Tables/Correlation_Matrix.html"
  },"85": {
    "doc": "Cross-Tabulation",
    "title": "Cross Tabulations",
    "content": "A cross-tabulation is a table that shows the relationship between two or more variables. While more complex models are incredibly important, it is just as useful to quickly understand and present a basic picture of your data. This is where cross-tabulations come in handy, they simplify the data by creating subgroups which can be interpreted at a smaller and more granular scale. A cross-tabulation is rudimentary form of analysis, and a great starting point for working with relationships between discrete variables. When presenting data for initial qualitative and quantitative analysis it is important to show how distribution of responses and distribution of groups works in the dataset. This can allow you to immediately see where deeper analysis can be used and the patterns within the data. They are specifically useful in both market research and population surveys. ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#cross-tabulations",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#cross-tabulations"
  },"86": {
    "doc": "Cross-Tabulation",
    "title": "Keep in Mind",
    "content": ". | Cross-tabulations are generally not appropriate if either of the variables you’re looking at is continuous. You may want to consider a scatterplot in this case, or a number of other options. | By default, cross-tabulations count (tabulate) the number of observations in each cell. But it is usually straightforward to have it provide the percentage in each cell instead. Which one you want depends on what question you’re trying to answer. | . ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#keep-in-mind",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#keep-in-mind"
  },"87": {
    "doc": "Cross-Tabulation",
    "title": "Also Consider",
    "content": ". | Cross-tabulations are relatively simple but the possibilities are endless. It is less about knowing how to make the table and more about knowing what variables to include in order to spot trends in the data. | . ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#also-consider",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#also-consider"
  },"88": {
    "doc": "Cross-Tabulation",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#implementations",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#implementations"
  },"89": {
    "doc": "Cross-Tabulation",
    "title": "Julia",
    "content": "Julia ecosystem has a specialized library for building cross-tables named FreqTables.jl. As any other registered library it can be installed from a REPL session with import Pkg; Pkg.add(\"FreqTables\") command. We’ll also need CSV and DataFrames libraries for data loading and preparation. using CSV, DataFrames, FreqTables # Get the data and convert from a CSV into a Data Frame lakers = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Tables/Data/lakers.csv\"), DataFrame) # Filter for only a couple Lakers players for space lakersd = lakers[lakers.team .== \"LAL\", :] # Ignore missing names while filtering for the given names lakersd = lakersd[coalesce.(in.(lakersd.player, Ref([\"Jordan Farmar\", \"Pau Gasol\", \"Kobe Bryant\"])), false), :] # Create a crosstab for each basketball action for each player ftbl = freqtable(lakersd, :player, :etype) # 3×6 Named Matrix{Int64} # player ╲ etype │ \"foul\" \"free throw\" \"rebound\" \"shot\" \"turnover\" \"violation\" # ────────────────┼─────────────────────────────────────────────────────────────────────────────────── # \"Jordan Farmar\" │ 99 75 110 389 87 4 # \"Kobe Bryant\" │ 180 529 410 1619 200 7 # \"Pau Gasol\" │ 159 423 752 993 148 6 . You can even create tables analyzing three (or more) variables. Here is the same table as above for home and away games. freqtable(lakersd, :player, :etype, :game_type) # 3×6×2 Named Array{Int64, 3} # [:, :, game_type=\"away\"] = # player ╲ etype │ \"foul\" \"free throw\" \"rebound\" \"shot\" \"turnover\" \"violation\" # ────────────────┼─────────────────────────────────────────────────────────────────────────────────── # \"Jordan Farmar\" │ 51 21 48 189 45 2 # \"Kobe Bryant\" │ 93 315 213 882 93 3 # \"Pau Gasol\" │ 78 221 389 513 78 5 # [:, :, game_type=\"home\"] = # player ╲ etype │ \"foul\" \"free throw\" \"rebound\" \"shot\" \"turnover\" \"violation\" # ────────────────┼─────────────────────────────────────────────────────────────────────────────────── # \"Jordan Farmar\" │ 48 54 62 200 42 2 # \"Kobe Bryant\" │ 87 214 197 737 107 4 # \"Pau Gasol\" │ 81 202 363 480 70 1 . In order to obtain proportions instead of count use prop function with an optional margins argument. If margins is nothing (the default), proportions over the whole table are computed. If margins is an Integer, or an iterable of Integers, proportions sum to 1 over dimensions specified by margins. In particular for a two-dimensional array, when margins is 1 row proportions are calculated, and when margins is 2 column proportions are calculated. prop(ftbl) # 3×6 Named Matrix{Float64} # player ╲ etype │ \"foul\" \"free throw\" \"rebound\" \"shot\" \"turnover\" \"violation\" # ────────────────┼─────────────────────────────────────────────────────────────────────────────────── # \"Jordan Farmar\" │ 0.0159935 0.0121163 0.0177706 0.0628433 0.0140549 0.000646204 # \"Kobe Bryant\" │ 0.0290792 0.0854604 0.0662359 0.261551 0.0323102 0.00113086 # \"Pau Gasol\" │ 0.0256866 0.068336 0.121486 0.16042 0.0239095 0.000969305 prop(ftbl, margins = 2) # 3×6 Named Matrix{Float64} # player ╲ etype │ \"foul\" \"free throw\" \"rebound\" \"shot\" \"turnover\" \"violation\" # ────────────────┼─────────────────────────────────────────────────────────────────────────────────── # \"Jordan Farmar\" │ 0.226027 0.0730282 0.086478 0.129623 0.2 0.235294 # \"Kobe Bryant\" │ 0.410959 0.515093 0.322327 0.539487 0.45977 0.411765 # \"Pau Gasol\" │ 0.363014 0.411879 0.591195 0.33089 0.34023 0.352941 . ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#julia",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#julia"
  },"90": {
    "doc": "Cross-Tabulation",
    "title": "Python",
    "content": "In Python we can use the .crosstab() method in pandas. import pandas as pd # Get Lakers data lakers = pd.read_csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Tables/Data/lakers.csv') # Filter for only a couple Lakers players for space lakersd = lakers.loc[lakers['team'] == \"LAL\"] lakersd = lakersd.loc[lakers['player'].isin(['Jordan Farmar', 'Pau Gasol', 'Kobe Bryant'])] # Create a crosstab for each basketball action for each player pd.crosstab(lakersd['player'], lakersd['etype']) # etype foul free throw rebound shot turnover violation # player # Jordan Farmar 99 75 110 389 87 4 # Kobe Bryant 180 529 410 1619 200 7 # Pau Gasol 159 423 752 993 148 6 . You can even create tables analyzing three variables. Here is the same table as above for home and away games. # add game_type to the crosstab # Use brackets[] to distinguish which two-way crosstab is \"inside\" a broader one. pd.crosstab([lakersd['player'], lakersd['etype']], lakersd['game_type']) # game_type away home # player etype # Jordan Farmar foul 51 48 # free throw 21 54 # rebound 48 62 # shot 189 200 # turnover 45 42 # violation 2 2 # Kobe Bryant foul 93 87 # free throw 315 214 # rebound 213 197 # shot 882 737 # turnover 93 107 # violation 3 4 # Pau Gasol foul 78 81 # free throw 221 202 # rebound 389 363 # shot 513 480 # turnover 78 70 # violation 5 1 . You can use the normalize argument to get percentages instead of counts. normalize = 'index' gets proportions relative to the row totals, normalize = 'column' goes relative to the column, and normalize = 'all' does relative to the whole table. pd.crosstab(lakersd['player'], lakersd['etype'], normalize = 'all') # etype foul free throw rebound shot turnover violation # player # Jordan Farmar 0.015994 0.012116 0.017771 0.062843 0.014055 0.000646 # Kobe Bryant 0.029079 0.085460 0.066236 0.261551 0.032310 0.001131 # Pau Gasol 0.025687 0.068336 0.121486 0.160420 0.023910 0.000969 . Tests of independence between the two variables can be obtained by passing .crosstab() output to scipy.stats.contingency_table. ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#python",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#python"
  },"91": {
    "doc": "Cross-Tabulation",
    "title": "R",
    "content": "There are many functions for creating cross tabulations in R, including the base-R table() function. Here we will focus on one: tabyl, which is part of the janitor package, and is the tidyverse version of the table() function. This command can be used to quickly create pretty cross-tabulations which are report ready. #Load in the packages library(pacman) p_load(tidyverse,janitor,kableExtra, lubridate) . First we can see how tabyl can create a quick summary table. To create the input just put the desired x and y variables into the tabyl() command. # Get lakers data data(\"lakers\") # Filter for only a couple Lakers players for space lakersd = lakers %&gt;% filter(team == \"LAL\") %&gt;% filter(player %in% c('Jordan Farmar','Pau Gasol','Kobe Bryant')) # Example using the base-R table() function table(lakersd$player, lakersd$etype) # Create a crosstab for each basketball action for each player # Syntax is similar to table() but tabyl takes the data set as its first argument lakersd %&gt;% tabyl(player,etype) # player foul free throw rebound shot turnover violation # Jordan Farmar 99 75 110 389 87 4 # Kobe Bryant 180 529 410 1619 200 7 # Pau Gasol 159 423 752 993 148 6 . You can even create tables analyzing three variables. Here is the same table as above for home and away games. # add game_type to the crosstab lakersd %&gt;% tabyl(player, etype, game_type) # $away # player foul free throw rebound shot turnover violation # Jordan Farmar 51 21 48 189 45 2 # Kobe Bryant 93 315 213 882 93 3 # Pau Gasol 78 221 389 513 78 5 # # $home # player foul free throw rebound shot turnover violation # Jordan Farmar 48 54 62 200 42 2 # Kobe Bryant 87 214 197 737 107 4 # Pau Gasol 81 202 363 480 70 1 . With tabyl you can also use the adorn commands to add percentages. For basketball this would help see what players you would be analyzing when looking at team shot data. lakers_shot = lakersd %&gt;% filter(etype == \"shot\") lakers_shot %&gt;% # Create tabyl tabyl(player,result) %&gt;% # Add a totals row adorn_totals(where = c(\"row\",\"col\")) %&gt;% # Add percentages of total shots taken by players adorn_percentages(denominator = \"col\") %&gt;% # Format to 1 decimal place adorn_pct_formatting(digits = 1)%&gt;% # Include amount of observations adorn_ns() # player made missed Total # Jordan Farmar 10.3% (153) 15.5% (236) 13.0% (389) # Kobe Bryant 51.1% (757) 56.7% (862) 53.9% (1619) # Pau Gasol 38.5% (570) 27.8% (423) 33.1% (993) # Total 100.0% (1480) 100.0% (1521) 100.0% (3001) . A chi square test of independence can use the chisq.test() function, which can accept table() output, or just your two variables directly. ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#r",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#r"
  },"92": {
    "doc": "Cross-Tabulation",
    "title": "Stata",
    "content": "In Stata we can use the base tabulate command, which can be shortened to tab. * Load data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Tables/Data/lakers.csv\" * Filter for only a couple Lakers players for space keep if team == \"LAL\" &amp; inlist(player, \"Jordan Farmar\",\"Pau Gasol\",\"Kobe Bryant\") * Create a crosstab for each basketball action for each player tab player etype * | etype * player | foul free th.. rebound shot turnover violation | Total *----------------------+------------------------------------------------------------------+---------- * Jordan Farmar | 99 75 110 389 87 4 | 764 * Kobe Bryant | 180 529 410 1,619 200 7 | 2,945 * Pau Gasol | 159 423 752 993 148 6 | 2,481 *----------------------+------------------------------------------------------------------+---------- * Total | 438 1,027 1,272 3,001 435 17 | 6,190 . To creat tables analyzing three variables, use by with tab. The following example produces a separate table for home and away games. You could alternately use the table command, using table player etype game_type to get a very similar result. Switching to table is a good idea for anything more complex than this, as it’s a very flexible command. bysort game_type: tab player etype *---------------------------------------------------------------------------------------------------------------- *-&gt; game_type = away * * | etype * player | foul free th.. rebound shot turnover violation | Total *----------------------+------------------------------------------------------------------+---------- * Jordan Farmar | 51 21 48 189 45 2 | 356 * Kobe Bryant | 93 315 213 882 93 3 | 1,599 * Pau Gasol | 78 221 389 513 78 5 | 1,284 *----------------------+------------------------------------------------------------------+---------- * Total | 222 557 650 1,584 216 10 | 3,239 * *---------------------------------------------------------------------------------------------------------------- *-&gt; game_type = home * * | etype * player | foul free th.. rebound shot turnover violation | Total *----------------------+------------------------------------------------------------------+---------- * Jordan Farmar | 48 54 62 200 42 2 | 408 * Kobe Bryant | 87 214 197 737 107 4 | 1,346 * Pau Gasol | 81 202 363 480 70 1 | 1,197 *----------------------+------------------------------------------------------------------+---------- * Total | 216 470 622 1,417 219 7 | 2,951 . tab has a number of options that allow for different kinds of analysis. row calculates the percentage of the row that each cell constitutes. col does the same for columns, and cell works for the overall table. Tests of independence between the two variables can be performed with the chi2 option (among some other available tests; see the help file). ",
    "url": "/Presentation/Tables/Cross-Tabulation.html#stata",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html#stata"
  },"93": {
    "doc": "Cross-Tabulation",
    "title": "Cross-Tabulation",
    "content": " ",
    "url": "/Presentation/Tables/Cross-Tabulation.html",
    
    "relUrl": "/Presentation/Tables/Cross-Tabulation.html"
  },"94": {
    "doc": "Building Custom Tables",
    "title": "Building Custom Tables",
    "content": "Sometimes you need to create a table that doesn’t fit neatly into any of the other categories of table shown the Tables page. ",
    "url": "/Presentation/Tables/Custom_Tables.html",
    
    "relUrl": "/Presentation/Tables/Custom_Tables.html"
  },"95": {
    "doc": "Building Custom Tables",
    "title": "Keep in Mind",
    "content": ". | Graphs are sometimes a more effective way to convey information. | . ",
    "url": "/Presentation/Tables/Custom_Tables.html#keep-in-mind",
    
    "relUrl": "/Presentation/Tables/Custom_Tables.html#keep-in-mind"
  },"96": {
    "doc": "Building Custom Tables",
    "title": "Also Consider",
    "content": ". | Check the Tables page to see if any of those approaches will work for your application. | . ",
    "url": "/Presentation/Tables/Custom_Tables.html#also-consider",
    
    "relUrl": "/Presentation/Tables/Custom_Tables.html#also-consider"
  },"97": {
    "doc": "Building Custom Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Custom_Tables.html#implementations",
    
    "relUrl": "/Presentation/Tables/Custom_Tables.html#implementations"
  },"98": {
    "doc": "Building Custom Tables",
    "title": "R",
    "content": "There are lots of ways to create custom tables, I am using RStudio’s gt because it is easy to use and gives me a lot of control over the table elements. It has many more options than I am demonstrating so be sure to look at their documentation. # Install gt if necessary # install.packages('gt') library(gt) # I'm also loading magrittr so I can use the %&gt;% pipe library(magrittr) # you data will need to be in a data.frame or tibble object # for this example I'll use the generic dataset mtcars # which I'm truncating to make the final table easier to see input_df = head(mtcars, 6) input_df = input_df[, 1:5] # create the table including the data.frame rownames Ex_table = gt(input_df, rownames_to_stub = TRUE) # add title, subtitle, and source note Ex_table = Ex_table %&gt;% tab_header( title = md(\"**Title in Bold Text**\"), subtitle = \"subtitle\" ) %&gt;% tab_source_note(\"Data from mtcars\") # add groupings to rows Ex_table = Ex_table %&gt;% tab_row_group( label = \"4 Cylinder\", rows = cyl == 4 ) %&gt;% tab_row_group( label = \"6 Cylinder\", rows = cyl == 6 ) %&gt;% tab_row_group( label = \"8 Cylinder\", rows = cyl == 8 ) . This produces: . | Title in Bold Text | . | subtitle | . | | | mpg | cyl | disp | hp | drat | . | 8 Cylinder | . | Hornet Sportabout | 18.7 | 8 | 360 | 175 | 3.15 | . | 6 Cylinder | . | Mazda RX4 | 21.0 | 6 | 160 | 110 | 3.90 | . | Mazda RX4 Wag | 21.0 | 6 | 160 | 110 | 3.90 | . | Hornet 4 Drive | 21.4 | 6 | 258 | 110 | 3.08 | . | Valiant | 18.1 | 6 | 225 | 105 | 2.76 | . | 4 Cylinder | . | Datsun 710 | 22.8 | 4 | 108 | 93 | 3.85 | . | Data from mtcars | . Citation for gt: Iannone R, Cheng J, Schloerke B (2022). gt: Easily Create Presentation-Ready Display Tables. https://gt.rstudio.com/, https://github.com/rstudio/gt. ",
    "url": "/Presentation/Tables/Custom_Tables.html#r",
    
    "relUrl": "/Presentation/Tables/Custom_Tables.html#r"
  },"99": {
    "doc": "Figures",
    "title": "Figures",
    "content": " ",
    "url": "/Presentation/Figures/Figures.html",
    
    "relUrl": "/Presentation/Figures/Figures.html"
  },"100": {
    "doc": "GARCH Model",
    "title": "Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model",
    "content": "Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) is a statistical model used in analyzing time-series data where the variance error is believed to be serially autocorrelated. GARCH models assume that the variance of the error term follows an autoregressive moving average process. GARCH (p, q) model (where p is the order of the GARCH terms \\(\\sigma^{2}\\) and q is the order of the ARCH terms \\(\\epsilon^{2}\\)) is a model which \\(\\epsilon_{t}\\), the error terms, can be split into a stochastic piece \\(z_{t}\\) and a time-dependent standard deviation \\(\\sigma_{t}\\) characterizing the typical size of the terms so that \\(\\epsilon_{t}=\\sigma_{t}z_{t}\\). The random variable \\(z_{t}\\) is a strong white noise process while \\(\\sigma_{t}^{2}\\) is an ARMA process, i.e., . \\[\\sigma_{t}^{2} = \\alpha_{0} + \\sum_{i=1}^{q}\\alpha_{i}\\epsilon_{t-i}^{2} + \\sum_{i=1}^{p}\\beta_{i}\\sigma_{t-i}^{2} \\, .\\] ",
    "url": "/Time_Series/GARCH_Model.html#generalized-autoregressive-conditional-heteroscedasticity-garch-model",
    
    "relUrl": "/Time_Series/GARCH_Model.html#generalized-autoregressive-conditional-heteroscedasticity-garch-model"
  },"101": {
    "doc": "GARCH Model",
    "title": "Keep in Mind",
    "content": ". | Data should be properly formatted for estimation as a time-series. See creating a time series data set. If not, you may fail to execute or receive erroneous output. | GARCH is appropriate for time series data where the variance of the error term is serially autocorrelated following an autoregressive moving average process. | . ",
    "url": "/Time_Series/GARCH_Model.html#keep-in-mind",
    
    "relUrl": "/Time_Series/GARCH_Model.html#keep-in-mind"
  },"102": {
    "doc": "GARCH Model",
    "title": "Also Consider",
    "content": ". | GARCH can be used to help predict the volatility of returns on financial assets. | GARCH is useful to assess risk and expected returns for assets that exhibit clustered periods of volatility in returns. | If an autoregressive(AR) model is assumed for the error variance, the model is an autoregressive conditional heteroskedasticity (ARCH) model. For more information on GARCH models, see Wikipedia: ARCH. For information about estimating an ARCH model, see LOST: ARCH models. | . ",
    "url": "/Time_Series/GARCH_Model.html#also-consider",
    
    "relUrl": "/Time_Series/GARCH_Model.html#also-consider"
  },"103": {
    "doc": "GARCH Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/GARCH_Model.html#implementations",
    
    "relUrl": "/Time_Series/GARCH_Model.html#implementations"
  },"104": {
    "doc": "GARCH Model",
    "title": "Julia",
    "content": "The ARCHModels.jl package offers a variety of ARCH-model simulation, estimation, and forecasting tools. We start by loading the package. # Load necessary packages using ARCHModels . Next, we use the simulate function to specify a GARCH{1,1} model with coefficient parameters a0, b1, and a1, and then simulate a realization of the specified data-generating process with 1000 observations. Here the a0 parameter corresponds to the intercept term, b1 corresponds to the \\(p=1\\) lag coefficient in GARCH(\\(p,q\\)), and a1 corresponds to the \\(q=1\\) lag coefficient. # Simulate a GARCH(1,1) process a0 = 0.2 a1 = 0.5 b1 = 0.3 garch11sim = simulate(GARCH{1,1}([a0, b1, a1]), 1000) . Lastly, we use the fit function to fit an GARCH{1,1} model to the generated series contained in the data attribute of the UnivariateARCHModel object we named garch11sim in the above code chunk. # Fit GARCH(1,1) model to simulated data fit(GARCH{1,1}, garch11sim.data) . ",
    "url": "/Time_Series/GARCH_Model.html#julia",
    
    "relUrl": "/Time_Series/GARCH_Model.html#julia"
  },"105": {
    "doc": "GARCH Model",
    "title": "Python",
    "content": "# setup from random import gauss from random import seed from matplotlib import pyplot from arch import arch_model import numpy as np # seed the process np.random.seed(1) # Simulating a GARCH(1, 1) process a0 = 0.2 a1 = 0.5 b1 = 0.3 n = 1000 w = np.random.normal(size=n) eps = np.zeros_like(w) sigsq = np.zeros_like(w) for i in range(1, n): sigsq[i] = a0 + a1*(eps[i-1]**2) + b1*sigsq[i-1] eps[i] = w[i] * np.sqrt(sigsq[i]) model = arch_model(eps) model_fit = model.fit() print(model_fit.summary) . ",
    "url": "/Time_Series/GARCH_Model.html#python",
    
    "relUrl": "/Time_Series/GARCH_Model.html#python"
  },"106": {
    "doc": "GARCH Model",
    "title": "R",
    "content": "# setup library(fGarch) # seed pseudorandom number generator set.seed(1) # Simulating a GARCH(1,1) process a0 &lt;- 0.2 a1 &lt;- 0.5 b1 &lt;- 0.3 obs &lt;- 1000 eps &lt;- rep(0, obs) sigsq &lt;- rep(0,obs) for (i in 2:obs) { sigsq[i] = a0 + a1*(eps[i-1]^2) + b1*sigsq[i-1] eps[i] &lt;- rnorm(1)*sqrt(sigsq[i])} # fit the model garch.fit &lt;- garchFit(~garch(1,1), data = eps, trace = F) summary(garch.fit) . ",
    "url": "/Time_Series/GARCH_Model.html#r",
    
    "relUrl": "/Time_Series/GARCH_Model.html#r"
  },"107": {
    "doc": "GARCH Model",
    "title": "GARCH Model",
    "content": " ",
    "url": "/Time_Series/GARCH_Model.html",
    
    "relUrl": "/Time_Series/GARCH_Model.html"
  },"108": {
    "doc": "Generalised Least Squares",
    "title": "Generalised Least Squares",
    "content": " ",
    "url": "/Model_Estimation/GLS/GLS.html",
    
    "relUrl": "/Model_Estimation/GLS/GLS.html"
  },"109": {
    "doc": "Geo-Spatial",
    "title": "Geo-Spatial",
    "content": " ",
    "url": "/Geo-Spatial/Geo-spatial.html",
    
    "relUrl": "/Geo-Spatial/Geo-spatial.html"
  },"110": {
    "doc": "Granger Causality",
    "title": "Introduction",
    "content": "Economic theory usually suggests other variables that could help to forecast the variable of interest over than itself. When we add other variables and their lags the result is what is known as The Autoregressive Lag (ADL) Model. For example, if we want to predict future changes in inflation, the theory (Phillips Curve) suggests that lagged values of the unemployment rate might be a good predictor. In particular, the method for indicating when one variable possibly causes a response in another is called the Granger Causality Test. But be careful and do not get confused with the name. The test does not strictly mean that we have estimated the causal effect of one variable on another. It means that the signal of the first one is a useful predictor of the second. A variable \\(X\\) is said to Granger cause another variable \\(Y\\) if \\(Y\\) can be better predicted from the past of \\(X\\) and \\(Y\\) together than the past of \\(Y\\) alone, other relevant information being used in the prediction (Pierce, 1977). ",
    "url": "/Time_Series/Granger_Causality.html#introduction",
    
    "relUrl": "/Time_Series/Granger_Causality.html#introduction"
  },"111": {
    "doc": "Granger Causality",
    "title": "Keep in Mind",
    "content": ". | Check that both series are stationary. If necessary, transform the data via logarithms or differences. | Estimate the model with lags enough to ensure white noise residuals. | When you are choosing the number of lags one variable might affect the other, there is a trade-off between bias and power. To see more click here. | . | Re-estimate both models, including the lags of the other variable. \\[Y_t = \\alpha + \\sum_{j=1}^{p} \\beta_j Y_{t-j} + \\sum_{j=1}^r \\theta_j X_{t-j}+ \\epsilon_t\\] \\[X_t = \\alpha^{\\ast} + \\sum_{j=1}^{p} \\beta_j^{\\ast} Y_{t-j} + \\sum_{j=1}^r \\theta_j^{\\ast} X_{t-j}+ \\epsilon_t^{\\ast}\\] . | \\(X_t\\) helps to predict \\(Y_t\\) if \\(\\theta_j \\neq 0\\) for some \\(j\\). | \\(Y_t\\) helps to predict \\(X_t\\) if \\(\\theta_j^{\\ast} \\neq 0\\) for some \\(j\\). | . | Use an F test to determine significance of the new variables. Consider the following ADL model: . \\[H_0: \\theta_j^{\\ast} =0 \\quad \\text{for all} \\quad j = 1,\\dots, r \\quad \\text{by means of F-test}.\\] . | Interpretation: \\(X\\) Granger causes \\(Y\\) if it helps to predict \\(Y\\), whereas \\(Y\\) does not help to predict \\(X\\). | . | . ",
    "url": "/Time_Series/Granger_Causality.html#keep-in-mind",
    
    "relUrl": "/Time_Series/Granger_Causality.html#keep-in-mind"
  },"112": {
    "doc": "Granger Causality",
    "title": "Also consider",
    "content": ". | You might also be interested in a Nonparametric Test for Granger Causality. Especially useful to examine a large number of lags, and flexible to find Granger causality in specific regions on the distribution. See more here | . ",
    "url": "/Time_Series/Granger_Causality.html#also-consider",
    
    "relUrl": "/Time_Series/Granger_Causality.html#also-consider"
  },"113": {
    "doc": "Granger Causality",
    "title": "Implementation",
    "content": " ",
    "url": "/Time_Series/Granger_Causality.html#implementation",
    
    "relUrl": "/Time_Series/Granger_Causality.html#implementation"
  },"114": {
    "doc": "Granger Causality",
    "title": "R",
    "content": "More information about ADL modeling with R is available in Chapter 14 of Hanck et al. (freely available online). Simulation ADL model . NOTE: Feel free to skip this section if you are just interested in how to apply the test. # set seed set.seed(1234) # Simulate error n = 200 # Sample size rho = 0.5 # Correlation between Y errors coe = 1.2 # Coefficient of X in model Y alpha = 0.5 # Intercept of the model Y # Function to create the error of Y ARsim2 = function(rho, first, serieslength, distribution) { if (distribution==\"runif\") { a = runif(serieslength,min=0,max=1) } else { a = rnorm(serieslength,0,1) } Y = first for (i in (length(rho)+1):serieslength) { Y[i] = rho*Y[i-1]+(sqrt(1-(rho^2)))*a[i] } return(Y) } # Error for Y model error = ARsim2(rho, c(0, 0), n, \"rnorm\") # times series X (simulation) X = arima.sim(list(order = c(1, 0, 0), ar = c(0.2)), n) # times series Y (simulation) Y = NULL for (i in 2:200) { Y[i] = alpha + (coe * X[i - 1]) + error[i] } . Data . data = as.data.frame(cbind(1:200,X,as.ts(Y))) colnames(data) = c(\"time\", \"X\",\"Y\") . Graph . # If necessary # install.packages(\"tidyr\") # install.packages(\"ggplot2\") library(tidyr) library(ggplot2) graphdata = data[2:200,] |&gt; pivot_longer(cols = -c(time), names_to=\"variable\", values_to=\"value\") ggplot(graphdata, aes(x = time, y = value, group=variable)) + geom_line(aes(color = variable), size = 0.7) + scale_color_manual(values = c(\"#00AFBB\", \"#E7B800\")) + theme_minimal()+ labs(title = \"Simulated ADL models\")+ theme(text = element_text(size = 15)) . It seems that: . | Both series are stationary (later is check with the ADF test), and | Disturbances in variable \\(X\\) are visible after periods in \\(Y\\) (as expected). | . Check Stationarity . # If necessary # install.packages(\"tseries\") library(tseries) ## ADF test adf.test(X, k=3) adf.test(na.omit(Y), k=3) #na.omit() to delete the first 2 periods of lag . | With a p-value of 0.01 and 0.01 for series \\(X\\), and \\(Y\\), we assure that both are stationary. | No transformation needed for the series. | . Granger Test . Note: grangertest() only performs tests for Granger causality in bivariate series. Step 1. \\((Y \\sim X)\\) . # If neccesary # install.packages(\"lmtest\") library(lmtest) grangertest(Y ~ X, order = 2, data = data) . ## Granger causality test ## ## Model 1: Y ~ Lags(Y, 1:2) + Lags(X, 1:2) ## Model 2: Y ~ Lags(Y, 1:2) ## Res.Df Df F Pr(&gt;F) ## 1 192 ## 2 194 -2 198.42 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 . Step 2. \\((X \\sim Y)\\) . grangertest(X ~ Y, order = 2, data = data) . ## Granger causality test ## ## Model 1: X ~ Lags(X, 1:2) + Lags(Y, 1:2) ## Model 2: X ~ Lags(X, 1:2) ## Res.Df Df F Pr(&gt;F) ## 1 192 ## 2 194 -2 1.2028 0.3026 . | We see that the effect of lags of number of \\(X\\) is highly significant, and conclude that \\(X\\) predicts the future of \\(Y\\). | The null hypothesis is not rejected for the converse relationship. Thus, we conclude that \\(X\\) Granger causes \\(Y\\). | . Impulse Response Functions . After establishing that \\(X\\) predicts the future of \\(Y\\), it is often useful to show exactly how how a shock in \\(X\\) in the present will impact \\(Y\\) in the future. Impulse response functions are a great way to visualize the magnitude and duration of this impact on \\(Y\\). Vector Autoregression . In order to visualize the impulse response function, we first need to transform the \\(X\\) and \\(Y\\) variables through Vector Autoregression (VAR) modeling. In brief, VAR models are a multivariate generalization of scalar autoregressive (AR) models (for more information, see the AR Models page). Through a multivariate auto regression, we can use the autoregressive elements of each variable to visualize the granger causality we tested for earlier. VAR modeling can be done through the vars package in R. # If necessary # install.packages(\"vars\") library(vars) # Select only the X and Y columns and save as time series data data_ts = ts(data[2:3]) # Remove first lagged observation to omit NA values data_ts = data_ts |&gt; na.omit() # Save the model as a vector autoregression (VAR) model = VAR(data_ts, type = \"const\") . Plotting the IRF . Now that we have created our VAR model, we can plot how a shock to \\(X\\) will impact \\(Y\\). To do this we will use the vars::irf() function and the base R plot() function. # Create the impulse response function model_IRF = irf(model) # Plot plot(model_IRF) . This plot shows the response in \\(Y\\) to a shock in \\(X\\). As we would expect from the granger tests we ran earlier, a shock to \\(X\\) has a statistically significant effect on \\(Y\\). A positive shock to \\(X\\) results in a positive shock of similar magnitude in \\(Y\\) in the following period. This plot shows the response in \\(X\\) of a shock to \\(Y\\). A shock in \\(Y\\) elicits a very small, not statistically significant effect in \\(X\\). This is a visual confirmation that \\(Y\\) does not Granger cause \\(X\\). References . Granger, C. W. (1969). Investigating Causal Relations by Econometric Models and Cross-Spectral Methods. Econometrica, 37(3), 424–438. Pierce, D.A. (1977). $R^2$ Measures for Time Series. Special Studies Paper No. 93, Washington, D.C.: Federal Reserve Board. ",
    "url": "/Time_Series/Granger_Causality.html#r",
    
    "relUrl": "/Time_Series/Granger_Causality.html#r"
  },"115": {
    "doc": "Granger Causality",
    "title": "Granger Causality",
    "content": " ",
    "url": "/Time_Series/Granger_Causality.html",
    
    "relUrl": "/Time_Series/Granger_Causality.html"
  },"116": {
    "doc": "Graph Annotations",
    "title": "Graph Annotations",
    "content": "In general, data visualization should be capable of clearly demonstrating something about the underlying data. In many cases, that demonstration can be made clearer by adding annotations to the graph. These could be text annotations that emphasize the point being made, arrows that direct the eye to particularly interesting data, or other things. This page will demonstrate how to add text and arrow annotations to graphs. Typically, other types of annotations can be added using nearly the same steps - see the relevant documentation, or look for other buttons in cases where point-and-click systems are used. ",
    "url": "/Presentation/Figures/Graph_Annotation.html",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html"
  },"117": {
    "doc": "Graph Annotations",
    "title": "Keep in Mind",
    "content": ". | Annotations directly on the graph are generally best used to emphasize data or help clarify the point being made. Technical details are often better placed in a caption below the graph. | The ideal placement of annotations is often sensitive to the size of the graph. You often want to place an annotation in a spot where it’s not blocking anything, but in a smaller graph that’s going to be a different area than on a bigger graph. So decide, before adding your annotations, the size you’re going to export your graph at. | . ",
    "url": "/Presentation/Figures/Graph_Annotation.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html#keep-in-mind"
  },"118": {
    "doc": "Graph Annotations",
    "title": "Also Consider",
    "content": ". | Working with text descriptions on the graph axes with Formatting Graph Axes. | . ",
    "url": "/Presentation/Figures/Graph_Annotation.html#also-consider",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html#also-consider"
  },"119": {
    "doc": "Graph Annotations",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/Graph_Annotation.html#implementations",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html#implementations"
  },"120": {
    "doc": "Graph Annotations",
    "title": "Python",
    "content": "In matplotlib.pyplot package, the .text() and .annotate() methods, which can follow a properly-created graph (from matplotlib or seaborn, which is used here) can be used to add an annotation without, or with, an arrow. import matplotlib.pyplot as plt import seaborn as sns # Just for the mtcars data import statsmodels.datasets as smd mtcars = smd.get_rdataset('mtcars').data # Scatterplot sns.scatterplot(x='mpg', y = 'hp', data = mtcars) # Use .text() to add a basic text label at coordinates x and y # Here I use horizontalalignment = 'center' to more easily pick the coordinates plt.text(x=27, y=250, s='Look at that downward slope!', horizontalalignment = 'center') # Alternately, use .annotate() to add both a text label and also an arrow # Note separate coordinates for the text (xytext) and for the target point of the arrow (xy) sns.scatterplot(x='mpg', y = 'hp', data = mtcars) plt.annotate('Look at that downward slope!', xy = (20, 200), xytext = (27, 250), horizontalalignment = 'center', arrowprops=dict(facecolor='black', shrink=0.05)) . ",
    "url": "/Presentation/Figures/Graph_Annotation.html#python",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html#python"
  },"121": {
    "doc": "Graph Annotations",
    "title": "R",
    "content": "We’ll be using the annotate() function to add annotations to a ggplot2 graph. library(tidyverse) data(mtcars) # Make a graph p &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point() # Look at what you have to figure out the appropriate coordinates to put your label # You'll likely have to try your annotations a few times to get the coordinates correct p # Add text annotations with geom = 'label' or geom = 'text' # And fill in the label with \"label\" p + annotate(geom = 'text', x = 27, y = 250, label = 'Look at that downward slope!') # label_wrap from the scales package can often come in handy here p &lt;- p + annotate(geom = 'text', x = 27, y = 250, label = scales::label_wrap(20)('Look at that downward slope!')) # annotate should be compatible with most other geometries. A common one is 'segment' for lines p &lt;- p + annotate(geom = 'segment', x = 24, xend = 20, y = 230, yend = 200, arrow = arrow()) . This results in: . An alternate appraoch is to use ggannotate() from the ggannotate package. This package isn’t on CRAN, so instead of install.packages('ggannotate'), first install the **remotes** package and then run remotes::install_github(‘MattCowgill/ggannotate’)`. p2 &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point() ggannotate::ggannotate(p2) # This will open up a point-and-click annotation window . ",
    "url": "/Presentation/Figures/Graph_Annotation.html#r",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html#r"
  },"122": {
    "doc": "Graph Annotations",
    "title": "Stata",
    "content": "In Stata, the twoway style of graph (as well as some others, has a text() option we can use to add text annotations. * Load data import delimited using \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\", clear * Specify the text coordinates with y-axis first, then x, then the label * It may take a few tries to get the coordinates right twoway scatter hp mpg, text(250 27 \"Look at that downward slope!\") . Another approach, and one that will allow you to easily add arrow annotations, is to use the Stata Graph Editor, which is point-and-click. After creating your graph (without annotations), click File and Start Graph Editor. In this case, the graph editor was used to add an arrow annotation. ",
    "url": "/Presentation/Figures/Graph_Annotation.html#stata",
    
    "relUrl": "/Presentation/Figures/Graph_Annotation.html#stata"
  },"123": {
    "doc": "MA Models",
    "title": "Moving Average (MA) Models",
    "content": "Moving Average Models are a common method for modeling linear processes that exhibit serial correlation. This can take many forms, but an easy example is weather patterns. Imagine we modelled Ski Lift Purchases over a 5 consecutive weekday period. Let weather shocks be identically and independently distributed. A weather shock with large amounts of snow would cause more individuals to go ski. This shock would have future impacts on ski ticket purchases; individuals tomorrow will go skiing due to the snowfall today. This means when we think of the process, we should account for previous shock effects on outcomes today: . \\[SkiTicketPurchases_t = \\mu + \\theta_1*Weather_t + \\theta_2*Weather_{t-1}\\] ",
    "url": "/Time_Series/MA_Model.html#moving-average-ma-models",
    
    "relUrl": "/Time_Series/MA_Model.html#moving-average-ma-models"
  },"124": {
    "doc": "MA Models",
    "title": "Definition",
    "content": "Let \\(\\epsilon_t \\sim N(0, \\sigma^2_{\\epsilon})\\). A moving average process, which is denoted MA(q), take the form as: . \\[y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + ... + \\theta_q \\epsilon_{t-q}\\] Note that a MA(q) process has q lagged \\(\\epsilon\\) terms. Thus a MA(1) process would take the form: . \\[y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1}\\] ",
    "url": "/Time_Series/MA_Model.html#definition",
    
    "relUrl": "/Time_Series/MA_Model.html#definition"
  },"125": {
    "doc": "MA Models",
    "title": "Properties of a MA(1) process:",
    "content": ". | The mean is constant. | . \\[E(y_t) = E(\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1}) = \\mu\\] . | The variance is constant. | . \\[Var(y_t) = Var(\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1}) = (1 + \\theta_1^2) * \\sigma^2_{\\epsilon}\\] . | The covariance between \\(y_t\\) and \\(y_{t-q}\\) is decreasing as \\(q \\to \\infty\\). | . \\[Cov(y_t, y_{t-1}) = E(y_t*y_{t-1}) - E(y_t)E(y_{t-1}) \\\\ = E([\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1}] [\\mu + \\epsilon_{t-1} + \\theta_1 \\epsilon_{t-2}]) - \\mu^2 \\\\ = \\mu^2 + \\theta_1 \\sigma_{\\epsilon}^2 -\\mu^2 \\\\ = \\theta_1 \\sigma_{\\epsilon}^2\\] \\[Cov(y_t, y_{t-2}) = E(y_t*y_{t-2}) - E(y_t)E(y_{t-2}) \\\\ = E([\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1}] [\\mu + \\epsilon_{t-2} + \\theta_1 \\epsilon_{t-3}]) - \\mu^2 \\\\ = \\mu^2 -\\mu^2 \\\\ = 0\\] Additional helpful information can be found at Wikipedia: Moving Average Models . ",
    "url": "/Time_Series/MA_Model.html#properties-of-a-ma1-process",
    
    "relUrl": "/Time_Series/MA_Model.html#properties-of-a-ma1-process"
  },"126": {
    "doc": "MA Models",
    "title": "Keep in Mind",
    "content": ". | Time series data needs to be properly formatted (e.g. date columns should be formatted into a time) | Model Selection uses the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or the Akaike Information Criterion corrected (AICc) to determine the appropriate number of terms to include. Refer to Wikipedia:Model Selection for further information. | . ",
    "url": "/Time_Series/MA_Model.html#keep-in-mind",
    
    "relUrl": "/Time_Series/MA_Model.html#keep-in-mind"
  },"127": {
    "doc": "MA Models",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/MA_Model.html#implementations",
    
    "relUrl": "/Time_Series/MA_Model.html#implementations"
  },"128": {
    "doc": "MA Models",
    "title": "Julia",
    "content": "MA(q) models in Julia can be estimated using the StateSpaceModels.jl package, which also allows for the estimation of a variety of time series models that have linear state-space representations. Begin by importing and loading necessary packages into your work environment. # Load necessary packages using StateSpaceModels, LinearAlgebra, ShiftedArrays . We may simulate an MA(3) model, where the MA coefficients are set to \\(\\theta_1 = 0.5\\), \\(\\theta_2 = 0.3\\), and \\(\\theta_3 = 0.2\\). # Set parameters theta1, theta2, theta3 = (0.5, 0.3, 0.2) # Draw a sample of an iid N(0,1) disturbance/noise process # Set number of observations to 1000 u = randn(1000) # Simulate MA(3) process Y = u + theta1 .* lag(u, 1, default = 0.0) + theta2 .* lag(u, 2, default = 0.0) + theta3 .* lag(u, 3, default = 0.0) # Remove first 3 observations Y = Y[4:end] . The lag(u, k, default = 0.0) portions of the above code chunk creates a k-lag of series u, and sets the missing observations equal to 0.0. The data generated by a single sample draw from the above data-generating process can then be assigned a general ARIMA(p,d,q) representation, where if p and d are set to zero, the model specification becomes MA(q). The p=d= 0 constraint can be applied by inputting order = (0,0,q), where q&gt;0. # Specify the generated Y series as an MA(3) model model = SARIMA(Y, order = (0,0,3)) . Lastly, the above-specified model can be estimated using the fit! function, and the estimation results printed using the results function. The sole input for both of these functions is the model object that contains the chosen data series and its assigned ARIMA structure. # Fit (estimate) the model fit!(model) # Print estimates results(model) . ",
    "url": "/Time_Series/MA_Model.html#julia",
    
    "relUrl": "/Time_Series/MA_Model.html#julia"
  },"129": {
    "doc": "MA Models",
    "title": "R",
    "content": "#in the stats package we can simulate an ARIMA Model. ARIMA stands for Auto-Regressive Integrated Moving Average model. We will be setting the AR and I parts to 0 and only simulating a MA(q) model. set.seed(123) DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5))) plot(DT, ylab = \"Value\") . set.seed(123) DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5))) #ACF stands for Autocorrelation Function #Here we can see that there may be potential for 3 lags in our MA process. (Note: This is due to property (3): the covariance of y_t and y_{t-3} is nonzero while the covariance of y_t and y_{t-4} is 0) acf(DT, type = \"covariance\") . set.seed(123) DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5))) #Here I'm estimating an ARIMA(0,0,3) model which is a MA(3) model. Changing c(0,0,q) allows us to estimate a MA(q) process. arima(x = DT, order = c(0,0,3)) ## ## Call: ## arima(x = DT, order = c(0, 0, 3)) ## ## Coefficients: ## ma1 ma2 ma3 intercept ## 0.0722 0.2807 0.4781 0.0265 ## s.e. 0.0278 0.0255 0.0294 0.0573 ## ## sigma^2 estimated as 0.9825: log likelihood = -1410.63, aic = 2831.25 #We can also estimate a MA(7) model and see that the ma4, ma5, ma6, and ma7 are close to 0 and insignificant. arima(x = DT, order = c(0,0,7)) ## ## Call: ## arima(x = DT, order = c(0, 0, 7)) ## ## Coefficients: ## ma1 ma2 ma3 ma4 ma5 ma6 ma7 intercept ## 0.0714 0.2694 0.4607 -0.0119 -0.0380 -0.0256 -0.0219 0.0267 ## s.e. 0.0316 0.0321 0.0324 0.0363 0.0339 0.0332 0.0328 0.0533 ## ## sigma^2 estimated as 0.9806: log likelihood = -1409.65, aic = 2837.3 #fable is a package designed to estimate ARIMA models. We can use it to estimate our MA(3) model. library(fable) #an extension of tidyverse to temporal data (this allows us to create time series data into tibbles which are needed for fable functionality) library(tsibble) #visit https://dplyr.tidyverse.org/ to understand dplyr syntax; this package is important for fable functionality library(dplyr) #When using the fable package, we need to convert our object into a tsibble (a time series tibble). This gives us a data frame with values and an index for the time periods DT = DT %&gt;% as_tsibble() head(DT) ## # A tsibble: 6 x 2 [1] ## index value ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.123 ## 2 2 0.489 ## 3 3 2.53 ## 4 4 0.706 ## 5 5 -0.640 ## 6 6 0.182 #Now we can use the dplyr package to pipe our dataset and create a fitted model #Note: the ARIMA function in the fable package uses an information criterion for model selection; these can be set as shown below; additional information is above in the Keep in Mind section (the default criterion is aicc) MAfit = DT %&gt;% model(arima = ARIMA(value, ic = \"aicc\")) #report() is needed to view our model report(MAfit) ## Series: value ## Model: ARIMA(0,0,3) ## ## Coefficients: ## ma1 ma2 ma3 ## 0.0723 0.2808 0.4782 ## s.e. 0.0278 0.0255 0.0294 ## ## sigma^2 estimated as 0.9857: log likelihood=-1410.73 ## AIC=2829.47 AICc=2829.51 BIC=2849.1 #if instead we want to specify the model manually, we need to specify it. For MA models, set the pdq(0,0,q) term to the MA(q) order you want to estimate. For example: Estimating a MA(7) would mean that I should put pdq(0,0,7). Additionally, you can add a constant if wanted; this is shown below #with constant MAfit = DT %&gt;% model(arima = ARIMA(value ~ 1 + pdq(0,0,3), ic = \"aicc\")) report(MAfit) ## Series: value ## Model: ARIMA(0,0,3) w/ mean ## ## Coefficients: ## ma1 ma2 ma3 constant ## 0.0722 0.2807 0.4781 0.0265 ## s.e. 0.0278 0.0255 0.0294 0.0573 ## ## sigma^2 estimated as 0.9865: log likelihood=-1410.63 ## AIC=2831.25 AICc=2831.31 BIC=2855.79 #without constant MAfit = DT %&gt;% model(arima = ARIMA(value ~ 0 + pdq(0,0,3), ic = \"aicc\")) report(MAfit) ## Series: value ## Model: ARIMA(0,0,3) ## ## Coefficients: ## ma1 ma2 ma3 ## 0.0723 0.2808 0.4782 ## s.e. 0.0278 0.0255 0.0294 ## ## sigma^2 estimated as 0.9857: log likelihood=-1410.73 ## AIC=2829.47 AICc=2829.51 BIC=2849.1 #A faster, more compact way to write a code would be as follows: #Automatic estimation DT %&gt;% as_tsibble() %&gt;% model(arima = ARIMA(value)) %&gt;% report() ## Series: value ## Model: ARIMA(0,0,3) ## ## Coefficients: ## ma1 ma2 ma3 ## 0.0723 0.2808 0.4782 ## s.e. 0.0278 0.0255 0.0294 ## ## sigma^2 estimated as 0.9857: log likelihood=-1410.73 ## AIC=2829.47 AICc=2829.51 BIC=2849.1 #Manual estimation DT %&gt;% as_tsibble() %&gt;% model(arima = ARIMA(value ~ 0 + pdq(0, 0, 3))) %&gt;% report() ## Series: value ## Model: ARIMA(0,0,3) ## ## Coefficients: ## ma1 ma2 ma3 ## 0.0723 0.2808 0.4782 ## s.e. 0.0278 0.0255 0.0294 ## ## sigma^2 estimated as 0.9857: log likelihood=-1410.73 ## AIC=2829.47 AICc=2829.51 BIC=2849.1 . ",
    "url": "/Time_Series/MA_Model.html#r",
    
    "relUrl": "/Time_Series/MA_Model.html#r"
  },"130": {
    "doc": "MA Models",
    "title": "MA Models",
    "content": " ",
    "url": "/Time_Series/MA_Model.html",
    
    "relUrl": "/Time_Series/MA_Model.html"
  },"131": {
    "doc": "Machine Learning",
    "title": "Machine Learning",
    "content": " ",
    "url": "/Machine_Learning/Machine_Learning.html",
    
    "relUrl": "/Machine_Learning/Machine_Learning.html"
  },"132": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "Marginal Effects in Nonlinear Regression",
    "content": "In linear regression, the effect of a predictor can be interpreted directly in terms of the outcome variable. For example, in the model \\(Y = \\beta_0 + \\beta_1X + \\varepsilon\\), a one-unit increase in \\(X\\) is associated with a \\(\\beta\\)-unit change in \\(Y\\). However, in nonlinear regression, this is no longer the case. For example, if \\(Y\\) is binary and \\(E(Y=1) = F(\\beta_0 + \\beta_1X)\\) is estimated using a logit regression as a generalized linear model, then a one-unit increase in \\(X\\) is associated with a \\(\\beta\\)-unit change in the index function which then passes through the logit link function to produce a change in \\(E(Y=1)\\). Confusing! . Commonly, we would prefer to state our results in terms of changes in the actual dependent variable, which can be more intuitive. Marginal effects are one way of doing this. The marginal effect of \\(X\\) on \\(Y\\) in that logit regression is the relationship between a one-unit change in \\(X\\) and the probability that \\(Y=1\\). Marginal effects can be calculated for all sorts of nonlinear models. This page will discuss only logit and probit, but the same concepts (and, often, code, especially for other generalized linear models) work for other nonlinear models. The marginal effect can be calculated by taking the derivative of the outcome variable with respect to the predictor of interest. This is how effects can be interpreted in general. Even in a linear model like \\(Y = \\beta_0 + \\beta_1X + \\varepsilon\\), we can see that \\(\\partial{Y}/\\partial{X} = \\beta_1\\), i.e. a one-unit change in \\(X\\) is associated with a \\(\\beta_1\\)-unit change in \\(Y\\). For both probit and logit models in the form \\(E(Y=1) = F(\\beta_0 + \\beta_1X)\\), where \\(F()\\) is the link function, the marginal effect can be calculated as . \\[\\frac{\\partial{E(Y=1)}}{\\partial X} = \\beta_1F(\\cdot)(1-F(\\cdot))\\] Where \\(F(\\cdot) = F(\\beta_0 + \\beta_1X)\\) is the predicted probability that \\(Y=1\\) given the predictors in the model. This means that the marginal effect is different for each observation, since the predicted probability is different for each observation. Also, since \\(F(\\cdot)(1-F(\\cdot))\\) is highest near \\(F(\\cdot) = .5\\), this means that the marginal effect will be highest for observations with predicted probabilities near .5, and lowest for observations with predicted probabilities near 0 or 1. This makes sense - if you’re already predicted to have \\(Y = 1\\) with .99 probability, there’s not much more room for your probability to increase anyway - the marginal effect must be small! . The fact that the marginal effect is different for each individual also means that in order to present a single marginal effect, you must make a decision about how to combine everyone’s effect. There are several common approaches: . | The Marginal Effect of a Representative selects a single representative set of predictors (and thus a single representative predicted probability) and calculates the marginal effect for that representative. | The Marginal Effect at the Mean is the Marginal Effect of a Representative again, but this time the “representative” has the mean values of all of the predictors in the model. | The Average Marginal Effect calculates the marginal effect for each individual separately, and then takes the mean of the marginal effects. | . The average marginal effect is generally considered preferred (unless there is a particular representative of interest), since it accounts for correlations between the predictors, and creates an easily interpretable marginal effect. For more detail see The Effect, which this page draws from thoroughly. ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html"
  },"133": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "Keep in Mind",
    "content": ". | Some software commands default to the marginal effect at the mean, while others default to average marginal effects. Be sure you know which one you’re getting - read the documentation! | The marginal effect is necessarily a simplification of the actual nonlinear regression model - you are throwing a little information out. Be aware of that. | Marginal effect of a representative and marginal effect at the mean calculate marginal effects at particular values of the predictors. This does not calculate the marginal effect for subgroups with those levels of the predictors. Instead, it sets predictors to those values for all observations and calculates marginal effects as though those are the true values. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#keep-in-mind"
  },"134": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "Also Consider",
    "content": ". | While the code on this page will generalize to a number of different nonlinear regression methods beyond just probit and logit, it won’t cover all of them. If these don’t work, look for marginal effects estimators that are custom-suited to the method you’re using. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#also-consider",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#also-consider"
  },"135": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#implementations",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#implementations"
  },"136": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "Python",
    "content": "The statsmodels package contains methods for estimating some nonlinear models, and has built-in marginal effects methods for many of those models. import statsmodels.formula.api as sm from causaldata import restaurant_inspections df = restaurant_inspections.load_pandas().data # sm.logit wants the dependent variable to be numeric df['Weekend'] = 1*df['Weekend'] # Use sm.logit to run logit m1 = sm.logit(formula = \"Weekend ~ Year\", data = df).fit() # See the result # m1.summary() would also work Stargazer([m1]) # And get average marginal effects m1.get_margeff().summary() # Or marginal effects at the mean m1.get_margeff(at = 'mean').summary() . ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#python",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#python"
  },"137": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "R",
    "content": "The marginaleffects package (link) covers a wide variety of marginal effects methods in R. # Load packages and data library(marginaleffects) library(causaldata) data(restaurant_inspections) # Run a (silly) logit model mod1 = glm(Weekend ~ Year, data = restaurant_inspections, family = binomial) # Default link function for binomial family is \"logit\" # Use `marginaleffects()` to get the average marginal effects (AMEs) for all our # predictors mfx1 = marginaleffects(mod1) # Use `summary()` to pretty-print these AMEs summary(mfx1) # Use the `newdata = datagrid()` constructor argument to pick a representative # observation marginaleffects(mod1, newdata = datagrid(Year = 2005)) # An alternative to AME is the marginal effect at the mean (MEM). # We can use the `newdata = \"mean\"` convenience argument to retrieve MEMs. marginaleffects(mod1, newdata = \"mean\") . ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#r",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#r"
  },"138": {
    "doc": "Marginal Effects in Nonlinear Regression",
    "title": "Stata",
    "content": "Stata comes with the built-in margins command for marginal effects. * Get data from the causaldata package (ssc install causaldata) causaldata restaurant_inspections.dta, use clear download * Use the logit command to regress logit weekend year * and the margins command to get the average marginal effect for all predictors margins, dydx(*) * Use the at() specification to pick a representative observation margins, dydx(*) at(year = 2005) * Or use atmeans to use the mean of all predictors (for year... 2010.337? Marginal effect at the means is weird) margins, dydx(*) atmeans . ",
    "url": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#stata",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Marginal_Effects_in_Nonlinear_Regression.html#stata"
  },"139": {
    "doc": "Model Estimation",
    "title": "Model Estimation",
    "content": " ",
    "url": "/Model_Estimation/Model_Estimation.html",
    
    "relUrl": "/Model_Estimation/Model_Estimation.html"
  },"140": {
    "doc": "Multilevel Models",
    "title": "Multilevel Models",
    "content": " ",
    "url": "/Model_Estimation/Multilevel_Models/Multilevel_Models.html",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/Multilevel_Models.html"
  },"141": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "Introduction",
    "content": "K-Nearest Neighbor Matching is to classify a new input vector x, examine the k-closest training data points to x and assign the object to the most frequently occurring class. Optionally, we give closer points larger weights and more distant points smaller weights. Common value for k is 3 or 5. At k=1, the error rate is always zero for the training sample because the closest point to any training data point is itself. Therefore, it will always overfit. ",
    "url": "/Machine_Learning/Nearest_Neighbor.html#introduction",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html#introduction"
  },"142": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "Keep in Mind",
    "content": "| When to Consider | Advantages | Disadvantages | . | Instances map to points in $R^{n}$ | Traning is very fast | Slow at query time | . | Less than 20 attributes per instance | Learn complex target functions | Easily fooled by irrelevant attributes | . | Lots of training data | Do not lose information |   | . ",
    "url": "/Machine_Learning/Nearest_Neighbor.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html#keep-in-mind"
  },"143": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "Also Consider",
    "content": ". | Distance measure . | Most common: Euclidean distance | Euclidean distance makes sense when different measurements are commensurate; each is variable measured in the same units. | If the measurements are different, say length and weight, it is not clear. | . | . \\[d_{E}(x^{i}, x^{j}) = (\\sum_{k=1}^{p}(x^{i}_k - x^{j}_k)^2)^\\frac{1}{2}\\] . | Standardization . | When variables are not commensurate, we want to standardize them by dividing by the sample standard deviation. This makes them all equally important. | The estimate for the standard deviation of $x_k$: \\(\\hat{\\sigma}_k = \\biggl(\\frac{1}{n}\\sum_{i=1}^{n}(x^{i}_k - \\bar{x}_k)^2\\biggr)^\\frac{1}{2}\\) . where $\\bar{x}_k$ is the sample mean: \\(\\bar{x}_k = \\frac{1}{n}\\sum_{i=1}^{n}x^i_k\\) . | . | Weighted Euclidean Distance . | Finally, if we have some idea of the relative importance of each variable, we can weight them: | . | . \\[d_{WE}(i,j) = \\biggl(\\sum_{k=1}^{p}w_k(x^i_k - x^j_k)^2\\biggr)^\\frac{1}{2}\\] . | Choosing k . | Increasing k reduces variance and increases bias. | . | For high-dimensional space, problem that the nearest neighbor may not be very close at all. | Memory-based technique. Must make a pass through the data for each classification. This can be prohibitive for large data sets. | . ",
    "url": "/Machine_Learning/Nearest_Neighbor.html#also-consider",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html#also-consider"
  },"144": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/Nearest_Neighbor.html#implementations",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html#implementations"
  },"145": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "Python",
    "content": "For KNN, it is not required to import packages other than numpy. You can basically do KNN with one package because it is mostly about computing distance and normalization. You would need TensorFlow and Keras as you try more advanced algorithms such as convolutional neural network. import argparse import numpy as np from collections import Counter # Process arguments for k-NN classification def handle_args(): parser = argparse.ArgumentParser(description= 'Make predictions using the k-NN algorithms.') parser.add_argument('-k', type=int, default=1, help='Number of nearest neighbors to consider') parser.add_argument('--varnorm', action='store_true', help='Normalize features to zero mean and unit variance') parser.add_argument('--rangenorm', action='store_true', help='Normalize features to the range [-1,+1]') parser.add_argument('--exnorm', action='store_true', help='Normalize examples to unit length') parser.add_argument('train', help='Training data file') parser.add_argument('test', help='Test data file') return parser.parse_args() # Load data from a file def read_data(filename): data = np.genfromtxt(filename, delimiter=',', skip_header=1) x = data[:, 0:-1] y = data[:, -1] return (x,y) # Distance between instances x1 and x2 def dist(x1, x2): euclidean_distance = np.linalg.norm(x1 - x2) return euclidean_distance # Predict label for instance x, using k nearest neighbors in training data def classify(train_x, train_y, k, x): dists = np.sqrt(np.sum((x - train_x) ** 2, axis=1)) idx = np.argsort(dists, 0)[:k] k_labels = [train_y[index] for index in idx] prediction = list() prediction.append(max(k_labels, key=k_labels.count)) prediction = np.array(prediction) return prediction # Process the data to normalize features and/or examples. # NOTE: You need to normalize both train and test data the same way. def normalize_data(train_x, test_x, rangenorm, varnorm, exnorm): if rangenorm: train_x = 2 * (train_x - np.min(train_x, axis=0)) / np.nan_to_num(np.ptp(train_x, axis=0)) - 1 test_x = 2 * (test_x - np.min(test_x, axis=0)) / np.nan_to_num(np.ptp(train_x, axis=0)) - 1 pass if varnorm: train_x = (train_x - np.mean(train_x, axis=0)) / np.nan_to_num(np.std(train_x, axis=0)) test_x = (test_x - np.mean(test_x, axis=0)) / np.nan_to_num(np.std(test_x, axis=0)) pass if exnorm: for i in train_x: train_x = i / np.linalg.norm(i) for k in test_x: test_x = k / np.linalg.norm(k) pass return train_x, test_x # Run classifier and compute accuracy def runTest(test_x, test_y, train_x, train_y, k): correct = 0 for (x,y) in zip(test_x, test_y): if classify(train_x, train_y, k, x) == y: correct += 1 acc = float(correct)/len(test_x) return acc # Load train and test data. Learn model. Report accuracy. def main(): args = handle_args() # Read in lists of examples. Each example is a list of attribute values, # where the last element in the list is the class value. (train_x, train_y) = read_data(args.train) (test_x, test_y) = read_data(args.test) # Normalize the training data (train_x, test_x) = normalize_data(train_x, test_x, args.rangenorm, args.varnorm, args.exnorm) acc = runTest(test_x, test_y,train_x, train_y,args.k) print(\"Accuracy: \",acc) if __name__ == \"__main__\": main() . A very simple way to also get a very basic KNN down in Python is leverage the knowledge of the many smart people that contribute to sci-kit learn library (sklean) as it is a powerhouse of machine learning models, as well as other very useful tools like data splitting, model evaluation, and feature selections. #Import Libraries from seaborn import load_dataset import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Load a sample dataset iris_df = load_dataset('iris') # Quick and rough sketch comparing the petal feature to species sns.scatterplot(data=iris_df, x='petal_length', y='petal_width', hue='species') # Quick and rough sketch comparing the sepals feature to species sns.scatterplot(data=iris_df, x='sepal_length', y='sepal_width', hue='species') # Let's seperate the data into X and Y (features and target) X = iris_df.drop(columns='species') Y = iris_df['species'] # Split the data into training and testing for model evaluations X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.70, shuffle=True, random_state=777) # Iterate through different neighbors to find the best accuracy with N neighbors. accuracies = {} errors = {} for i in range(1, 15): clf = KNeighborsClassifier(n_neighbors=i) clf.fit(X=X_train, y=y_train) y_pred = clf.predict(X_test) accu_score = accuracy_score(y_true=y_test, y_pred=y_pred) accuracies[i] = accu_score sns.lineplot(x=accuracies.keys(), y=accuracies.values()).set_title('Accuracies by N-Neighbors') # Looks like about 8 is the first best accuracy, so we'll go with that. print(f\"{accuracies[8]:.1%}\") #100% accuracy for 8 neighbors. ",
    "url": "/Machine_Learning/Nearest_Neighbor.html#python",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html#python"
  },"146": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "R",
    "content": "The simplest way to perform KNN in R is with the package class. It has a KNN function that is rather user friendly and does not require you to do distance computing as it runs everything with Euclidean distance. For more advanced types of nearest neighbors matching it would be best to use the matchit function from the matchit package. To verify results this example also used the confusionMatrix function from the package caret. Due to how this package is designed the most room for error is during normalization, by normalizing variables that do not require or cannot accept it, like character variables. Another good source of error is not including drop = TRUE for your target, or y, vector which will prevent the model from running. Finally, given the way this example verifies results it is vital to convert the target into a factor as the data has to be in similar kind in order for R to give you an output. This walkthrough uses data from the UCI Machine Learning Repository under Breast Cancer Wisconsin (Diagnostic) Data Set (Rdocumentation for KNN). Also, statology’s “how to create a confusion matrix”. library(tidyverse) library(readr) # For KNN library(class) library(caret) # Import the Dataset df &lt;- read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/files/7088929/wdbc.csv\") view(df) # the first column is an identifier so remove that, anything that does not aid in classifying can be removed df &lt;- df[-1] # See the count of the target, either B, benign, or M, malignant table(df[1]) # Normalize the Dataset normal &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } # Apply to what needs to be normalized, in this case not the target df_norm &lt;- as.data.frame(lapply(df[2:31], normal)) # Verify that normalization has occurred summary(df_norm[1]) summary(df_norm[3]) summary(df_norm[11]) summary(df_norm[23]) # Split the dataframe into test and train datasets - note there are two dataframes # First test and train from the features, here is an example of about a 70/30 split for testing and training x_train &lt;- df_norm[1:397,] x_test &lt;- df_norm[398:568,] # Now test and train for the target - here it is important that you do \", 1\" to indicate only one column # It will not work unless you use drop = TRUE y_train &lt;- df[1:397, 1, drop = TRUE] y_test &lt;- df[398:568, 1, drop = TRUE] # The purpose of installing those packages were to use these next functions, first KNN # Like the Python example states, the best practice for a choice of K unless assigned is the square root of the number of observations pred &lt;- knn(train = x_train, test = x_test, cl = y_train, k = 23) # Confusion Matrix from Caret # KNN converts to a factor with two levels so we need to make sure the test dataset is similar y_test &lt;- y_test %&gt;% factor(levels = c(\"B\", \"M\")) # See how well the model did confusionMatrix(y_test, pred) . ",
    "url": "/Machine_Learning/Nearest_Neighbor.html#r",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html#r"
  },"147": {
    "doc": "K-Nearest Neighbor Matching",
    "title": "K-Nearest Neighbor Matching",
    "content": " ",
    "url": "/Machine_Learning/Nearest_Neighbor.html",
    
    "relUrl": "/Machine_Learning/Nearest_Neighbor.html"
  },"148": {
    "doc": "Ordinary Least Squares",
    "title": "Ordinary Least Squares",
    "content": " ",
    "url": "/Model_Estimation/OLS/OLS.html",
    
    "relUrl": "/Model_Estimation/OLS/OLS.html"
  },"149": {
    "doc": "Other",
    "title": "Other",
    "content": " ",
    "url": "/Other/Other.html",
    
    "relUrl": "/Other/Other.html"
  },"150": {
    "doc": "Presentation",
    "title": "Presentation",
    "content": " ",
    "url": "/Presentation/Presentation.html",
    
    "relUrl": "/Presentation/Presentation.html"
  },"151": {
    "doc": "Regression Tables",
    "title": "Regression Tables",
    "content": "Statistical packages often report regression results in a way that is not how you would want to display them in a paper or on a website. Additionally, they rarely provide an option to display multiple regression results in the same table. Two (bad) options for including regression results in your paper include copying over each desied number by hand, or taking a screenshot of your regression output. Much better is using a command that outputs regression results in a nice format, in a way you can include in your presentation. ",
    "url": "/Presentation/Tables/Regression_Tables.html",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html"
  },"152": {
    "doc": "Regression Tables",
    "title": "Keep in Mind",
    "content": ". | Any good regression table exporting command should include an option to limit the number of significant digits in your result. You should almost always make use of this option. It is very rare that the seventh or eighth decimal place (commonly reported in statistics packages) is actually meaningful, and it makes it difficult to read your table. | Variable names serve different purposes in statistical coding and in papers. Variable names in papers should be changed to be readable in the language of the paper. So for example, while employment may be recorded as EMP_STAT in your statistics package, you should rename it Employment for your paper. Most table exporting commands include options to perform this renaming. But if it doesn’t, you can always change it by hand after exporting. | If you use asterisks to indicate significance, be sure to check the significance levels that different numbers of asterisks indicate in the command you’re using, as standards for what significance levels the asterisks mean vary across fields (and so vary across commands as well). Most commands include an option to change the significance levels used. On that note, always include a table note saying what the different asterisk indicators mean! These commands should all include one by default - don’t take it out! | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#keep-in-mind",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html#keep-in-mind"
  },"153": {
    "doc": "Regression Tables",
    "title": "Also Consider",
    "content": ". | If you are a Word user, and the command you are using does not export to Word or RTF, you can get the table into Word by exporting an HTML, CSV, or LaTeX, then opening up the result in your browser, Excel, or TtH, respectively. Excel and HTML tables can generally be copy/pasted directly into Word (and then formatted within Word). You may at that point want to use Word’s “Convert Text to Table” command, especially if you’ve pasted in HTML. | By necessity, regression-output commands often have about ten million options, and they can’t all be covered on this page. If you want it to do something, it probably can. To reduce errors, it is probably a good idea to do as little formatting and copy/pasting by hand as possible. So if you want to do something it doesn’t do by default, like adding additional calculations, check out the help file for your command to see how you can do it automatically. | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#also-consider",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html#also-consider"
  },"154": {
    "doc": "Regression Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Regression_Tables.html#implementations",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html#implementations"
  },"155": {
    "doc": "Regression Tables",
    "title": "Python",
    "content": "The most convenient package for showing multiple regressions together is stargazer. In the code below, we’ll see an example of using stargazer and a couple of its most basic customisation options. Note that in order to run this example yourself, you may need to run ‘pip install packagename’ on the command line on your computer to install the pandas, stargazer, and statsmodels packages (if you don’t already have these packages installed). import pandas as pd import statsmodels.formula.api as smf from stargazer.stargazer import Stargazer . Get the mtcars data . mtcars = (pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv', dtype={'model': str, 'mpg': float, 'hp': float, 'disp': float, 'cyl': \"float\"})) . Let’s run two separate regressions that will be added to our summary table . results1 = smf.ols('mpg ~ cyl', data=mtcars).fit() results2 = smf.ols('mpg ~ cyl + hp', data=mtcars).fit() . To see the results table for an individual reg, use . print(results1.summary()) . OLS Regression Results ============================================================================== Dep. Variable: mpg R-squared: 0.726 Model: OLS Adj. R-squared: 0.717 Method: Least Squares F-statistic: 79.56 Date: Sat, 22 May 2021 Prob (F-statistic): 6.11e-10 Time: 10:09:14 Log-Likelihood: -81.653 No. Observations: 32 AIC: 167.3 Df Residuals: 30 BIC: 170.2 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 37.8846 2.074 18.268 0.000 33.649 42.120 cyl -2.8758 0.322 -8.920 0.000 -3.534 -2.217 ============================================================================== Omnibus: 1.007 Durbin-Watson: 1.670 Prob(Omnibus): 0.604 Jarque-Bera (JB): 0.874 Skew: 0.380 Prob(JB): 0.646 Kurtosis: 2.720 Cond. No. 24.1 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Now let’s create a table with both regressions in . stargazer_tab = Stargazer([results1, results2]) # Using a Python interactive window, simply run the # name of the object to display the table stargazer_tab . | . | | Dependent variable:mpg | . | | | (1) | (2) | . | . | Intercept | 37.885*** | 36.908*** | . | | (2.074) | (2.191) | . | cyl | -2.876*** | -2.265*** | . | | (0.322) | (0.576) | . | hp | | -0.019 | . | | | (0.015) | . | . | Observations | 32 | 32 | . | R2 | 0.726 | 0.741 | . | Adjusted R2 | 0.717 | 0.723 | . | Residual Std. Error | 3.206 (df=30) | 3.173 (df=29) | . | F Statistic | 79.561*** (df=1; 30) | 41.422*** (df=2; 29) | . | . | Note: | *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 | . To export to a file, use, for example: . open('regression.tex', 'w').write(stargazer_tab.render_latex()) # for latex open('regression.html', 'w').write(stargazer_tab.render_html()) # for html . Finally, it’s good practice to use informative names for variables . stargazer_tab.rename_covariates({'cyl': 'Cylinders', 'hp': 'Horsepower'}) stargazer_tab . | . | | Dependent variable:mpg | . | | | (1) | (2) | . | . | Intercept | 37.885*** | 36.908*** | . | | (2.074) | (2.191) | . | Cylinders | -2.876*** | -2.265*** | . | | (0.322) | (0.576) | . | Horsepower | | -0.019 | . | | | (0.015) | . | . | Observations | 32 | 32 | . | R2 | 0.726 | 0.741 | . | Adjusted R2 | 0.717 | 0.723 | . | Residual Std. Error | 3.206 (df=30) | 3.173 (df=29) | . | F Statistic | 79.561*** (df=1; 30) | 41.422*** (df=2; 29) | . | . | Note: | *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#python",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html#python"
  },"156": {
    "doc": "Regression Tables",
    "title": "R",
    "content": "There are many, many packages for exporting regression results in R, including RStudio’s gt, texreg, and xtable. Here we will focus on two: stargazer, which is probably the easiest to use, and huxtable, which is slightly more up-to-date and offers advanced formatting options, outlined on its website. # Install stargazer if necessary # install.packages('stargazer') library(stargazer) # Get mtcars data data(mtcars) # Let's give it two regressions to output lm1 &lt;- lm(mpg ~ cyl, data = mtcars) lm2 &lt;- lm(mpg ~ cyl + hp, data = mtcars) # Let's output an HTML table, perhaps for pasting into Word # We could instead set type = 'latex' for LaTeX or type = 'text' for a text-only table. stargazer(lm1, lm2, type = 'html', out = 'my_reg_table.html') # In line with good practices, we should use readable names for our variables stargazer(lm1, lm2, type = 'html', out = 'my_reg_table.html', covariate.labels = c('Cylinders','Horsepower'), dep.var.labels = 'Miles per Gallon') . This produces: . | | (1) | (2) | . | Cylinders | -2.876 *** | -2.265 *** | . | | (0.322)&nbsp;&nbsp;&nbsp; | (0.576)&nbsp;&nbsp;&nbsp; | . | Horsepower | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -0.019&nbsp;&nbsp;&nbsp;&nbsp; | . | | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | (0.015)&nbsp;&nbsp;&nbsp; | . | N | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | . | R2 | 0.726&nbsp;&nbsp;&nbsp;&nbsp; | 0.741&nbsp;&nbsp;&nbsp;&nbsp; | . | logLik | -81.653&nbsp;&nbsp;&nbsp;&nbsp; | -80.781&nbsp;&nbsp;&nbsp;&nbsp; | . | AIC | 169.306&nbsp;&nbsp;&nbsp;&nbsp; | 169.562&nbsp;&nbsp;&nbsp;&nbsp; | . | *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. | . Now we will do the same thing with huxtable, using mostly defaults. # Install huxtable and magrittr if necessary # install.packages('huxtable', 'magrittr') # huxtable works more easily with the pipe %&gt;% # which can come from magrittr or dplyr or tidyverse, etc. library(huxtable) library(magrittr) # First we build a huxreg object, using readable names huxreg(lm1, lm2, coefs=c('Cylinders' = 'cyl', 'Horsepower' = 'hp')) %&gt;% # We can send it to the screen to view it instantly print_screen() # Or we can send it to a file with the quick_ functions, which can # output to pdf, docx, html, xlsx, pptx, rtf, or latex. huxreg(lm1, lm2, coefs=c('Cylinders' = 'cyl', 'Horsepower' = 'hp')) %&gt;% # Let's make an HTML file quick_html(file = 'my_reg_output.html') . Which produces (note the different asterisks behavior, which can be changed with huxreg’s stars option): . | . | | Dependent variable: | . | | . | | Miles per Gallon | . | | (1) | (2) | . | . | Cylinders | -2.876*** | -2.265*** | . | | (0.322) | (0.576) | . | | | . | Horsepower | | -0.019 | . | | | (0.015) | . | | | . | Constant | 37.885*** | 36.908*** | . | | (2.074) | (2.191) | . | | | . | . | Observations | 32 | 32 | . | R2 | 0.726 | 0.741 | . | Adjusted R2 | 0.717 | 0.723 | . | Residual Std. Error | 3.206 (df = 30) | 3.173 (df = 29) | . | F Statistic | 79.561*** (df = 1; 30) | 41.422*** (df = 2; 29) | . | . | Note: | *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#r",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html#r"
  },"157": {
    "doc": "Regression Tables",
    "title": "Stata",
    "content": "There are two main ways of outputting regression results in Stata, both of which must be installed from ssc install: outreg2 and estout. We will use estout here, as it is more flexible. More detail is available on the estout website. Also note that, in a pinch, if you’re using a strange command that does not play nicely with estout, you can often select any Stata regression output, select the output, right-click, do “Copy Table”, and paste the result into Excel. This is only if all else fails. * Install estout if necessary * ssc install estout * Load auto data sysuse auto.dta, clear * Let's provide it two regressions * Making sure to store the results each time reg mpg weight estimates store weightonly reg mpg weight foreign estimates store weightandforeign * Now let's export the table using estout * while renaming the variables for readability using the variable labels already in Stata * replacing any table we've already made * and making an HTML table with style(html) * style(tex) also works, and the default is tab-delimited data for use in Excel. * Note also the default is to display t-statistics in parentheses. If we want * standard errors instead, we say so with se esttab weightonly weightandforeign using my_reg_output.html, label replace style(html) se . Which produces: . | . | . | | (1) | (2) | . | | Mileage (mpg) | Mileage (mpg) | . | . | . | Weight (lbs.) | -0.00601*** | -0.00659*** | . | | (0.000518) | (0.000637) | . | &nbsp; | . | Car type | | -1.650 | . | | | (1.076) | . | &nbsp; | . | Constant | 39.44*** | 41.68*** | . | | (1.614) | (2.166) | . | . | . | Observations | 74 | 74 | . | . | . | Standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001 | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#stata",
    
    "relUrl": "/Presentation/Tables/Regression_Tables.html#stata"
  },"158": {
    "doc": "Regular Expressions",
    "title": "Introduction",
    "content": "Regular expressions (AKA “Regex”) can be thought of as a pattern of characters that describes a specific set of strings with a common structure. They can also be thought of as state machines. String functions can take a character variable and a regular expression, and show you whether and how they match. Regex is useful for extracting information from text such as documents, spreadsheets, log files, and code. Regex utilizes metacharacters that have specific meaning: $ * + . ? [ ] ^ { } | ( ) \\ to find what we are looking for within the string. They can be used for string matching / replacing, and are supported across several programming languages such as Python, R, Stata, SQL, and more, although syntax does sometimes differ slightly between languages. The metacharacters can be broken up into a few different groups based on their meaning. | Specify the amount of repetitions of the pattern . | *: matches at least 0 times. | +: matches at least 1 times. | ?: matches at most 1 times. | {n}: matches exactly n times. | {n,}: matches at least n times. | {n,m}: matches between n and m times. | . | Specify where the match will be located in the string or word . | ^: matches the start of the string. | $: matches the end of the string. | \\b: matches the empty string at either edge of a word. Don’t confuse it with ^ $ which marks the edge of a string. | \\B: matches the empty string provided it is not at an edge of a word. | . | Operators . | .: matches any single character. | [...]: a character list, matches any one of the characters inside the square brackets. | [^...]: an inverted character list. Matches any characters except those inside the square brackets. | \\: suppress the special meaning of metacharacters in regular expression. | |: an “or” operator. Matches patterns on either side of the |. | (...): grouping in regular expressions. | . | Specify entire classes of characters like numbers or letters . | Two Types: One uses [: and :] around a predefined name, and the other uses \\ and a special character | [:digit:] or \\d: digits, equivalent to [0-9]. | \\D: non-digits, equivalent to [^0-9]. | [:lower:]: lower-case letters, equivalent to [a-z]. | [:upper:]: upper-case letters, equivalent to [A-Z]. | [:punct:]: punctuation characters. | [:alnum:]: alphanumeric characters. | . | Subexpressions can refer back to previously found strings . | () can be wrapped around a part of the regular expression, and that part will be remembered. | \\1, later in that regular expression, will refer to the contents of the first () being matched again (and \\2 would get the second, and so on). For example, (.)\\1 would match any string with a repeated character. This can also be used in substitution. For example, regex .*(pp).* being substituted with a\\1ly would replace apple with apply. | . | . ",
    "url": "/Data_Manipulation/Regular_Expressions.html#introduction",
    
    "relUrl": "/Data_Manipulation/Regular_Expressions.html#introduction"
  },"159": {
    "doc": "Regular Expressions",
    "title": "Keep in Mind",
    "content": ". | When using regular expressions everything is essentially a character, and we are writing patterns to match a specific sequence of characters. | Most patterns use normal ASCII, which includes letters, digits, punctuation and other symbols like %#$@!, but unicode characters can be used to match any type of international text. | . ",
    "url": "/Data_Manipulation/Regular_Expressions.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/Regular_Expressions.html#keep-in-mind"
  },"160": {
    "doc": "Regular Expressions",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Regular_Expressions.html#implementations",
    
    "relUrl": "/Data_Manipulation/Regular_Expressions.html#implementations"
  },"161": {
    "doc": "Regular Expressions",
    "title": "Python",
    "content": "In Python, regular expressions are implemented in the re module or, alternatively, through the .str interface in pandas. Let’s go through a few examples: . If we want to search for all words that started with the characters “ab” we would use the character ^ to specify we want words that start with “ab”. import re strings = [\"abcd\", \"cdab\", \"cabd\", \"c abd\"] filtered_strings = [s for s in strings if re.search(r\"^ab\", s)] print(filtered_strings) # ['abcd'] . Some notes about this example: . | We’ve used the re.search function. Sometimes people instead use re.match. re.match only looks for matches at the beginning of strings, so the functions re.search(r\"^ab\", s) and re.match(r\"ab\", s) are equivalent. | You’ll notice that instead of using plain double quotes around our regular expression we used the pattern r\"^ab\". The r means “raw string”, and it turns off (almost) all escaping. Thus, r\"\\d\" and \"\\\\d\" are equivalent. Because of the how common \\ is in regular expressions, it’s usually wise to use raw strings for regular expressions to avoid confusing errors. | . Alternatively, we could use also implement the above example with pandas: . import pandas as pd df = pd.DataFrame({\"strings\": [\"abcd\", \"cdab\", \"cabd\", \"c abd\"]}) df[df[\"strings\"].str.contains(r\"^ab\", regex=True)] . If we instead want to replace the text that matches a regular expression, we can use the re.sub or pandas .str.replace: . import re import pandas as pd strings = [\"what\", \"why\", \"you\", \"blazers\", \"fire\", \"hello\"] # Replace all the vowels in strings containing a vowel with \"!\" changed_strings = [ re.sub(r\"[aeiou]\", \"!\", s) for s in strings if re.search(r\"[aeiou]\", s) ] print(changed_strings) # ['wh!t', 'y!!', 'bl!z!rs', 'f!r!', 'h!ll!'] # Do the same thing in pandas df = pd.DataFrame({\"strings\": strings}) changed_series = df.loc[ df[\"strings\"].str.contains(r\"[aeiou]\", regex=True), \"strings\" ].str.replace(r\"[aeiou]\", \"!\", regex=True) print(changed_series) # 0 wh!t # 2 y!! # 3 bl!z!rs # 4 f!r! # 5 h!ll! # Name: strings, dtype: object . You can also see this guide for a fuller description of all the tools available. ",
    "url": "/Data_Manipulation/Regular_Expressions.html#python",
    
    "relUrl": "/Data_Manipulation/Regular_Expressions.html#python"
  },"162": {
    "doc": "Regular Expressions",
    "title": "R",
    "content": "In R, we can write our Regular expressions using the base R functions or with the stringr package functions. RegExplain is an RStudio addin for regular expressions. Regular expressions can be tricky at times, and RegExplain can help make it easier. RegExplain will allow you to build your regular expressions interactively. First, we can write a regular expression using the base R functions. Additional resources on Regex, string functions, and syntax. We can use the grep() function to identify filenames, for example. If we set the argument value = TRUE, grep() returns the matches, while value = FALSE returns their indices. grepl() is a similar function but returns a logical vector. Including ignore.case = TRUE ignores case sensitivity. Some special characters in R cannot be directly coded in a string (i.e '), so we have to “escape” the single quote in the pattern, by preceding it with \\. One important note about working with regular expressions in R is that, because \\ itself needs to be escaped, we must escape this metacharacter with a double backslash like \\\\$. For example, if we want to search for all words that started with the characters “ab” we would use the character ^ to specify we want words that start with “ab”. #create a string (string &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\")) # regex to search for strings that start with ab grep(\"^ab\", string, value = TRUE) . grepl() will tell us if there is a match to the pattern. Functions like sub() and gsub() replace the matches with new text. There are plenty of subexpressions so I will show a few. string &lt;- c(\"what\", \"why\", \"you\", \"blazers\", \"fire\", \"hello\") # create string # here we get TRUE/FALSE depending on if the text has any of the vowels \"[aeiou]\" grepl(\"[aeiou]\", string) #we can use sub() and gsub() to replace text sub(\"[aeiou]\", \"!\", string) . Second, We can write the regular expression using the stringr package, which is said to be easier to use and remember. We can write a regular expression and use the stringr::str_match() function to extract all the phone numbers from the string. There are a number of other useful string functions in the stringr package. Additional resources on Regex, string functions, and syntax can be found here. Similar to using the base R function, we still want to use ^ to specify we want a work that starts with “ab”. Now we have to specify that we want the entire word, not just the “ab” portion by including the .* syntax, which means “look for any character (.) any number of times (*)”, in other words it picks up anything. # load package library(stringr) # create a string (string &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\")) # regex to search for strings that start with ab str_match(string, \"^ab.*\") # Similar to the grepl function there is str_detect, and similar to the gsub function there is str_replace str_detect(string, \"[aeiou]\") # tells us T/F if the word contains any of the vowels in the given range. str_replace(x, \"[aeiou]\", \"!\") # we can replace any vowels in the specified range with a ```!``` in their respective words . ",
    "url": "/Data_Manipulation/Regular_Expressions.html#r",
    
    "relUrl": "/Data_Manipulation/Regular_Expressions.html#r"
  },"163": {
    "doc": "Regular Expressions",
    "title": "Regular Expressions",
    "content": " ",
    "url": "/Data_Manipulation/Regular_Expressions.html",
    
    "relUrl": "/Data_Manipulation/Regular_Expressions.html"
  },"164": {
    "doc": "Research Design",
    "title": "Research Design",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/Research_Design.html",
    
    "relUrl": "/Model_Estimation/Research_Design/Research_Design.html"
  },"165": {
    "doc": "Rolling Regression",
    "title": "Rolling Regression",
    "content": "Rolling regressions are one of the simplest models for analysing changing relationships among variables overtime. They use linear regression but allow the data set used to change over time. In most linear regression models, parameters are assumed to be time-invariant and thus should not change overtime. Rolling regressions estimate model parameters using a fixed window of time over the entire data set. A larger sample size, or window, used will result in fewer parameter estimates but use more observations. For more information, see Base on Rolling. ",
    "url": "/Time_Series/Rolling_Regression.html",
    
    "relUrl": "/Time_Series/Rolling_Regression.html"
  },"166": {
    "doc": "Rolling Regression",
    "title": "Keep in Mind",
    "content": ". | When setting the width of your rolling regression you are also creating the starting position of your analysis given that it needs the a window sized amount of data begin. | . ",
    "url": "/Time_Series/Rolling_Regression.html#keep-in-mind",
    
    "relUrl": "/Time_Series/Rolling_Regression.html#keep-in-mind"
  },"167": {
    "doc": "Rolling Regression",
    "title": "Also Consider",
    "content": ". | An expanding window can be used where instead of a constantly changing fixed window, the regression starts with a predetermined time and then continually adds in other observations until the entire data set is used. | . ",
    "url": "/Time_Series/Rolling_Regression.html#also-consider",
    
    "relUrl": "/Time_Series/Rolling_Regression.html#also-consider"
  },"168": {
    "doc": "Rolling Regression",
    "title": "Implementation",
    "content": " ",
    "url": "/Time_Series/Rolling_Regression.html#implementation",
    
    "relUrl": "/Time_Series/Rolling_Regression.html#implementation"
  },"169": {
    "doc": "Rolling Regression",
    "title": "Python",
    "content": "This example will make use of the statsmodels package, and some of the description of rolling regression has benefitted from the documentation of that package. Rolling ordinary least squares applies OLS (ordinary least squares) across a fixed window of observations and then rolls (moves or slides) that window across the data set. The key parameter is window, which determines the number of observations used in each OLS regression. First, let’s import the packages we’ll be using. If you don’t already have these installed, open up your terminal (aka command line) and use the command pip install packagename where ‘packagename’ is the package you want to install. from statsmodels.regression.rolling import RollingOLS import statsmodels.api as sm from sklearn.datasets import make_regression import matplotlib.pyplot as plt import pandas as pd . Next we’ll create some random numbers to do our regression on . X, y = make_regression(n_samples=200, n_features=2, random_state=0, noise=4.0, bias=0) df = pd.DataFrame(X).rename(columns={0: 'feature0', 1: 'feature1'}) df['target'] = y df.head() . | | feature0 | feature1 | target | . | 0 | -0.955945 | -0.345982 | -36.740556 | . | 1 | -1.225436 | 0.844363 | 7.190031 | . | 2 | -0.692050 | 1.536377 | 44.389018 | . | 3 | 0.010500 | 1.785870 | 57.019515 | . | 4 | -0.895467 | 0.386902 | -16.088554 | . Now let’s fit the model using a formula and a window of 25 steps. roll_reg = RollingOLS.from_formula('target ~ feature0 + feature1 -1', window=25, data=df) model = roll_reg.fit() . Note that -1 just suppresses the intercept. We can see the parameters using model.params. Here are the params for time steps 20 to 30: . model.params[20:30] . | | feature0 | feature1 | . | 20 | NaN | NaN | . | 21 | NaN | NaN | . | 22 | NaN | NaN | . | 23 | NaN | NaN | . | 24 | 20.736214 | 35.287604 | . | 25 | 20.351719 | 35.173493 | . | 26 | 20.368027 | 35.095621 | . | 27 | 20.532655 | 34.919468 | . | 28 | 20.470171 | 35.365235 | . | 29 | 20.002261 | 35.666997 | . Note that there aren’t parameters for entries between 0 and 23 because our window is 25 steps wide. We can easily look at how any of the coefficients are changing over time. Here’s an example for ‘feature0’. fig = model.plot_recursive_coefficient(variables=['feature0']) plt.xlabel('Time step') plt.ylabel('Coefficient value') plt.show() . Recursive ordinary least squares (aka expanding window rolling regression) . A rolling regression with an expanding (rather than moving) window is effectively a recursive least squares model. We can perform this kind of estimation using the RecursiveLS function from statsmodels. Let’s fit this to the whole dataset: . reg_rls = sm.RecursiveLS.from_formula( 'target ~ feature0 + feature1 -1', df) model_rls = reg_rls.fit() print(model_rls.summary()) . Statespace Model Results ============================================================================== Dep. Variable: target No. Observations: 200 Model: RecursiveLS Log Likelihood -570.923 Date: Sun, 23 May 2021 R-squared: 0.988 Time: 17:05:03 AIC 1145.847 Sample: 0 BIC 1152.444 - 200 HQIC 1148.516 Covariance Type: nonrobust Scale 17.413 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ feature0 20.6872 0.296 69.927 0.000 20.107 21.267 feature1 34.0655 0.302 112.870 0.000 33.474 34.657 =================================================================================== Ljung-Box (Q): 40.19 Jarque-Bera (JB): 3.93 Prob(Q): 0.46 Prob(JB): 0.14 Heteroskedasticity (H): 1.17 Skew: -0.31 Prob(H) (two-sided): 0.51 Kurtosis: 3.31 =================================================================================== Warnings: [1] Parameters and covariance matrix estimates are RLS estimates conditional on the entire sample. But now we can look back at how the values of the coefficients changed in real time: . fig = model_rls.plot_recursive_coefficient(range(reg_rls.k_exog), legend_loc='upper right') ax_list = fig.axes for ax in ax_list: ax.set_xlim(0, None) ax_list[-1].set_xlabel('Time step') ax_list[0].set_title('Coefficient value'); . ",
    "url": "/Time_Series/Rolling_Regression.html#python",
    
    "relUrl": "/Time_Series/Rolling_Regression.html#python"
  },"170": {
    "doc": "Rolling Regression",
    "title": "R",
    "content": "In R the rollRegres (one s, not two) package can compute rolling regressions while being able to specify the linear regression, window size, whether you want a rolling or expanding window, the minimum number of observations required in a window, and other options. #Load in the package library(rollRegres) . The data will be manually created where x can be interpreted as any independent variable over a fixed time period, and y is an outcome variable. #Simulate data set.seed(29132867) n &lt;- 200 p &lt;- 2 X &lt;- cbind(1, matrix(rnorm(p * n), ncol = p)) y &lt;- drop(X %*% c(1, -1, 1)) + rnorm(n) df_1 &lt;- data.frame(y, X[, -1]) #Run the rolling regression (Rolling window) roll_rolling &lt;- roll_regres(y ~ X1, df_1, width = 25L, do_downdates = TRUE) #Check the first 10 coefficients roll_rolling$coefs %&gt;% tail(25) . ## (Intercept) X1 ## 176 1.467609 -1.1887381 ## 177 1.561271 -1.0356878 ## 178 1.598559 -1.0142755 ## 179 1.615713 -0.8161185 ## 180 1.680253 -0.8814448 ## 181 1.593962 -0.8878947 ## 182 1.623287 -1.0246186 ## 183 1.596735 -1.0535530 ## 184 1.761971 -0.9466231 ## 185 1.703679 -0.8138562 ## 186 1.619849 -0.8948204 ## 187 1.729696 -0.9266218 ## 188 1.606376 -1.0624713 ## 189 1.676579 -0.9823355 ## 190 1.657288 -1.0377997 ## 191 1.579115 -1.1432474 ## 192 1.586334 -1.1829136 ## 193 1.393517 -1.1447650 ## 194 1.250160 -1.0244144 ## 195 1.243020 -1.0232251 ## 196 1.205068 -1.0201693 ## 197 1.273287 -0.9792404 ## 198 1.336927 -0.9346933 ## 199 1.343452 -0.9316855 ## 200 1.284072 -0.9379035 . To demonstrate the point about a starting position for your analysis, the entries up to 24 are null because of the choice of window size. #Check the first 25 coefficients roll_rolling$coefs %&gt;% head(25) . ## (Intercept) X1 ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## 7 NA NA ## 8 NA NA ## 9 NA NA ## 10 NA NA ## 11 NA NA ## 12 NA NA ## 13 NA NA ## 14 NA NA ## 15 NA NA ## 16 NA NA ## 17 NA NA ## 18 NA NA ## 19 NA NA ## 20 NA NA ## 21 NA NA ## 22 NA NA ## 23 NA NA ## 24 NA NA ## 25 1.401621 -1.127908 . Finally, here are the results when using an expanding window (also known as recursive regression) . #Run the rolling regression (Rolling window) roll_expanding &lt;- roll_regres(y ~ X1, df_1, width = 25L,do_downdates = FALSE) #Check the last 10 coefficients roll_expanding$coefs %&gt;% tail(10) . ## (Intercept) X1 ## 191 1.189651 -1.066285 ## 192 1.196722 -1.077508 ## 193 1.180968 -1.068470 ## 194 1.178508 -1.064603 ## 195 1.177739 -1.064419 ## 196 1.170369 -1.063868 ## 197 1.172466 -1.064678 ## 198 1.179917 -1.056945 ## 199 1.181876 -1.056017 ## 200 1.178556 -1.054890 . ",
    "url": "/Time_Series/Rolling_Regression.html#r",
    
    "relUrl": "/Time_Series/Rolling_Regression.html#r"
  },"171": {
    "doc": "Scatterplots",
    "title": "Introduction",
    "content": "Scatterplots are a useful tool for visualizing data and the possible relationships present in that data. This introduction contains a brief tutorial on how to implement scatterplots, as well as some basic techniques for formatting them. ",
    "url": "/Presentation/Figures/Scatterplots.html#introduction",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html#introduction"
  },"172": {
    "doc": "Scatterplots",
    "title": "Keep in Mind",
    "content": ". | Scatterplots can become cluttered if there are too many datapoints or if datapoints are too large. | . ",
    "url": "/Presentation/Figures/Scatterplots.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html#keep-in-mind"
  },"173": {
    "doc": "Scatterplots",
    "title": "Also Consider",
    "content": ". | Scatterplots are not always the best presentation method for data. | If the independent variable that is being presented is categorical or discrete, a bar graph might be a better presentation method. A guide to implementing bar graphs can be found here. | If the goal is to represent distributions of continuous variables, a histogram would be a good option for presentation. | . | Additional techniques for formatting scatterplots can by found here. | . ",
    "url": "/Presentation/Figures/Scatterplots.html#also-consider",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html#also-consider"
  },"174": {
    "doc": "Scatterplots",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/Scatterplots.html#implementations",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html#implementations"
  },"175": {
    "doc": "Scatterplots",
    "title": "Python",
    "content": "There are many plotting libraries in Python, covering both imperative (specify all of the steps to get the desired outcome) and declarative (specify the desired outcome without the steps) approaches. Imperative plotting gives more control and some people may find each step clearer to read, but it can also be fiddly and cumbersome, especially with simple plots. Declarative plotting trades away control in favour of tried and tested processes that can quickly produce standardised charts, but the specialised syntax can be a barrier for newcomers. The code below shows examples of scatterplots using both methods using the declarative library seaborn, drawing from the packages’ website. For imperative scatter plots, use matplotlib. As usual with Python, you may need to install seaborn using pip install seaborn or conda install seaborn on the command line. import seaborn as sns # Load the tips dataset df = sns.load_dataset(\"tips\") # Plot the data. hue sets the colour of points. # alpha sets the transparency of points. There # are various other keyword arguments to add other # dimensions of information too, eg size. sns.scatterplot(data=df, x=\"total_bill\", y=\"tip\", alpha=.8, hue='time').set_title('Tips data', loc='right') . This results in: . Alternatively, you can also use the seaborn.objects library which is more built around the grammar of graphics, by layering objects. import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset df = load_dataset(\"tips\") \"\"\" Below we'll make a simple scatter plot with a linear trendline fit to the scatter using seaborn.objects Comments are provided on each line to help call out what each line is doing \"\"\" # Initialize the plot instance plot = (so.Plot(data=df, x='total_bill', y='tip', color='time') #General aesthetic variables .add(so.Dots(pointsize=5)) # Layer dots (scatter) aesthetic with size 5 for the dots .add(so.Line(), so.PolyFit(order=1)) # Layer a trendline with order 1 (linear) .label(title='Total Bill Due vs Tip Amount (by Time)', # Manually set the labels x='Total Bill Due (in dollars)', y='Tip Amount (in dollars)', color='Time') .theme({\"axes.facecolor\": \"w\"}) # Remove spines and make the entire chart area white ) plot.plot() #Call the plot . This results in: . ",
    "url": "/Presentation/Figures/Scatterplots.html#python",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html#python"
  },"176": {
    "doc": "Scatterplots",
    "title": "R",
    "content": "In R, one of the best tools for creating scatterplots is the function ggplot(), found in the ggplot2 package. For this demonstration, we will also be using a dataset already built in to R called mtcars. To begin we will need to make sure we install and load ggplot2 as well as any other packages that are useful. #install and load necessary packages library(ggplot2) #load the dataset data(mtcars) . Next, we will use ggplot(), aes(), and geom_point() in order to create a basic scatterplot. For this plot, we will put car weight on the x-axis and miles-per-gallon on the y-axis. #assign the mtcars dataset to the plot and set each axis ggplot(data = mtcars,aes(x=wt,y=mpg)) + #create points on the plot for each observation geom_point() . It is important to remember to include the + after each line when creating a plot using ggplot(). This + tells R that the lines of code belong together and omitting it will lead to our plot not having important parts. Labelling is also an important task. In order to give our scatterplot axis labels and title, we will use the labs() function, in conjunction with our previous code. Don’t forget your +’s! . #assign our dataset and variables of interest to the plot ggplot(data = mtcars, aes(x = wt, y = mpg)) + #create the points geom_point() + #create axis labels and a title labs(x = \"Weight\", y = \"Miles Per Gallon\", title = \"Car MPG by Weight\") . That is starting to look better, but our graph could still use a little variety to it. Next, we will learn how to change the size and color of our plot points. | To change the size of the points in our scatterplot, we need to use the option size. The default size of points in ggplot is 1.5. We’re going to make the points 4, just in case someone is having trouble seeing them. | To change the color of our points, we will use color. In this example we will make our points blue. | . #assign our dataset and variables of interest ggplot(data = mtcars, aes(x =wt, y = mpg)) + #create points and tell ggplot we want them to be size 4 and blue geom_point(size = 4, color = \"blue\") + #don't forget the labels labs(x = \"Weight\", y = \"Miles Per Gallon\", title = \"Car MPG by Weight\") . Finally, lets label our points. We can do this by adding a new element to our plot, geom_text(). For this example we will label the points on our plot with their horse power. This will allow us to see how horsepower is related to weight and miles-per-gallon. We are also going to set the size of our points to 0.5 to avoid cluttering the scatterplot too much. Just like we can change the color of our points, we can change the color of the labels we put on them. We’ll make them red in this example, but feel free to choose another color. #assign our dataset and variables of interest ggplot(data = mtcars, aes(x =wt, y = mpg)) + #create points and tell ggplot we want them to be size 0.5 and blue geom_point(size = 0.5, color = 'blue') + #add the labels for our points geom_text(label = mtcars$hp, color = 'red') #don't forget the labels labs(x = \"Weight\", y = \"Miles Per Gallon\", title = \"Car MPG by Weight\") . Congrats! You’re well on your way to becoming a scatterplot master! Don’t forget to check out the LOST page on styling scatterplots if you would like to learn more. ",
    "url": "/Presentation/Figures/Scatterplots.html#r",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html#r"
  },"177": {
    "doc": "Scatterplots",
    "title": "Scatterplots",
    "content": " ",
    "url": "/Presentation/Figures/Scatterplots.html",
    
    "relUrl": "/Presentation/Figures/Scatterplots.html"
  },"178": {
    "doc": "State Space Models",
    "title": "Linear Gaussian State Space Models",
    "content": "The state space model can be used to represent a variety of dynamic processes, including standard ARMA processes. It has two main components: (1) a hidden/latent \\(x_t\\) process referred to as the state process, and (2) an observed process \\(y_t\\) that is independent conditional on \\(x_t\\). Let us consider the most basic state space model – the linear Gaussian model – in which \\(x_t\\) follows a linear autoregressive process and \\(y_t\\) is a linear mapping of \\(x_t\\) with added noise. The linear Gaussian state space model is characterized by the following state equation: . \\[x_{t+1} = F \\, x_{t} + u_{t+1} \\, ,\\] where \\(x_t\\) and \\(u_t\\) are both \\(p \\times 1\\) vectors, such that \\(u_t \\sim i.i.d. N(0,Q)\\). It is assumed that the initial state vector \\(x_0\\) is drawn from a normal distribution. The observation equation is expressed as . \\[y_t = A_t \\, x_t + v_t \\, ,\\] where \\(y_t\\) is a \\(q \\times 1\\) observed vector, \\(A_t\\) is a \\(q \\times p\\) observation matrix, and \\(v_t \\sim i.i.d. N(0,R)\\) is a \\(q \\times 1\\) noise vector. For additional information about the state-space representation, refer to Wikipedia: State-Space Representation. ",
    "url": "/Time_Series/State_Space_Models.html#linear-gaussian-state-space-models",
    
    "relUrl": "/Time_Series/State_Space_Models.html#linear-gaussian-state-space-models"
  },"179": {
    "doc": "State Space Models",
    "title": "Keep in Mind",
    "content": ". | Expressing a dynamic process in state-space form allows us to apply the Kalman filter and smoother. | The parameters of a linear Gaussian state space model can be estimated using a maximum likelihood approach. This is made possible by the fact that the innovation vectors \\(u_t\\) and \\(v_t\\) are assumed to be multivariate standard normal. The Kalman filter can be used to construct the likelihood function, which can be transformed into a log-likelihood function and simply optimized with respect to the parameters. | If the innovations are assumed to be non-Gaussian, then we may still apply the maximum likelihood procedure to yield quasi-maximum likelihood parameter estimates that are consistent and asymptotically normal. | Unless appropriate restrictions are placed on the parameter matrices, the parameter matrices obtained from the above-mentioned optimization procedure will not be unique. In other words, in the absence of restrictions, the parameters of the state space model are unidentified. | The Kalman filter can be used to recursively generate forecasts of the state vector within a sample period given information up to time \\(t \\in \\{t_0,\\ldots,T\\}\\), where \\(t_0\\) and \\(T\\) represent the initial and final periods of a sample, respectively. | The Kalman smoother can be used to generate historical estimates of the state vector throughout the entire sample period given all available information in the sample (information up to time \\(T\\)). | . ",
    "url": "/Time_Series/State_Space_Models.html#keep-in-mind",
    
    "relUrl": "/Time_Series/State_Space_Models.html#keep-in-mind"
  },"180": {
    "doc": "State Space Models",
    "title": "Also Consider",
    "content": ". | Recall that a stationary ARMA process can be expressed as a state space model. This may not be necessary, however, unless the given data has missing observations. If there are no missing data, then one can defer to the standard method of estimating ARMA models described on the ARMA page. | . ",
    "url": "/Time_Series/State_Space_Models.html#also-consider",
    
    "relUrl": "/Time_Series/State_Space_Models.html#also-consider"
  },"181": {
    "doc": "State Space Models",
    "title": "Implementations",
    "content": "First, follow the instructions for creating and formatting time-series data using your software of choice. We will again use quarterly US GDP data downloaded from FRED as an example. We estimate the quarterly log change in GDP using an ARMA(3,1) model in state space form to follow the ARMA implementation. An ARMA(\\(p,q\\)) process . \\[y_t = c + \\sum_{i = 1}^{3} \\phi_i Y_{t-i} + \\sum_{j = 1}^{1} \\theta_j \\varepsilon_{t-j} + \\varepsilon_t\\] may be expressed in state-space form in a variety of ways – the following is an example of a common parsimonious approach. The state equation (also referred to as the transition equation) may be expressed as . \\[\\begin{bmatrix} y_t \\\\ y_{t-1} \\\\ y_{t-2} \\\\ \\varepsilon_t \\end{bmatrix} = \\begin{bmatrix} c \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\phi_1 &amp; \\phi_2 &amp; \\phi_3 &amp; \\theta \\\\ 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\, \\begin{bmatrix} y_{t-1} \\\\ y_{t-2} \\\\ y_{t-3} \\\\ \\varepsilon_{t-1} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_t \\\\ 0 \\\\ 0 \\\\ \\varepsilon_t \\end{bmatrix} \\, ,\\] while the observation equation (also referred to as the measurement equation) may be expressed as \\(y_t = \\begin{bmatrix} 1&amp;0&amp;0&amp;0 \\end{bmatrix} \\, \\begin{bmatrix} y_t \\\\ y_{t-1} \\\\ y_{t-2} \\\\ \\varepsilon_t \\end{bmatrix} \\, .\\) . The observation matrix \\(A_t\\) in our implementation will be time-invariant (\\(A_t = A, \\forall t\\)). ",
    "url": "/Time_Series/State_Space_Models.html#implementations",
    
    "relUrl": "/Time_Series/State_Space_Models.html#implementations"
  },"182": {
    "doc": "State Space Models",
    "title": "R",
    "content": "The following R implementation relies on the dlm package to build an ARMA(3,1) model and express it in state-space form. The dlm package is used to work with dynamic linear models (DLMs), which is an alternative name to linear state space models. To learn more about the package, check out its CRAN page, as well as this vignette written by the author of the package. First the tsibble and dlm packages are installed and loaded. Then US quarterly GDP data is loaded and log-differentiated in the same exact way as shown on the ARMA page. Then an ARMA(3,1) model is built and implicitly put in state-space form using the dlm package, after which it is fit to the loaded series using a maximum likelihood approach. The parameters of the estimated model are stored in mod, and the estimated observation and state error matrices are also stored in obs.error.var and state.error.var, respectively. Lastly, both the Kalman filter and smoother are applied to the model, the results of which are stored in the filtered and smoothed objects, respectively. ## Install and load time series packages if (!require(\"tsibble\")) install.packages(\"tsibble\") library(tsibble) if (!require(\"dlm\")) install.packages(\"dlm\") library(dlm) # Prepare the data ## Load data gdp = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") ## Set our data up as a time-series gdp$DATE &lt;- as.Date(gdp$DATE) gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) ## Construct our first difference of log gdp variable gdp_ts$lgdp=log(gdp_ts$GDPC1) gdp_ts$ldiffgdp=difference(gdp_ts$lgdp, lag=1, difference=1) # Estimate ARMA(3,1) using the above data ## Define log-diff gdp as vector y y &lt;- gdp_ts$ldiffgdp ## Build ARMA(3,1) model fn &lt;- function(parm) { dlmModARMA(ar = c(parm[1], parm[2], parm[3]), ma = parm[4], sigma2 = parm[5]) } ## Fit the model to the data fit &lt;- dlmMLE(y, c(rep(0, 4),1), build = fn, hessian = TRUE) (conv &lt;- fit$convergence) ## Store var-cov stats mod &lt;- fn(fit$par) obs.error.var &lt;- V(mod) state.error.var &lt;- W(mod) # Apply the Kalman filter filtered &lt;- dlmFilter(y, mod = mod) # Apply the Kalman smoother smoothed &lt;- dlmSmooth(filtered) . ",
    "url": "/Time_Series/State_Space_Models.html#r",
    
    "relUrl": "/Time_Series/State_Space_Models.html#r"
  },"183": {
    "doc": "State Space Models",
    "title": "State Space Models",
    "content": " ",
    "url": "/Time_Series/State_Space_Models.html",
    
    "relUrl": "/Time_Series/State_Space_Models.html"
  },"184": {
    "doc": "Statistical Inference",
    "title": "Statistical Inference",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Statistical_Inference.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Statistical_Inference.html"
  },"185": {
    "doc": "Styling Scatterplots",
    "title": "Introduction",
    "content": "A scatterplot is a useful and straightforward way to visualize the relationship between two variables,eventually revealing a correlation. It is often used to make initial diagnoses before any other statistical analyses are conducted.This tutorial will not only teach you how to make scatterplots, but also explore the ways to help you design your own styling scatterplots. ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#introduction",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#introduction"
  },"186": {
    "doc": "Styling Scatterplots",
    "title": "Keep in Mind",
    "content": ". | REMEMBER always clean your dataset before you try to make scatterplots since in the real world, the dataset is always messier than the iris dataset used below. | Scatterplots may not work well if the variables that you are interested in are discrete, or if there are a large number of data points. | Be more careful if you have Date (which is time-series data) as your x-variable, Date can be very tricky in many ways. | . ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#keep-in-mind"
  },"187": {
    "doc": "Styling Scatterplots",
    "title": "Also Consider",
    "content": ". | If one of your variables is discrete, then instead of scatterplots, you may want to check how to make bar graphs here. | . Specifically in R: . | Formatting graph legends is important for styling scatterplots. So check here if you want to work with graph legends. | If you are working with time series visualization with ggplot2 package, see here for more help. | Check here for more data visualization with ggplot2 package. | . ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#also-consider",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#also-consider"
  },"188": {
    "doc": "Styling Scatterplots",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#implementations",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#implementations"
  },"189": {
    "doc": "Styling Scatterplots",
    "title": "R",
    "content": "For this R demonstration, we will introduce how to use ggplot2 package to create nice scatterplots. First, we load all the libraries we will need. library(ggplot2) library(viridis) library(dplyr) library(RColorBrewer) library(tidyverse) library(ggthemes) library(ggpubr) . Step 1: Basic Scatterplot . Let’s start with the basic scatterplot. Say we want to check the relationship between Sepal width and Sepal length of the iris species. There are a few steps to construct the scatterplot: . | Step1: specify the dataset that we want to visualize | Step2: tell which variable to show on x and y axis | Step3: add a geom_point() in order to show the points | . If you have questions about how to use ggplot and aes, check Here for more help. ggplot(data = iris, aes( ## Put Sepal.Length on the x-axis, Sepal.Width on the y-axis x=Sepal.Length, y=Sepal.Width))+ ## Make it a scatterplot with geom_point() geom_point() . Step 2: Map a variable to marker feature . One of the most powerful and magic abilities of the ggplot2 package is to map a variable to marker features. Notice that attributes set outside of aes() apply to all points (like size=4 here), while attributes set inside of aes() set the attribute separately for the values of the variable. Transparency . We can distinguish the Species by alpha (transparency). ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, ## Where transparency comes in alpha=Species)) + geom_point(size =4, color=\"seagreen\") . Shape . shape is also a common way to help us to see relationship between two variables within different groups. Additionally, you can always change the shape of the points. Check here for more ideas. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, ## Where shape comes in shape=Species)) + geom_point(size = 4,color=\"orange\") . Size . size is a great option that we can take a look at as well. However, note that size will work better with continuous variables. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, ## Where size comes in size=Species)) + geom_point(shape = 18, color = \"#FC4E07\") . Color . Last but not least, let’s color these points depends on the variable Species in the iris dataset. ## First, we need to make sure that 'Species' is a factor variable ## class(iris$Species) ## Since 'Species' is already a factor variable, we do not need to do conversion ## However, in case 'Species' is not a factor variable, we can solve this question using as.factor() function, like below ## iris$Species &lt;- as.factor(iris$Species) ## Then, we are ready to plot ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, ## distinguish the species by color color=Species))+ geom_point() . | Note . | If you do not like the default colors in the ggplot2, there are a couple of ways to change that.The RColorBrewerpackage will definitely help. If you want to know more about RColorBrewer package,see here. Additionally,the viridis package is also very helpful to change the default colors. For more information of the viridis package, check here. | If you do not like all the options that the RColorBrewer and viridis packages provide, see here to work with color in the ggplot2 package. | . | . ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()+ ## Where RColorBrewer package comes in scale_colour_brewer(palette = \"Set1\") ## There are more options available for palette ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()+ ## Where viridis package comes in scale_color_viridis(discrete=TRUE,option = \"D\") ## There are more options to choose . This first graph is using RColorBrewer package,and the second graph is using viridis package. Put all the options together . Of course, we can always mix color,transparency,shape and size together to get prettier plot. Simply set more than one of them in aes()! . Step 3: Find the comfortable themes . The next step that we can do is to figure out what the most fittable themes to match all the hard work we have done above. Themes from **ggplot2 package** . In fact, ggplot2 package has many cool themes available alreay such as theme_classic(), theme_minimal() and theme_bw(). Another famous theme is the dark theme: theme_dark(). Let’s check out some of them. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + theme_minimal(base_size = 12) . Themes from the ggthemes package . ggthemes package is also worth to check out for working any plots (maps,time-series data, and any other plots) that you are working on. theme_gdocs(), theme_tufte(), and theme_calc() all work very well. See here to get more cool themes. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + ## Using the theme_tufte() theme_tufte() . Create by your own . If you do not like themes that ggplot2 and ggthemes packages have, don’t worry. You can always create your own style for your themes. Check here to desgin your own unique style. Step 4: Play with labels . It is time to label all the useful information to make the plot be clear to your audiences. Basic Labelling . Both labs() and ggtitle() are great tools to deal with labelling information. In the following code, we provide the example how to use labs() to label the all the things that we need. Take a look here if you want to learn how to use ggtitle(). ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + theme_minimal(base_size = 12)+ ## Where the labelling comes in labs( ## Tell people what x and y variables are x=\"Sepal Length\", y=\"Sepal Width\", ## Title of the plot title = \"Sepal length vs. Sepal width\", subtitle = \" plot within different Iris Species\" ) . Postion and Appearance . After the basic labelling, we want to make them nicer by playing around the postion and appearance (text size, color and faces). ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + labs( x=\"Sepal Length\", y=\"Sepal Width\", title = \"Sepal length vs. Sepal width\", subtitle = \"plot within different Iris Species\" )+ theme_minimal(base_size = 12) + ## Change the title and subtitle position to the center theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+ ## Change the appearance of the title and subtitle theme (plot.title = element_text(color = \"black\", size = 14, face = \"bold\"), plot.subtitle = element_text(color = \"grey40\",size = 10, face = 'italic') ) . Step 5: Show some patterns . After done with step 4, you should end with a very neat and unquie plot. Let’s end up with this tutorial by checking whether there are some specific patterns in our dataset. Linear Trend . According to the plot, it seems like there exists a linear relationship between sepal length and sepal width. Thus, let’s add a linear trend to our scattplot to help readers see the pattern more directly using geom_smooth(). Note that the method argument in geom_smooth() allows to apply different smoothing method like glm, loess and more. See the doc for more. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + labs( x=\"Sepal Length\", y=\"Sepal Width\", title = \"Sepal length vs. Sepal width\", subtitle = \"plot within different Iris Species\" )+ theme_minimal(base_size = 12) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+ theme (plot.title = element_text(color = \"black\", size = 14, face = \"bold\"), plot.subtitle = element_text(color = \"grey40\",size = 10, face = 'italic')) + ## Where linear trend + confidence interval come in geom_smooth(method = 'lm',se=TRUE) . Congratulations!!! You just make your own style of scatterplots if you are following all the steps above and try to play around the different options. ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#r",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#r"
  },"190": {
    "doc": "Styling Scatterplots",
    "title": "Stata",
    "content": "For this Stata demonstration, I will use a combination of scatter and twoway, both native commands in Stata, to create all the figures trying to emulate the structure you see above in R. While I want to use only official commands within Stata, I will use Ben Jann’s grstyle to set some basic graph themes, although I’ll keep them minimalistic. see help grstyle for more options. Setup . To replicate all figures here, you will need to make sure you have grstyle installed in your computer. To make things comparable to the R example, I will also use the Iris dataset. Other than that, I use the following setup: . ssc install grstyle grstyle init grstyle color background white grstyle set legend , nobox webuse iris, clear . Basic Scatterplot . Let’s start with the basic scatterplot. Say we want to check the relationship between Sepal width and Sepal length of the iris species. Basic scatterplots can be obtained using the command scatter. The general syntax is as follows: . scatter yvar1 [yvar2 yvar3 ...] xvar, [options] . You can choose to use one or more variables that will be measured in the vertical axis yvar. The last variable xvar will be used for the horizontal axis. For the example, I will plot only two variables: Sepal width and Sepal length. scatter sepwid seplen . Scatterplot by groups . Something you may want to do when producing Scatterplots is to visually separate different groups within the same scatterplot. For example, in the Iris data, we would like to see how sepal dimensions change by Iris type. To be able to do this, you need to use twoway to overlap multiple graphs together. There are two ways to create multiple overalping plots. The easier one is this: . twoway scatter sepwid seplen if iris==1 || /// scatter sepwid seplen if iris==2 || /// scatter sepwid seplen if iris==3 . | Where each subplot (by iris type) is separated using * |   | . The tripple forward slash */// is used to break the line, and avoid code that is too long to follow. | . The second option, my preferred option, is to separate each subplot, using parenthesis to encapsulate each subplot: . twoway (scatter sepwid seplen if iris==1) /// (scatter sepwid seplen if iris==2) /// (scatter sepwid seplen if iris==3) . This is convenient because allows you to add and differentiate options that affect a subplot, vs options that affect the whole twoway plot. One more thing. The basic plot (as above) differentiates each plot by color, but uses generic labels for each subgroup. So lets add labels for the three types of Irises. twoway (scatter sepwid seplen if iris==1) /// (scatter sepwid seplen if iris==2) /// (scatter sepwid seplen if iris==3), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . Transparency . Starting in Stata 15, it is possible to add transparency to a figure. See that this is done using the option `color()’. For fun, Im using the same color on each subgroup: “forest_green”. twoway (scatter sepwid seplen if iris==1, color(forest_green%10)) /// (scatter sepwid seplen if iris==2, color(forest_green%40)) /// (scatter sepwid seplen if iris==3, color(forest_green%80)), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . Shape/symbols . In Stata, you will use the word symbol rather than shape to differentiate the markers in a scatter plot. Symbols can be modified using the option symbol(). To see all options for symbols, you can type palette symbol. twoway (scatter sepwid seplen if iris==1, color(gold) symbol(O)) /// (scatter sepwid seplen if iris==2, color(gold) symbol(T)) /// (scatter sepwid seplen if iris==3, color(gold) symbol(S)), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . Size . Size of a marker can be modified using the option size(). See help markersizestyle for all available size options. twoway (scatter sepwid seplen if iris==1, color(red) msize(small)) /// (scatter sepwid seplen if iris==2, color(red) msize(medium)) /// (scatter sepwid seplen if iris==3, color(red) msize(large)), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . Color . This is the first option I applied earlier. However, this is a good opportunity to point out all the color options Stata has. See help colorstyle##colorstyle for options. In the example below I use the RBG approach to choose colors. twoway (scatter sepwid seplen if iris==1, color(\"240 120 140\") ) /// (scatter sepwid seplen if iris==2, color(\"100 190 150\") ) /// (scatter sepwid seplen if iris==3, color(\"125 190 230\") ), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . Labels, Titles and Subtitles . I mentioned this earlier. Stata uses a variable label for the plot axis titles. However, you can modify that using the options xtitle() and ytitle(). It is also possible to add a title and subtitle to the figure, using options title() and subtitle() . twoway (scatter sepwid seplen if iris==1, color(\"72 27 109\") symbol(O)) /// (scatter sepwid seplen if iris==2, color(\"33 144 140\") symbol(T)) /// (scatter sepwid seplen if iris==3, color(\"253 231 37\") symbol(s)), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\") col(3)) /// title(Sepal length vs Sepal width) subtitle(plot within different Iris Species) . Showing some patterns . Something else you may want to do is add bivarite fitted lines to emphasize particular relationships within pairs of variables. You can do this by adding additional subplots that will produce this information. In the example below, I add linear fitted values for all cases. I make sure to use the same line color, so they are consistent with the scatter plot. twoway (scatter sepwid seplen if iris==1, color(\"72 27 109\") symbol(O)) /// (scatter sepwid seplen if iris==2, color(\"33 144 140\") symbol(T)) /// (scatter sepwid seplen if iris==3, color(\"253 231 37\") symbol(s)) /// (lfitci sepwid seplen if iris==1, clcolor(\"72 27 109\") clwidth(0.5) acolor(%50) ) /// (lfitci sepwid seplen if iris==2, clcolor(\"33 144 140\") clwidth(0.5) acolor(%50) ) /// (lfitci sepwid seplen if iris==3, clcolor(\"253 231 37\") clwidth(0.5) acolor(%50) ), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\") col(3)) /// title(Sepal length vs Sepal width) subtitle(plot within different Iris Species) /// xtitle(Sepal length in cm) ytitle(Sepal width in cm) . Of course, you can be more sophisticated, and use a local polynomial to identify those relationships: . twoway (scatter sepwid seplen if iris==1, color(\"72 27 109\") symbol(O)) /// (scatter sepwid seplen if iris==2, color(\"33 144 140\") symbol(T)) /// (scatter sepwid seplen if iris==3, color(\"253 231 37\") symbol(s)) /// (lpolyci sepwid seplen if iris==1, clcolor(\"72 27 109\") clwidth(0.5) acolor(%50) ) /// (lpolyci sepwid seplen if iris==2, clcolor(\"33 144 140\") clwidth(0.5) acolor(%50) ) /// (lpolyci sepwid seplen if iris==3, clcolor(\"253 231 37\") clwidth(0.5) acolor(%50) ), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\") col(3)) /// title(Sepal length vs Sepal width) subtitle(plot within different Iris Species) /// xtitle(Sepal length in cm) ytitle(Sepal width in cm) . And done. You can use the above guide to modify your plots as needed. ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#stata",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#stata"
  },"191": {
    "doc": "Styling Scatterplots",
    "title": "Styling Scatterplots",
    "content": " ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html",
    
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html"
  },"192": {
    "doc": "Summary Statistics Tables",
    "title": "Summary Statistics Tables",
    "content": "Before looking at relationships between variables, it is generally a good idea to show a reader what the distributions of individual variables look like. A common way to do this, which allows you to show information about many variables at once, is a “Summary statistics table” or “descriptive statistics table” in which each row is one variable in your data, and the columns include things like number of observations, mean, median, standard deviation, and range. ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html",
    
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html"
  },"193": {
    "doc": "Summary Statistics Tables",
    "title": "Keep in Mind",
    "content": ". | Make sure that you are using the appropriate summary measures for the variables that you have. For example, if you have a variable indicating the country someone is from coded as that country’s international calling code, don’t include it in a table that reports the mean - you’d get an answer but that answer wouldn’t make any sense. | If you have categorical variables, you can generally still incorporate them into a summary statistics table by turning them into binary “dummy” variables. | . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#keep-in-mind",
    
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#keep-in-mind"
  },"194": {
    "doc": "Summary Statistics Tables",
    "title": "Also Consider",
    "content": ". | Graphs can be more informative ways of showing the distribution of a variable, and you may want to show a graph of your variable’s distribution in addition to its inclusion on a summary statistics table. There are many ways to do this, but two common ones are density plots or histograms for continuous variables, or bar plots for categorical variables. | . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#also-consider",
    
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#also-consider"
  },"195": {
    "doc": "Summary Statistics Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#implementations",
    
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#implementations"
  },"196": {
    "doc": "Summary Statistics Tables",
    "title": "R",
    "content": "Probably the most straightforward and simplest way to do a summary statistics table in R is with the sumtable function in the vtable package, which also has many options for customization. There are also other options like stargazer in stargazer, dfsummary() in summarytools, summary_table() in qwraps2 or table1() in table1. See this page for a comparison of different packages. # If necessary # install.packages('vtable') library(vtable) data(mtcars) # Feed sumtable a data.frame with the variables you want summarized mt_tosum &lt;- mtcars[,c('mpg','cyl','disp')] # By default, the table shows up in the Viewer pane (in RStudio) or your browser (otherwise) # (or if being run inside of RMarkdown, in the RMarkdown document format) sumtable(mt_tosum) # st() as a shortcut also works st(mt_tosum) # There are *many* options and customizations. For all of them, see # help(sumtable) # Some useful ones include out, which designates a file to send the table to # (note that HTML tables can be copied straight into Word from an output file) sumtable(mt_tosum, out = 'html', file = 'my_summary.html') # sumtable will handle factor variables as expected, # and you can replace variable names with \"labels\" mt_tosum$trans &lt;- factor(mtcars$am, labels = c('Manual','Automatic')) st(mt_tosum, labels = c('Miles per Gallon','Cylinders','Displacement','Transmission')) # Use group to get summary statistics by group st(mt_tosum, labels = c('Miles per Gallon','Cylinders','Displacement'), group = 'trans') . Another good option is the package skimr, which is an excellent alternative to base::summary(). skimr::skim() takes different data types and outputs a summary statistic data frame. Numeric data gets miniature histograms and all types of data get information about the number of missing entries. # If necessary # install.packages('dplyr') # install.packages('skimr') library(dplyr) library(skimr) skim(starwars) #If you're wondering which columns have missing values, you can use skim() in a pipeline. starwars %&gt;% skim() %&gt;% dplyr::filter(n_missing &gt; 0) %&gt;% dplyr::select(skim_variable, n_missing, complete_rate) #You can analyze grouped data with skimr. You can also easily customize the output table using skim_with(). my_skim &lt;- skim_with(base = sfl( n = length )) starwars %&gt;% group_by(species) %&gt;% my_skim() %&gt;% dplyr::filter(skim_variable == \"height\" &amp; n &gt; 1) . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#r",
    
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#r"
  },"197": {
    "doc": "Summary Statistics Tables",
    "title": "Stata",
    "content": "The built-in Stata command summarize (which can be referred to in short as su or summ) easily creates summary statistics tables. However, while summarize is well-suited for viewing descriptive statistics on your own, it is not well-suited for making tables to publish in a paper, since it is difficult to limit the number of significant digits, and does not offer an easy way to export the table other than selecting the Stata output, selecting “Copy Table”, and pasting into a spreadsheet. For more flexible tables that can be easily exported, we will be using the highly flexible estout package. For more information on the many different options and specifications for estout summary tables, see this page. We will also see how to use outreg2 in the outreg2 package, which is less flexible but is slightly less work to use for standard tables that are basically summarize but nicer-looking and output to a file. * If necessary * ssc install estout * ssc install outreg2 sysuse auto.dta, clear * summarize will give us a table that is great for our own purposes, not so much for exporting summarize price mpg rep78 i.foreign * Instead using estpost summarize will give us an esttab-compatible table * Note that factor variables no longer work - we must make dummies by hand xi i.foreign, pre(f_) noomit estpost summarize price mpg rep78 f_* * We can then use esttab and cells() to pick columns * Now it's nicely formatted * The quotes around the statistics put all the statistics in one row esttab, cells(\"count mean sd min max\") * If we want to limit the number of significant digits we must do this stat by stat * Using a standard format option (see help format) esttab, cells(\"count mean(fmt(%9.2f)) sd(fmt(%9.2f)) min max\") * And write out to file with \"using\" esttab using mytable.rtf, cells(\"count mean(fmt(%9.2f)) sd(fmt(%9.2f)) min max\") replace * Or we can work with outreg2 * First, limit the data to the variables we want to summarize preserve keep price mpg rep78 f_* * Then outreg2 with the sum(log) option to get summary statistics outreg2 using myoutreg2table.doc, word sum(log) replace * Defaults are very similar to what you'd get with summarize, but you can do things like change * number of significant digits with dec(), or which stats are in there with eqkeep() outreg2 using mysmalltable.doc, word sum(log) eqkeep(N mean) dec(3) replace restore . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#stata",
    
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#stata"
  },"198": {
    "doc": "Tables",
    "title": "Tables",
    "content": " ",
    "url": "/Presentation/Tables/Tables.html",
    
    "relUrl": "/Presentation/Tables/Tables.html"
  },"199": {
    "doc": "Time Series",
    "title": "Time Series",
    "content": " ",
    "url": "/Time_Series/Time_Series.html",
    
    "relUrl": "/Time_Series/Time_Series.html"
  },"200": {
    "doc": "VAR Models",
    "title": "Vector Autoregression (VAR) Models",
    "content": "A vector autoregression (VAR) of order \\(p\\), often abbreviated as VAR(\\(p\\)), is the following data-generating process (DGP): . \\[y_t = \\upsilon + A_1 y_{t-1} + \\ldots + A_p y_{t-p} + u_t \\, ,\\] for \\(t = 0, 1, 2, \\ldots\\), where \\(y_t = (y_{1t}, \\ldots, y_{Kt})'\\) is a (\\(K \\times 1\\)) random vector of observed data, the \\(A_i\\) are fixed (\\(K \\times K\\)) coefficient matrices, \\(\\upsilon = (\\upsilon_1 , \\ldots , \\upsilon_K)'\\) is a fixed (\\(K \\times 1\\)) vector of intercept terms, and \\(u_t = (u_{1t} , \\ldots , u_{Kt})'\\) is a \\(K\\)-dimensional innovation process with \\(E(u_t) = 0\\), \\(E(u_t u_t') = \\Sigma_u\\), and \\(E(u_t u_s') = 0\\) for \\(s \\neq t\\). Simply put, a VAR(\\(p\\)) is a model of the DGP underlying some random data vector \\(y_t\\) for all \\(t\\) as a function of \\(1, \\ldots , p\\) of its own lags, along with identically and independently distributed (iid) innovations. Any given VAR(\\(p\\)) process has an equivalent VAR(1) representation: . \\[Y_t = \\boldsymbol{\\upsilon} + \\boldsymbol{A} Y_{t-1} + U_t \\, ,\\] where . \\[Y_t = \\begin{bmatrix} y_t \\\\ y_{t-1} \\\\ \\vdots \\\\ y_{t-p+1} \\end{bmatrix} \\, ,\\] \\[\\boldsymbol{\\upsilon} = \\begin{bmatrix} \\upsilon \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\, ,\\] \\[A = \\begin{bmatrix} A_1 &amp; A_2 &amp; \\ldots &amp; A_{p-1} &amp; A_p \\\\ I_K &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; I_K &amp; &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; I_K &amp; 0 \\end{bmatrix} \\, ,\\] and . \\[U_t = \\begin{bmatrix} u_t \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\, .\\] By the above ubiquitous formulation, any given VAR(\\(p\\)) is stable if \\(\\text{det}(I_{Kp} - \\boldsymbol{A}z) \\neq 0\\) for \\(|z| \\leq 1\\). In other words, if all eigenvalues of \\(\\boldsymbol{A}\\) live within the complex unit circle, we may express the VAR(1) model as . \\[Y_t = \\boldsymbol{\\mu} + \\sum_{i=0}^\\infty \\boldsymbol{A}^i U_{t-i} \\, ,\\] where \\(\\boldsymbol{\\mu} = E(Y_t) = (I_{Kp} - \\boldsymbol{A})^{-1} \\boldsymbol{\\upsilon}\\), \\(\\Gamma_Y(h) = \\sum_{i=0}^\\infty \\boldsymbol{A}^{h+i} \\Sigma_U (\\boldsymbol{A}^i)'\\), and \\(\\frac{\\partial Y_t}{U_{t-i}} = \\boldsymbol{A}^i \\rightarrow 0\\) as \\(i \\rightarrow \\infty\\). Intuitively, this means that the impulse response of \\(Y_t\\) to innovations converges to zero over time. Furthermore, a stable VAR(\\(p\\)) process is stationary – its first and second moments are time invariant. VAR(\\(p\\)) models may be estimated using a variety of statistical methods, with one of the most popular approaches being multivariate least squares estimation. Suppose we observe a sample time series \\(y_1, \\ldots, y_T\\), along with \\(p\\) presample values for each variable (effectively a combined sample size of \\(T+p\\)). Define . \\[Y = (y_1, \\ldots, y_T) \\, ,\\] \\[B = (\\upsilon, A_1, \\ldots, A_p) \\, ,\\] \\[Z_t = \\begin{bmatrix} 1 \\\\ y_t \\\\ \\vdots \\\\ y_{t-p+1} \\end{bmatrix} \\, ,\\] \\[Z = (Z_0 , \\ldots, Z_{T-1}) \\, ,\\] \\[U = (u_1, \\ldots, u_T) \\, ,\\] \\[\\boldsymbol{y} = \\text{vec}(Y) \\,\\] \\[\\boldsymbol{\\beta} = \\text{vec}(B) \\,\\] \\[\\boldsymbol{b} = \\text{vec}(B') \\, ,\\] \\[\\boldsymbol{u} = \\text{vec}(U) \\, .\\] Using the above notvation, we may express any given VAR(\\(p\\)) model as . \\[Y = BZ + U \\, ,\\] or equivalently as . \\[\\text{vec}(Y) = \\text{vec}(B Z) + \\text{vec}(U) = (Z' \\otimes I_K) \\text{vec}(B) + \\text{vec}(U) \\, ,\\] or . \\[\\boldsymbol{y} = (Z' \\otimes I_K) \\boldsymbol{\\beta} + \\boldsymbol{u} \\, ,\\] with the covariance matrix of \\(\\boldsymbol{u}\\) being \\(\\Sigma_{\\boldsymbol{u}} = I_t \\otimes \\Sigma_u\\). It can be shown that the least-squares (LS) estimator for the given model is . \\[\\widehat{\\boldsymbol{b}} = \\text{vec}(\\widehat{B}') = (I_K \\otimes (Z Z')^{-1} Z) \\text{vec}(Y') \\, ,\\] which is equivalent to separately estimating each of the \\(K\\) equations in the standard formulation of a VAR(\\(p\\)) model using OLS. It can also be shown that if \\(y_t\\) is stable with standard white noise disturbances, we can ues the \\(t\\)-ratios provided by common regression programs in setting up confidence intervals and tests for individual coefficients. These \\(t\\)-statistics can be obtained by dividing the elements of \\(\\widehat{B}\\) by square roots of the corresponding diagonal elements of \\((Z Z')^{-1} \\otimes \\widehat{\\Sigma}_u\\). ",
    "url": "/Time_Series/VAR-models.html#vector-autoregression-var-models",
    
    "relUrl": "/Time_Series/VAR-models.html#vector-autoregression-var-models"
  },"201": {
    "doc": "VAR Models",
    "title": "Keep in Mind",
    "content": ". | VARs are often used for impulse response analysis, which is plagued with a multitude of identification limitations that you can read about in Lütkepohl’s (2005) and Kilian and Lütkepohl’s (2017) textbooks. VARs are reduced-form models – it is necessary to impose structural restrictions to identify the relevant innovations and impulse responses. | . ",
    "url": "/Time_Series/VAR-models.html#keep-in-mind",
    
    "relUrl": "/Time_Series/VAR-models.html#keep-in-mind"
  },"202": {
    "doc": "VAR Models",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/VAR-models.html#implementations",
    
    "relUrl": "/Time_Series/VAR-models.html#implementations"
  },"203": {
    "doc": "VAR Models",
    "title": "R",
    "content": "Begin by loading relevant packages. dplyr provides us with data manipulation capabilities, lubridate allows us to generate and work with date data, and vars contains VAR-related tools. if (!require(\"pacman\")) install.packages(\"pacman\") library(pacman) p_load(dplyr, lubridate, vars) . Then we create an arbitrary dataset containing two different time series. The actual relationship between these time series is irrelevant for this demonstration – the focus is on estimating VARs. gdp &lt;- read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") fdefx &lt;- read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/FDEFX.csv\") data &lt;- inner_join(gdp, fdefx) %&gt;% # Join the two data sources into a single data frame mutate(DATE = as.Date(DATE), GDPC1 = log(GDPC1), # log GDPC1 FDEFX = log(FDEFX)) # log FDEFX . We may use the vars::VARselect function to obtain optimal lag orders under a variety of information criteria. Notice that we are excluding the date vector when inputting the data into VARselect. lagorders &lt;- VARselect(data[,c(\"GDPC1\",\"FDEFX\")])$selection lagorders . Now we estimate the VAR by defaulting to the Akaike Information Criterium (AIC) optimal lag order. We include an intercept in the model by passing the type = \"const\" argument inside of VAR. lagorder &lt;- lagorders[1] estim &lt;- VAR(data[,c(\"GDPC1\",\"FDEFX\")], p = lagorder, type = \"const\") . Print the estimated VAR roots – we must make sure that the VAR is stable (all roots lie within the unit circle). summary(estim)$roots . Regardless of stability issues, we are able to generate the non-cumulative impulse response function of FDEFX responding to an orthogonal shock to GDPC1. irf &lt;- irf(estim, impulse = \"FDEFX\", response = \"GDPC1\") plot(irf) . Lastly, we may also generate forecast error variance decompositions. fevd &lt;- fevd(estim) plot(fevd) . ",
    "url": "/Time_Series/VAR-models.html#r",
    
    "relUrl": "/Time_Series/VAR-models.html#r"
  },"204": {
    "doc": "VAR Models",
    "title": "VAR Models",
    "content": " ",
    "url": "/Time_Series/VAR-models.html",
    
    "relUrl": "/Time_Series/VAR-models.html"
  },"205": {
    "doc": "Artificial Neural Network",
    "title": "Artificial Neural Network",
    "content": "Artificial neural networks are universal function approximators that consist of nodes, each of which does a computation on an input, and layers, which are collections of nodes that have access to the same inputs. There are many variations of neural networks but the most common is the multi-layer perceptron. They can be applied to supervised learning (e.g. regression and classification), unsupervised learning, and reinforcement learning. As an example, a simple single-layer perceptron for regression with \\(M\\) neurons in the hidden layer is defined as . \\[Z_m = \\sigma(\\alpha_{0m} + \\alpha_{m}^TX), \\quad m = 1, ..., M\\] \\[f(X) = \\beta_0 + \\beta^TZ\\] for feature matrix \\(X\\) and activation function $\\sigma$. A popular choice of activation function is the rectified linear unit, \\(\\sigma(v) = \\max(0, v)\\). ",
    "url": "/Machine_Learning/artificial_neural_network.html",
    
    "relUrl": "/Machine_Learning/artificial_neural_network.html"
  },"206": {
    "doc": "Artificial Neural Network",
    "title": "Keep in Mind",
    "content": ". | There are many different choices of activation function. | There are many different choices of network architecture, appropriate for different tasks. | Neural networks are prone to overfitting, and are sensitive to both the scale of the inputs and the choice of starting weights in the hidden layer. There are many techniques available to reduce overfitting and other issues with neural networks. | . ",
    "url": "/Machine_Learning/artificial_neural_network.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/artificial_neural_network.html#keep-in-mind"
  },"207": {
    "doc": "Artificial Neural Network",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/artificial_neural_network.html#implementations",
    
    "relUrl": "/Machine_Learning/artificial_neural_network.html#implementations"
  },"208": {
    "doc": "Artificial Neural Network",
    "title": "Julia",
    "content": "Julia community puts a lot of effort into development of Neural Networks ecosystem. These efforts are largely fueled and greatly helped by two major features of Julia. First, pure Julia code (with a little help from the developer) gets translated into efficient and optimized machine code in many cases rivaling C/C++. This largely alleviates “two languages problem”. Second, Julia features advanced metaprogramming facilities making possible sophisticated source code transformations, Automatic Differentiation in particular. The main production-quality Machine Learning framework is Flux and Lux presents an alternative better suited for and motivated by Scientific Machine Learning tasks. In the following example we’ll stick with the former. using Flux, MLUtils n_samples = 1000 n_features = 10 hidden_layer_size = 100 # our underlying equation will be # y = x1 + 10*x2 - 7*x3 + 2*x4 + 3*x5 - 14*x6 + x7 + 3*x8 - x9 + 11*x10 - 40 # the coefficients are arbitrary, they don't represent anything # we'll collect them in a single (column) vector coeffs = [1, 10, -7, 2, 3, -14, 1, 3, -1, 11] b = -40 # a n_samples*n_features matrix of \"observations\" X = randn(Float32, n_samples, n_features) # calculating \"responses\" corresponding to \"observations\" X # according to the above equation (using matrix-vector product) # adding a little random noise y = X*coeffs .+ b .+ 0.02f0 .* randn(Float32, n_samples) # We leave out 15 % of the data for testing train_data, test_data = splitobs((X', y); at=0.85) # our model consists of two fully connected layers, first one using `relu` activation model = Chain(Dense(n_features =&gt; hidden_layer_size, relu), Dense(hidden_layer_size =&gt; 1)) # we're collecting all of the model's parameters (weights and biases of all the layers) parameters = Flux.params(model) # using Mean Sqared Error as a measure of the loss # we can use our Neural Network as if it was just a function, applying it # to a single observation or the whole set of observations at once loss(x, y) = Flux.Losses.mse(model(x), y) # currently our model is an extremely poor fit to both train and test data loss(train_data[1], train_data[2]') # 2123.2031f0 loss(test_data[1], test_data[2]') # 1939.8761f0 # we'll use ADAM optimization strategy with default parameters opt = Adam() # now let's train our model wrt. our data! Flux.train!(loss, parameters, eachobs(train_data), opt) # the fitness of the model improved a lot on both train and test data! loss(train_data[1], train_data[2]') # 96.052666f0 loss(test_data[1], test_data[2]') # 100.78666f0 . Another prominent feature of the Julia ecosystem, especially with relation to Neural Networks, is highly-developed support for NVidia GPUs (AMD and Intel GPUs support is under development as well). CUDA.jl library provides Julia bindings and high-level functions as well as underlying CUDA and CuNN C++ libraries, thus simple import Pkg; Pkg.add(\"CUDA\") makes you ready to offload your computations to a graphics card. For the most part Flux is oblivious to the location of data — RAM or GPU — therefore after copying all our data to a GPU we can feed it to exactly the same functions for training, loss and so on. For copying we’ll employ Flux.gpu function which does nothing if there’s no CUDA installed. This way our code can be GPU-oblivious too: it will use GPU if one is present, otherwise it will work just as fine on a CPU (albeit slower). import CUDA # we don't actually need to directly call functions from the CUDA library # but we want to be sure it got loaded. # also we can call `CUDA.functional()` to check if a GPU is present and accessible. # copying the data to a GPU with the `Flux.gpu` function gpu_train_data = (gpu(collect(train_data[1])), gpu(collect(train_data[2]))) gpu_test_data = (gpu(collect(test_data[1])), gpu(collect(test_data[2]))) # building a new untrained model and moving it to a GPU gpu_model = gpu(Chain(Dense(n_features =&gt; hidden_layer_size, relu), Dense(hidden_layer_size =&gt; 1))) gpu_ps = Flux.params(gpu_model) # the same loss function but now using `gpu_model` gpu_loss(x, y) = Flux.Losses.mse(gpu_model(x), y) # the loss of an untrained model is again very high # but now all the calculations happen on a GPU gpu_loss(gpu_train_data[1], gpu_train_data[2]') # 2134.6206f0 gpu_loss(gpu_test_data[1], gpu_test_data[2]') # 2145.352f0 # here we use `Flux.DataLoader` to feed our training data into the `Flux.train!` function # 50 (batchsize) instances at a time. Using batches is preferable on a GPU to improve # the throughput and a GPU can handle many more than 50 at once in our case. gpu_train_loader = Flux.DataLoader((gpu_train_data[1], gpu_train_data[2]'), batchsize = 50) # on the flip side considering a batch at a time reduces gradient information available # for training compared to instance-by-instance approach thus hampering learning rate # we compensate by running the same training loop over the same data several times (epochs) # there's no reason for the number of epochs to be the same as a batch size, I just like # the number 50 :) for i=1:50 Flux.train!(gpu_loss, gpu_ps, gpu_train_loader, opt) end # now the loss is significantly better :) gpu_loss(gpu_train_data[1], gpu_train_data[2]') # 38.33216f0 gpu_loss(gpu_test_data[1], gpu_test_data[2]') # 31.173874f0 . ",
    "url": "/Machine_Learning/artificial_neural_network.html#julia",
    
    "relUrl": "/Machine_Learning/artificial_neural_network.html#julia"
  },"209": {
    "doc": "Artificial Neural Network",
    "title": "Python",
    "content": "There are many libraries for artificial neural networks in Python, including the widely-used, production-oriented tensorflow (from Google) and PyTorch (from Facebook). scitkit-learn has a simple neural network regressor that’s just a single line: . from sklearn.neural_network import MLPRegressor from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10) # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) # Create and fit model regr = MLPRegressor(hidden_layer_sizes=(100,), activation='relu').fit(X_train, y_train) # Compute R^2 score regr.score(X_test, y_test) . ",
    "url": "/Machine_Learning/artificial_neural_network.html#python",
    
    "relUrl": "/Machine_Learning/artificial_neural_network.html#python"
  },"210": {
    "doc": "Bar Graphs",
    "title": "Introduction",
    "content": "This is a brief tutorial on how to make bar graphs. It also provides a little information on how to stylize bar graphs to make them look better. There are a plethora of options to make a bar graph look like the visualization that you want it to. Lets dive in! . ",
    "url": "/Presentation/Figures/bar_graphs.html#introduction",
    
    "relUrl": "/Presentation/Figures/bar_graphs.html#introduction"
  },"211": {
    "doc": "Bar Graphs",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/bar_graphs.html#implementations",
    
    "relUrl": "/Presentation/Figures/bar_graphs.html#implementations"
  },"212": {
    "doc": "Bar Graphs",
    "title": "Python",
    "content": "There are many plotting libraries in Python, including declarative (say what you want) and imperative (build what you want) options. In the example below, we’ll explore several different options for plotting bar chart data. For even greater control over plot elements, users may want to explore the matplotlib library (and its bar chart functionality here), but the examples below will cover most use cases. By far the quickest way to plot a bar chart is to use data analysis package pandas’ built-in bar chart option. import pandas as pd df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/Manitoba.lakes.csv\", index_col=0) df.plot.bar(y='area', legend=False, title='Area of lakes in Manitoba'); . This produces a functional, if not hugely attractive, plot. Calling the function without the y='area' keyword argument causes pandas to plot two columns for each lake based on the two variables in the dataframe, one for area and one for elevation (while sharing the same y-axis). pandas uses the plotting library matplotlib under the hood. Many extra configuration options are available using matplotlib. In this case, let’s just tidy the plot up a bit by applying a style, adding in a label, and putting the title on the left. import matplotlib.pyplot as plt plt.style.use('seaborn') ax = df.plot.bar(y='area', legend=False, ylabel='Area', rot=15) ax.set_title('Area of lakes in Manitoba', loc='left'); . For more sophisticated visualisations, let’s look first at the seaborn library. We’ll use the tips dataset. Note that if seaborn finds more than one row per category for the bar chart, it will automatically create error bars based on the standard deviation of your data. Although it is declarative, seaborn is built on matplotlib (like pandas built-in plots), so finer control of plots is available should it be needed. (Like df.plot.bar, sns.barplot returns an ax object when not used with the ; character.) . import seaborn as sns tips = sns.load_dataset(\"tips\") sns.barplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips); . Yet another declarative option comes from plotnine, which is a port of R’s ggplot and so has nearly identical syntax that library. from plotnine import ggplot, geom_bar, aes, labs ( ggplot(tips) + geom_bar(aes(x='day'), colour='black', fill='blue') + labs(x = \"Day\", y = \"Number\", title = \"Number of diners\") ) . Other packages for bar charts include proplot, an imperative library for publication-quality charts that wraps matplotlib, and altair, a declarative library which produces high-quality, web-ready graphics. ",
    "url": "/Presentation/Figures/bar_graphs.html#python",
    
    "relUrl": "/Presentation/Figures/bar_graphs.html#python"
  },"213": {
    "doc": "Bar Graphs",
    "title": "R",
    "content": "For the R demonstration, we will be calling the tidyverse package. library(tidyverse) library(ggplot2) . This tutorial will use a dataset that already exists in R, so no need to load any new data into your environment. The dataset we will use is called starwars, which uses data collected from the Star Wars Universe. The tidyverse package uses ggplot2 to construct bar graphs. For our first example, let’s look at species’ appearences in Star Wars movies. Follow along below! . | First for our graph, we need write a line that calls ggplot. However we just use ‘ggplot’ to do so. Note the + after ggplot(). This + ties the subsequent lines together to form the graph. A common error when making any type of graph in ggplot() is to forget these + symbols at the end of a code line, so just remember to use them! | There are a couple of steps to construct a bar graph. First we need to specify the data we want to visulaize. We are making a bar graph, so we will use geom_bar. Since we want to use the 'starwars' dataset, we set data = starwars. Remember the comma after this, otherwise an error will appear. | Next we want to tell ggplot what we want to map. We use the mapping function to do this. We set mapping to the aesthetic function. (mapping = aes(x = species)) Within the aes function we want to specify what we want our x value to be, in this case species. Copy the code below to make your first bar graph! | . starwars &lt;- read.csv(\"https://github.com/LOST-STATS/LOST-STATS.github.io/raw/source/Presentation/Figures/Data/Bar_Graphs/star_wars_characters.csv\") ggplot() + geom_bar(data = starwars, mapping = aes(x = species)) . As you can see, there are some issues. We can’t tell what the individual species are on the x axis. We also might want to give our graph a title, maybe give it some color, etc. How do we do this? By adding additional functions to our graph! . ggplot(data = starwars) + geom_bar( mapping = aes(x = species), color = \"black\", fill = \"blue\") + labs(x = \"Species\", y = \"Total\", title = \"Character Appearences in Movies by Species\") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) . This graph looks much more interpretable to me, though appearences are subjective. Let’s look at what we did. First there are two additional parts to our mapping function, color and fill. The “color = ” provides an outline color to the bars on the graph, while “fill = ” provides the color within the bars. The x and y axis have been renamed, and the graph has been given a title. This was done using the labs() function in R. This function has additional options as well which you should explore. Finally we come to the theme() function in ggplot2. theme() has many options to customize any type of graph in R. For this basic tutorial, the x values (species) have been rotated so that they are legible compared to our first graph. Congratualtions, you have made your first bar graph in R! . There is a similar ggplot() function in R called geom_col. In geom_col, you can specify what you want the y axis to be, whereas geom_bar is only a count. Want more information on how to customize your graph? The Hadley Wickam book called R for Data Science is a fantastic place to start, and best of all it’s free! . ",
    "url": "/Presentation/Figures/bar_graphs.html#r",
    
    "relUrl": "/Presentation/Figures/bar_graphs.html#r"
  },"214": {
    "doc": "Bar Graphs",
    "title": "Stata",
    "content": "Stata, like R, also has pre-installed datasets available for use. To find them, click on ‘file’, then click on ‘Example Datasets’ which will open up a new window. Under ‘Description’ click on the link for ‘Example datasets installed with Stata’ which will bring up a list of datasets to use for examples. For the purposes of this demonstration we will use the 'bplong.dta' option. To load it into stata, click ‘use’ and it will appear in Stata. This is fictionalized blood pressure data. In your variables column you should have five variables (patient, sex, agegrp, when, bp). Let’s make a bar chart that looks at the patients within our dataset by gender and age. To make a bar chart type into your stata command console: . graph bar, over(sex) over(agegrp) . and the following output should appear in another window. Congratulations, you’ve made your first bar chart in Stata! We can now visually see the make-up of our dataset by gender and age. We might want to change the axis labels or give this a title. To do so type the following in your command window: . graph bar, over(sex) over(agegrp) title(Our Graph) ytitle(Percent) . and the following graph shoud appear . Notice we gave our graph a title and capitalized the y axis. Lets add some color next. To do so type . graph bar, over(sex) over(agegrp) title(Our Graph) ytitle(Percent) bar(1, fcolor(red)) bar(2, fcolor(blue)) . and the following graph should appear . Our bars are now red with a blue outline. Pretty neat! There are many sources of Stata help on the internet and many different way to customize your bar graphs. There is an official Stata support page that can answer queries regarding Stata. ",
    "url": "/Presentation/Figures/bar_graphs.html#stata",
    
    "relUrl": "/Presentation/Figures/bar_graphs.html#stata"
  },"215": {
    "doc": "Bar Graphs",
    "title": "Bar Graphs",
    "content": " ",
    "url": "/Presentation/Figures/bar_graphs.html",
    
    "relUrl": "/Presentation/Figures/bar_graphs.html"
  },"216": {
    "doc": "Binned Scatterplots",
    "title": "Introduction",
    "content": "Binned scatterplots are a variation on scatterplots that can be useful when there are too many data points that are being plotted. Binned scatterplots take all data observations from the original scatterplot and place each one into exactly one group called a bin. Once every observation is in a bin, each bin will get one point on a scatterplot, reducing the amount of clutter on your plot, and potentially making trends easier to see visually. ",
    "url": "/Presentation/Figures/binscatter.html#introduction",
    
    "relUrl": "/Presentation/Figures/binscatter.html#introduction"
  },"217": {
    "doc": "Binned Scatterplots",
    "title": "Keep in Mind",
    "content": ". | Bins are determined based on the conditioning variable (usually the x variable). Bin width can be determined in multiple ways. For example, you can set bin width with the goal of getting the same amount of observations into each bin. In this scenario, bins will likely all differ in width unless your data observations are equally spaced. You could also set bin width so that every bin is of equal width (and has unequal amount of observations falling into each bin). | Once observations are placed into bins using the conditioning variable, an outcome variable (usually the y variable) is produced by aggregating all observations in the bin and using a summary statistic to obtain one single point. Possible summary statistics that can be used include mean, median or other quantiles, max/min, or count. | The number of bins you will separate your data into is the most important decision you will likely make. There is no one way to determine this, but you will face the bias-variance trade off when selecting this parameter. The binsreg package in R, Stata, and Python has a default optimal number of bins that it calculates to make this trade off. | . ",
    "url": "/Presentation/Figures/binscatter.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/binscatter.html#keep-in-mind"
  },"218": {
    "doc": "Binned Scatterplots",
    "title": "Also Consider",
    "content": ". | Scatterplots | Styling Scatterplots | Binned scatterplots are used frequently used in Regression Discontinuity | . ",
    "url": "/Presentation/Figures/binscatter.html#also-consider",
    
    "relUrl": "/Presentation/Figures/binscatter.html#also-consider"
  },"219": {
    "doc": "Binned Scatterplots",
    "title": "Implementations",
    "content": "The binsreg package is available for R, Stata, and Python. See the package homepage. ",
    "url": "/Presentation/Figures/binscatter.html#implementations",
    
    "relUrl": "/Presentation/Figures/binscatter.html#implementations"
  },"220": {
    "doc": "Binned Scatterplots",
    "title": "R",
    "content": "It is fairly straightforward to create a basic binned scatterplot in R by hand. There is also the binsreg package for more advanced methods that includes things like automatic bandwidth selection and nonparametric fitting of the binned data; see here for another example. For the example below I will be using this created data: . x = rnorm(mean=50, sd=50, n=10000) y = x + rnorm(mean=0, sd=100, n=10000) df = data.frame(x=x, y=y) . library(ggplot2) ggplot(df, aes(x=x, y=y)) + geom_point() . After plotting, we can see that there may be a trend, but the graph is over cluttered and not easy to interpret right away. This is when binned scatterplots are most useful. library(dplyr) # this will create 25 quantiles using y and assign the observations in each quantile to a separate bin df = df %&gt;% mutate(bin = ntile(y, n=25)) new_df = df %&gt;% group_by(bin) %&gt;% summarise(xmean = mean(x), ymean = mean(y)) #find the x and y mean of each bin ggplot(new_df, aes(x=xmean, y=ymean)) + geom_point() . After binning and summarizing the data, we can identify the trend much easier! (but lose a sense of the very high variance) . Now let’s use binsreg, which can be installed with install.packages('binsreg'). It will automatically select bins based on quantiles, allows you to apply control variables before plotting the residuals, and offers easy methods for fitting splines on top of the binned data with the line option: . binsreg(df$y, df$x, line = c(3,3)) . ",
    "url": "/Presentation/Figures/binscatter.html#r",
    
    "relUrl": "/Presentation/Figures/binscatter.html#r"
  },"221": {
    "doc": "Binned Scatterplots",
    "title": "Stata",
    "content": "To install binsreg in Stata Stata has the excellent user-provided package binscatter specifically for creating binned scatterplots, with plenty of options described in the help files. * net install binsreg, from(https://raw.githubusercontent.com/nppackages/binsreg/master/stata) replace * Create some data clear set obs 1000 g x = rnormal() g y = x + rnormal() * Default binsreg will plot means of y with quantile-based bins of x, the number of bins is by default chosen optimally * Let's make 40 bins of x, why not * And also add a best-fit line binsreg y x, nbins(40) polyreg(1) . ",
    "url": "/Presentation/Figures/binscatter.html#stata",
    
    "relUrl": "/Presentation/Figures/binscatter.html#stata"
  },"222": {
    "doc": "Binned Scatterplots",
    "title": "Python",
    "content": "The binsreg package in Python can be installed with pip. The syntax is similar to the other platforms. We can make the same plot as the Stata example above. import numpy as np import seaborn as sns pip install binsreg # Create random data x = np.random.normal(size = 1000) y = x + np.random.normal(size = 1000) est = binsreg(y, x, data=data, nbins=40, polyreg=1) est.bins_plot . ",
    "url": "/Presentation/Figures/binscatter.html#python",
    
    "relUrl": "/Presentation/Figures/binscatter.html#python"
  },"223": {
    "doc": "Binned Scatterplots",
    "title": "Binned Scatterplots",
    "content": " ",
    "url": "/Presentation/Figures/binscatter.html",
    
    "relUrl": "/Presentation/Figures/binscatter.html"
  },"224": {
    "doc": "Boosted Regression Trees",
    "title": "Introduction",
    "content": "Boosting is a numerical optimization technique for minimizing the loss function by adding, at each step, a new tree that best reduces (steps down the gradient of) the loss function. For Boosted Regression Trees (BRT), the first regression tree is the one that, for the selected tree size, maximally reduces the loss function. ",
    "url": "/Machine_Learning/boosted_regression_trees.html#introduction",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#introduction"
  },"225": {
    "doc": "Boosted Regression Trees",
    "title": "Keep in Mind",
    "content": "The Boosted Trees Model is a type of additive model that makes predictions by combining decisions from a sequence of base models. More formally we can write this class of models as: . \\[g(x) = f_0(x)+f_1(x)+f_2(x)+...\\] where the final classifier \\(g\\) is the sum of simple base classifiers \\(f_i\\). For the boosted trees model, each base classifier is a simple decision tree. This broad technique of using multiple models to obtain better predictive performance is called model ensembling. Random forests improve upon bagged trees by decorrelating the trees. In order to decorrelate its trees, a random forest only considers a random subset of predictors when making each split (for each tree). This is compared to boosted trees, which can pass information from one to the other. We add each new tree to our model (and update our residuals). Trees are typically small—slowly improving where it struggles. | Check here for more help. | . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#keep-in-mind"
  },"226": {
    "doc": "Boosted Regression Trees",
    "title": "Also Consider",
    "content": ". | There are non-boosted approaches to decision trees, which can be found at Decision Trees and Random Forest. | . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#also-consider",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#also-consider"
  },"227": {
    "doc": "Boosted Regression Trees",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/boosted_regression_trees.html#implementations",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#implementations"
  },"228": {
    "doc": "Boosted Regression Trees",
    "title": "Python",
    "content": "There are several packages that can be used to estimate boosted regression trees but sklearn provides a function GradientBoostingRegressor that is perhaps the most user-friendly. # Install scikit-learn using conda or pip if you don't already have it installed from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import train_test_split # Generate some synthetic data X, y = make_regression() # Split the synthetic data into train and test arrays X_train, X_test, y_train, y_test = train_test_split(X, y) # The number of trees is set by n_estimators; there are many other options that # you should experiment with. Typically the defaults will be sensible but are # unlikely to be perfect for your use case. Let's create the empty model: reg = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, min_samples_split=3) # Fit the model reg.fit(X_train, y_train) # Predict the value of the first test case reg.predict(X_test[:1]) # R^2 score for the model (on the test data) reg.score(X_test, y_test) . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#python",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#python"
  },"229": {
    "doc": "Boosted Regression Trees",
    "title": "R",
    "content": "Boosted trees can be produced using the gbm package . Boosting has three tuning parameters. | The number of trees B (important to prevent overfitting) | The shrinkage parameter lambda (controls boosting’s learning rate . | often 0.01 or 0.001) | . | The number of splits in each tree (the tree’s complexity) | . data from:https://www.kaggle.com/kondla/carinsurance . library(tidyverse) library(janitor) library(caret) library(glmnet) library(magrittr) library(dummies) library(rpart.plot) library(e1071) library(caTools) library(naniar) library(forcats) library(ggplot2) library(MASS) library(pROC) library(ROCR) library(readr) library(gbm) set.seed(101) # Load in data carInsurance_train &lt;- read_csv(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Machine_Learning/Data/boosted_regression_trees/carInsurance_train.csv\") summary(carInsurance_train) # Produce a training and a testing subset of the data sample = sample.split(carInsurance_train$Id, SplitRatio = .8) train = subset(carInsurance_train, sample == TRUE) test = subset(carInsurance_train, sample == FALSE) total &lt;- rbind(train ,test) gg_miss_upset(total) . Step 1: Produce dummies as appropriate . total$CallStart &lt;- as.character(total$CallStart) total$CallStart &lt;- strptime(total$CallStart,format=\" %H:%M:%S\") total$CallEnd &lt;- as.character(total$CallEnd) total$CallEnd &lt;- strptime(total$CallEnd,format=\" %H:%M:%S\") total$averagetimecall &lt;- as.numeric(as.POSIXct(total$CallEnd)-as.POSIXct(total$CallStart),units=\"secs\") time &lt;- mean(total$averagetimecall,na.rm = TRUE) . Produce dummy variables as appropriate . total_df &lt;- dummy.data.frame(total %&gt;% dplyr::select(-CallStart, -CallEnd, -Id, -Outcome)) summary(total_df) . Fill in missing values . total_df$Job[is.na(total_df$Job)] &lt;- \"management\" total_df$Education [is.na(total_df$Education)] &lt;- \"secondary\" total_df$Marital[is.na(total_df$Marital)] &lt;-\"married\" total_df$Communication[is.na(total_df$Communication)] &lt;- \"cellular\" total_df$LastContactMonth[is.na(total_df$LastContactMonth)] &lt;- \"may\" . Step 2: Preprocess data with median imputation and a central scaling . clean_new &lt;- preProcess( x = total_df %&gt;% dplyr::select(-CarInsurance) %&gt;% as.matrix(), method = c('medianImpute') ) %&gt;% predict(total_df) . Step 3: Divide the data into testing and training data . trainclean &lt;- head(clean_new, 3200) %&gt;% as.data.frame() testclean &lt;- tail(clean_new, 800) %&gt;% as.data.frame() summary(trainclean) . Step 4: Parameters . gbm needs the three standard parameters of boosted trees—plus one more: . | n.trees, the number of trees | interaction.depth, trees’ depth (max. splits from top) | shrinkage, the learning rate | n.minobsinnode, minimum observations in a terminal node | . Step 5: Train the boosted regression tree . Notice that trControl is being set to select parameters using five-fold cross-validation (\"cv\"). carinsurance_boost = train( factor(CarInsurance)~., data = trainclean, method = \"gbm\", trControl = trainControl( method = \"cv\", number = 5 ), tuneGrid = expand.grid( \"n.trees\" = seq(25, 200, by = 25), \"interaction.depth\" = 1:3, \"shrinkage\" = c(0.1, 0.01, 0.001), \"n.minobsinnode\" = 5) ) . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#r",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#r"
  },"230": {
    "doc": "Boosted Regression Trees",
    "title": "Boosted Regression Trees",
    "content": " ",
    "url": "/Machine_Learning/boosted_regression_trees.html",
    
    "relUrl": "/Machine_Learning/boosted_regression_trees.html"
  },"231": {
    "doc": "Bootstrap Standard Errors",
    "title": "Bootstrap Standard Errors",
    "content": "Boostrapping is a statistical method that uses random sampling with replacement to determine the sampling variation of an estimate. If you have a data set of size \\(N\\), then (in its simplest form) a “bootstrap sample” is a data set that randomly selects \\(N\\) rows from the original data, perhaps taking the same row multiple times. In fact, each observation has the same probability of being selected for each bootstrap sample. For more information, see Wikipedia. Bootstrap is commonly used to calculate standard errors. If you produce many bootstrap samples and calculate a statistic in each of them, then under certain conditions, the distribution of that statistic across the bootstrap samples is the sampling distribution of that statistic. So the standard deviation of the statistic across bootstrap samples can be used as an estimate of standard error. This approach is generally used in cases where calculating the analytical standard error of a statistic would be too difficult or impossible. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html"
  },"232": {
    "doc": "Bootstrap Standard Errors",
    "title": "Keep in Mind",
    "content": ". | Although it feels entirely data-driven, bootstrap standard errors rely on assumptions just like everything else. It assumes your original model is correctly specified, for example. Basic bootstrapping assumes observations are independent of each other. | It is possible to allow for correlations across units by using block-bootstrap. | Bootstrapping can also be used to calculate other features of the parameter’s sample distribution, like the percentile, not just the standard error. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#keep-in-mind"
  },"233": {
    "doc": "Bootstrap Standard Errors",
    "title": "Also Consider",
    "content": ". | This page will consider the simplest approach to bootstrapping (the basic resampling of rows), but there are many others, such as cluster (or blocked) bootstrap, Bayesian bootstrap, and Wild bootstrap. For more information, see Wikipedia. Check the help files of the bootstrap package you’re using to see if they support these approaches. | Bootstrap is relatively straightforward to program yourself: resample, calculate, repeat, and then look at the distribution. If your reason for doing bootstrap is because you want your standard errors to reflect an unusual sampling or data manipulation procedure, for example, you may be best off programming your own routine. | This page contains a general approach to bootstrap, but for some statistical procedures, bootstrap standard errors are common enough that the command itself has an option to produce bootstrap standard errors. If this option is available, it is likely superior. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#also-consider",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#also-consider"
  },"234": {
    "doc": "Bootstrap Standard Errors",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#implementations",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#implementations"
  },"235": {
    "doc": "Bootstrap Standard Errors",
    "title": "R",
    "content": "The sandwich package (link] provides a convenient vcovBS function for obtaining bootstrapped covariance-variance matrices, and thus standard errors, for a wide range of model classes in R. We normally combine this with the coeftest function from the lmtest package, which allows us to substitute in the adjusted (here: bootstrapped) errors into our model, post-estimation. # If necessary # install.packages('sandwich','lmtest') library(sandwich) library(lmtest) # Use in-built mtcars data data(mtcars) # Run a regression with normal (iid) errors m &lt;- lm(hp~mpg + cyl, data = mtcars) # Obtain the boostrapped SEs coeftest(m, vcov = vcovBS(m)) . Another approach to obtaining bootstrapping standard errors in R is to use the boot package (link). This is typcally more hands-on, but gives the user a lot of control over how the bootrapping procedure will execute. # If necessary # install.packages('boot','broom','stargazer') # Load boot library library(boot) # Create function that takes # A dataset and indices as input, and then # performs analysis and returns a parameter of interest regboot &lt;- function(data, indices) { m1 &lt;- lm(hp~mpg + cyl, data = data[indices,]) return(coefficients(m1)) } # Call boot() function using the function we just made with 200 bootstrap samples # Note the option for stratified resampling with \"strata\", in addition to other options # in help(boot) boot_results &lt;- boot(mtcars, regboot, R = 200) # See results boot_results plot(boot_results) # There are lots of diagnostics you can look at at this point, # see https://statweb.stanford.edu/~tibs/sta305files/FoxOnBootingRegInR.pdf # Optional: print regression table with the bootstrap SEs # This uses stargazer, but the method is similar # with other table-making packages, # see /Presentation/export_a_formatted_regression_table.html library(broom) tidy_results &lt;- tidy(boot_results) library(stargazer) m1 &lt;- lm(hp~mpg + cyl, data = mtcars) stargazer(m1, se = list(tidy_results$std.error), type = 'text') . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#r",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#r"
  },"236": {
    "doc": "Bootstrap Standard Errors",
    "title": "Stata",
    "content": "Many commands in Stata come with a vce(bootstrap) option, which will implement bootstrap standard errors. * Load auto data sysuse auto.dta, clear * Run a regression with bootstrap SEs reg mpg weight length, vce(bootstrap) * see help bootstrap to adjust options like number of samples * or strata reg mpg weight length, vce(bootstrap, reps(200)) . Alternatively, most commands will also accept using the bootstrap prefix. Even if they do not allow the option vce(bootstrap). * If a command does not support vce(bootstrap), there's a good chance it will * work with a bootstrap: prefix, which works similarly bootstrap, reps(200): reg mpg weight length . If your model uses weights, bootstrap prefix (or vce(bootstrap) ) will not be appropriate, and the above command may give you an error: . *This should give you an error bootstrap, reps(200): reg mpg foreign length [pw=weight] . bootstrap, however, can be used to estimate standard errors of more complex systems. This, however, require some programming. Below an example for bootstrapping marginal effects for ivprobit. webuse laborsup, clear ** Start creating a small program program two_ivp, eclass * estimate first stage reg other_inc male_educ fem_educ kids * estimate residuals capture drop res predict res, res * add them to the probit first stage * This is what ivprobit two step does. probit fem_work fem_educ kids other_inc res margins, dydx(fem_educ kids other_inc) post end ** now simply bootstrap the program: bootstrap, reps(100):two_ivp . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#stata",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#stata"
  },"237": {
    "doc": "Causal Forest",
    "title": "Causal Forest",
    "content": "Causal forests are a causal inference learning method that are an extension of Random Forests. In random forests, the data is repeatedly split in order to minimize prediction error of an outcome variable. Causal forests are built similarly, except that instead of minimizing prediction error, data is split in order to maximize the difference across splits in the relationship between an outcome variable and a “treatment” variable. This is intended to uncover how treatment effects vary across a sample. For more information, see Explicitly Optimizing on Causal Effects via the Causal Forest. ",
    "url": "/Machine_Learning/causal_forest.html",
    
    "relUrl": "/Machine_Learning/causal_forest.html"
  },"238": {
    "doc": "Causal Forest",
    "title": "Keep in Mind",
    "content": ". | Causal forests simply uncover heterogeneity in a causal effect, they do not by themselves make the effect causal. A standard causal forest must assume that the assignment to treatment is exogenous, as it might be in a randomized controlled trial. Some extensions of causal forest may allow for covariate adjustment or for instrumental variables. See your causal forest package’s documentation to see if it has an option for ways of identifying the causal effect when treatment is not exogenous such as conditional adjustment or “instrumental forest”. | If using causal forest to estimate confidence intervals for the effects, in addition to the effects itself, it is recommended that you increase the number of trees generated considerably. | . ",
    "url": "/Machine_Learning/causal_forest.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/causal_forest.html#keep-in-mind"
  },"239": {
    "doc": "Causal Forest",
    "title": "Also Consider",
    "content": ". | Your intuition for how causal forest works can be based on a thorough understanding of Random Forests, for which materials are much more widely available. | . ",
    "url": "/Machine_Learning/causal_forest.html#also-consider",
    
    "relUrl": "/Machine_Learning/causal_forest.html#also-consider"
  },"240": {
    "doc": "Causal Forest",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/causal_forest.html#implementations",
    
    "relUrl": "/Machine_Learning/causal_forest.html#implementations"
  },"241": {
    "doc": "Causal Forest",
    "title": "Python",
    "content": "The econml package from Microsoft provides a range of causal machine learning functions, including deep instrumental variables, doubly robust learning, double machine learning, and causal forests. As in the R example below, we will download some crime data and look at the effect of one variable (‘pctymle’, the % of young males, assumed to be exogenous) on another (‘crmrte’, the crime rate). # Use \"pip install econml\" on the command line to install the package import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from econml.orf import DMLOrthoForest as CausalForest df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv') # Set the categorical variables: cat_vars = ['year', 'region', 'smsa'] # Transform the categorical variables to dummies and add them back in xf = pd.get_dummies(df[cat_vars]) df = pd.concat([df.drop(cat_vars, axis=1), xf], axis=1) cat_var_dummy_names = list(xf.columns) regressors = ['prbarr', 'prbconv', 'prbpris', 'avgsen', 'polpc', 'density', 'taxpc', 'pctmin', 'wcon'] # Add in the dummy names to the list of regressors regressors = regressors + cat_var_dummy_names # Split into train and test train, test = train_test_split(df, test_size=0.2) # Estimate causal forest estimator = CausalForest(n_trees=100, model_T=DecisionTreeRegressor(), model_Y=DecisionTreeRegressor()) estimator.fit(Y=train['crmrte'], T=train['pctymle'], W=train[regressors], X=train[regressors], inference='blb') effects_train = estimator.effect(train[regressors]) effects_test = estimator.effect(test[regressors]) conf_intrvl = estimator.effect_interval(test[regressors]) . ",
    "url": "/Machine_Learning/causal_forest.html#python",
    
    "relUrl": "/Machine_Learning/causal_forest.html#python"
  },"242": {
    "doc": "Causal Forest",
    "title": "R",
    "content": "The grf package has a causal_forest function that can be used to estimate causal forests. Additional functions afterwards can estimate, for example, the average_treatment_effect(). See help(package='grf') for more options. # If necessary # install.packages('grf') library(grf) # Get crime data from North Carolina df &lt;- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv') # It's not, but let's pretend that \"percentage of young males\" pctymle is exogenous # and see how the effect of it on crmrte varies across the other measured covariates # Make sure the data has no missing values. Here I'm dropping observations # with missing values in any variable, but you can limit the data first to just # variables used in analysis to only drop observations with missing values in those variables df &lt;- df[complete.cases(df),] # Let's use training and holdout data split &lt;- sample(c(FALSE, TRUE), nrow(df), replace = TRUE) df.train &lt;- df[split,] df.hold &lt;- df[!split,] # Isolate the \"treatment\" as a matrix pctymle &lt;- as.matrix(df.train$pctymle) # Isolate the outcome as a matrix crmrte &lt;- as.matrix(df.train$crmrte) # Use model.matrix to get our predictor matrix # We might also consider adding interaction terms X &lt;- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris + avgsen + polpc + density + taxpc + factor(region) + factor(smsa) + pctmin + wcon, data = df.train)) # Estimate causal forest cf &lt;- causal_forest(X,crmrte,pctymle) # Get predicted causal effects for each observation effects &lt;- predict(cf)$predictions # And use holdout X's for prediction X.hold &lt;- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris + avgsen + polpc + density + taxpc + factor(region) + factor(smsa) + pctmin + wcon, data = df.hold)) # And get effects effects.hold &lt;- predict(cf, X.hold)$predictions # Get standard errors for the holding data predictions - we probably should have set the num.trees # option in causal_forest higher before doing this, perhaps to 5000. SEs &lt;- sqrt(predict(cf, X.hold, estimate.variance = TRUE)$variance.estimates) . ",
    "url": "/Machine_Learning/causal_forest.html#r",
    
    "relUrl": "/Machine_Learning/causal_forest.html#r"
  },"243": {
    "doc": "Causal Forest",
    "title": "Stata",
    "content": "The MLRtime package allows the causal_forest function in the R grf package to be run from inside of Stata. This does require that R be installed. * If necessary, install MLRtime * net install MLRtime, from(\"https://raw.githubusercontent.com/NickCH-K/MLRtime/master/\") * Then, before use, install R from R-project.org * and run the MLRtimesetup function * MLRtimesetup, go * Start a fresh R session rcall clear * Get crime data from North Carolina import delimited using \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv\", clear * Turn character variables numeric so we can use them encode region, g(regionn) encode smsa, g(smsan) drop region smsa * It's not, but let's pretend that \"percentage of young males\" pctymle is exogenous * and see how the effect of it on crmrte varies across the other measured covariates * Let's use training and holdout data by sending our holdout data to R with rcall g split = runiform() &gt; .5 preserve * Keep the predictors from the holding data, send it over, so later we can make an X matrix to predict with keep if split == 0 keep year prbarr prbconv prbpris avgsen polpc density taxpc regionn smsan pctmin wcon * R needs that data pre-processed! So using the same variables as in the main model, process the variables fvrevar year prbarr prbconv prbpris avgsen polpc density taxpc i.regionn i.smsan pctmin wcon keep `r(varlist)' * Then send the data to R rcall: df.hold &lt;- st.data() restore * Now go back to just the training data keep if split == 1 * Run causal_forest, storing the effect predictions for the training data in the \"effects\" variable * the SEs of those effects in effectSE * And the effects and SEs for the holdout data in matrices called effects_hold and effectSE_hold causal_forest crmrte pctymle year prbarr prbconv prbpris avgsen polpc density taxpc i.regionn i.smsan pctmin wcon, pred(effects) varreturn(effectSE = sqrt(predict(CF, X, estimate.variance = TRUE)@@variance.estimates)) return(effects_hold = predict(CF, as.matrix(df.hold))@@predictions; effectSE_hold = sqrt(predict(CF, as.matrix(df.hold), estimate.variance = TRUE)@@variance.estimates)) * Look at the holdout effects predicted di \"`r(effects_hold)'\" . ",
    "url": "/Machine_Learning/causal_forest.html#stata",
    
    "relUrl": "/Machine_Learning/causal_forest.html#stata"
  },"244": {
    "doc": "Choropleths",
    "title": "Choropleths",
    "content": "Choropleths are maps in which areas are shaded or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area. For instance, population might be represented by dark green where it is (relatively) high and light green where it is (relatively) low. Choropleths are useful when you want to show differences in variables across areas. ",
    "url": "/Geo-Spatial/choropleths.html",
    
    "relUrl": "/Geo-Spatial/choropleths.html"
  },"245": {
    "doc": "Choropleths",
    "title": "Keep in Mind",
    "content": ". | Geospatial packages in R and Python tend to have a large number of complex dependencies, which can make installing them painful. Best practice is to install geospatial packages in a new virtual environment. | Think carefully about the units you’re using and whether plotting your data on a map is really informative. Choropleths can all too easily end up simply being plots of population density - and no-one will be surprised to see that areas like Manhattan have a high population! | . ",
    "url": "/Geo-Spatial/choropleths.html#keep-in-mind",
    
    "relUrl": "/Geo-Spatial/choropleths.html#keep-in-mind"
  },"246": {
    "doc": "Choropleths",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/choropleths.html#implementations",
    
    "relUrl": "/Geo-Spatial/choropleths.html#implementations"
  },"247": {
    "doc": "Choropleths",
    "title": "Python",
    "content": "The geopandas package is the easiest way to start making choropleths in Python. For plotting more sophisticated maps, there’s geoplot. In the example below, we’ll see three ways of plotting data on GDP per capita by geography. The first uses geopandas built-in .plot method. The second combines this with the matplotlib package to create a more attractive looking chart. The third example uses geoplot to create a cartogram in which the area of each country on the map gets shrunk according to how small its GDP per capita is. # Geospatial packages tend to have many elaborate dependencies. The quickest # way to get going is to use a clean virtual environment and then # 'conda install geopandas' followed by # 'conda install -c conda-forge descartes' import matplotlib.pyplot as plt import geopandas as gpd world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) world = world[(world.pop_est &gt; 0) &amp; (world.name != \"Antarctica\")] world['gdp_per_cap'] = 1.0e6 * world.gdp_md_est / world.pop_est # Simple choropleth world.plot(column='gdp_per_cap') # Much better looking choropleth plt.style.use('seaborn-paper') fig, ax = plt.subplots(1, 1) world.plot(column='gdp_per_cap', ax=ax, cmap='plasma', legend=True, vmin=0., legend_kwds={'label': \"GDP per capita (USD)\", 'orientation': \"horizontal\"}) plt.axis('off') # Now let's try a cartogram # If you don't have it already, geoplot can be installed by runnning # 'conda install geoplot -c conda-forge' on the command line. import geoplot as gplt ax = gplt.cartogram( world, scale='gdp_per_cap', hue='gdp_per_cap', cmap='plasma', linewidth=0.5, figsize=(8, 12) ) gplt.polyplot(world, facecolor='lightgray', edgecolor='None', ax=ax) plt.title(\"GDP per capita (USD)\") plt.show() . ",
    "url": "/Geo-Spatial/choropleths.html#python",
    
    "relUrl": "/Geo-Spatial/choropleths.html#python"
  },"248": {
    "doc": "Choropleths",
    "title": "R",
    "content": "The sf is a fantastic package to make choropleths and more in R. In the following code, we will walk through an identical example of the python implementation above, with the same data from rnaturalearth, but using R and and creating our plots with the wonderful ggplot2 package. In order to produce the cartogram plots, we use the cartogram package. ## Load and install the packages that we'll be using today library(sf) library(tidyverse) library(ggplot2) library(rnaturalearth) library(viridis) library(cartogram) library(scales) # N.B. rnaturalearth is currently broken: https://github.com/ropensci/rnaturalearth/issues/29 world &lt;- ne_download() %&gt;% st_as_sf() world = world %&gt;% filter(POP_EST &gt; 0, NAME != \"Antarctica\") %&gt;% mutate(gdp_per_capita = 1.0e6*(GDP_MD_EST / as.numeric(POP_EST))) ## Simple choropleth plot ggplot(data = world) + geom_sf(aes(fill = gdp_per_capita)) ## Much better looking choropleth with ggplot2 world %&gt;% st_transform(crs = \"+proj=eqearth +wktext\") %&gt;% ggplot() + geom_sf(aes(fill = gdp_per_capita)) + theme_void() + labs(title = \"\", caption = \"Data downloaded from www.naturalearthdata.com\", fill = \"GDP per capita (USD)\") + scale_fill_viridis(labels = comma) + theme(legend.position = \"bottom\", legend.key.width = unit(1.5, \"cm\")) ## Now let's try a cartogram using the cartogram package that was loaded above world_cartogram = world %&gt;% st_transform(crs = \"+proj=eqearth +wktext\") %&gt;% cartogram_ncont(\"gdp_per_capita\", k = 100, inplace = TRUE) ggplot() + geom_sf(data = world, alpha = 1, color = \"grey70\", fill = \"grey70\") + geom_sf(data = world_cartogram, aes(fill = gdp_per_capita), alpha = 1, color = \"black\", size = 0.1) + scale_fill_viridis(labels = comma) + labs(title = \"Cartogram - GDP per capita\", caption = \"Data downloaded from www.naturalearthdata.com\", fill = \"GDP per capita\") + theme_void() . ",
    "url": "/Geo-Spatial/choropleths.html#r",
    
    "relUrl": "/Geo-Spatial/choropleths.html#r"
  },"249": {
    "doc": "Choropleths",
    "title": "Stata",
    "content": "The spmap package allows for the visual representation of spatial information . // If you have not already, download the spmap package from SSC ssc install spmap, replace // Additionally, you need to convert shapefiles into stata formatted datasets ssc install shp2dta, replace // For consistency with the other examples, we will use the Natural Earth dataset. You will need to download it into your current working directory // You can download \"Admin 0 – Countries\" from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/ // The file should be called \"ne_110m_admin_0_countries.zip\" // Unzip the shapefiles once downloaded unzipfile ne_110m_admin_0_countries.zip, replace // Import Natural Earth Dataset spshape2dta ne_110m_admin_0_countries, replace saving(world) // Import Shapefile created use world_shp, clear // Merge for each unique country and there information merge 1:1 _ID using world.dta // Remove Antartica and if Population is 0 drop if NAME == \"Antarctica\" keep if POP_EST &gt; 0 // Create GDP per Captia gen gdp_per_capita = 1.0e6*(GDP_MD / POP_EST) // Choropleth of GDP per Captia by Country (world_shp is the reference basemap) spmap gdp_per_capita using world_shp, id(_ID) fcolor(Greens) . ",
    "url": "/Geo-Spatial/choropleths.html#stata",
    
    "relUrl": "/Geo-Spatial/choropleths.html#stata"
  },"250": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Cluster-Robust Standard Errors (a.k.a. Clustered Standard Errors)",
    "content": "Data is considered to be clustered when there are subsamples within the data that are related to each other. For example, if you had data on test scores in a school, those scores might be correlated within classroom because classrooms share the same teacher. When error terms are correlated within clusters but independent across clusters, then regular standard errors, which assume independence between all observations, will be incorrect. Cluster-robust standard errors are designed to allow for correlation between observations within cluster. For more information, see A Practitioner’s Guide to Cluster-Robust Inference. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#cluster-robust-standard-errors-aka-clustered-standard-errors",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#cluster-robust-standard-errors-aka-clustered-standard-errors"
  },"251": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Keep in Mind",
    "content": ". | Just because there are likely to be clusters in your data is not necessarily a good justification for using cluster-robust inference. Generally, clustering is advised only if either sampling or treatment assignment is performed at the level of the clusters. See Abadie, Athey, Imbens, &amp; Wooldridge (2017), or this simple summary of the paper. | There are multiple kinds of cluster-robust standard errors, for example CR0, CR1, and CR2. Check in to the kind available to you in the commands you’re using. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#keep-in-mind"
  },"252": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Also Consider",
    "content": ". | Cluster Bootstrap Standard Errors, which are another way of performing cluster-robust inference that will work even outside of a standard regression context. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#also-consider",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#also-consider"
  },"253": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Implementations",
    "content": "Note: Clustering of standard errors is especially common in panel models, such as linear fixed effects. For this reason, software routines for these particular models typically offer built-in support for (multiway) clustering. The implementation pages for these models should be hyperlinked in the relevant places below. Here, we instead concentrate on providing implementation guidelines for clustering in general. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#implementations",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#implementations"
  },"254": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Julia",
    "content": "For cluster-robust estimation of (high-dimensional) fixed effect models in Julia, see here. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#julia",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#julia"
  },"255": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "R",
    "content": "For cluster-robust estimation of (high-dimensional) fixed effect models in R, see here. Note that these methods can easily be re-purposed to run and cluster standard errors of non-panel models; just omit the fixed-effects in the model call. But for this page we’ll focus on some additional methods. Cluster-robust standard errors for many different kinds of regression objects in R can be obtained using the vcovCL or vcovBS functions from the sandwich package (link). To perform statistical inference, we combine these with the coeftest function from the lmtest package. This approach allows users to adjust the standard errors for a model “on-the-fly” (i.e. post-estimation) and is thus very flexible. # Read in data from the College Scorecard df &lt;- read.csv( \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\" ) # Create a regression model with normal (iid) errors my_model &lt;- lm(repay_rate ~ earnings_med + state_abbr, data = df) # Swap out cluster-robust errors post-estimation with coeftest::lmtest and sandwich::vcovCL library(lmtest) library(sandwich) coeftest(my_model, vcov = vcovCL(my_model, cluster = ~inst_name)) . Alternately, users can specify clustered standard errors directly in the model call using the lm_robust function from the estimatr package (link). This latter approach is very similar to how errors are clustered in Stata, for example. # Alternately, use estimator::lm_robust to specify clustered SEs in the original model call. # Standard error types are referred to as CR0, CR1 (\"stata\"), CR2 here. # Here, CR2 is the default library(estimatr) my_model2 &lt;- lm_robust( repay_rate ~ earnings_med + state_abbr, data = df, clusters = inst_name, se_type = \"stata\" ) summary(my_model2) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#r",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#r"
  },"256": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Stata",
    "content": "Stata has clustered standard errors built into most regression commands, and they generally work the same way for all commands. * Load in College Scorecard data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\", clear * The missings are written as \"NA\", let's turn this numeric destring earnings_med repay_rate, replace force * If we want to cluster on a variable or include it as a factor it must not be a string encode inst_name, g(inst_name_encoded) encode state_abbr, g(state_encoded) * Just add vce(cluster) to the options of the regression * This will give you CR1 regress repay_rate earnings_med i.state_encoded, vce(cluster inst_name_encoded) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#stata",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#stata"
  },"257": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Cluster-Robust Standard Errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html"
  },"258": {
    "doc": "Collapse a Data Set",
    "title": "Collapse a Data Set",
    "content": "The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . | I | J | X | . | 1 | 1 | 3 | . | 1 | 2 | 3.5 | . | 2 | 1 | 2 | . | 2 | 2 | 4.5 | . the variables \\(I\\) and \\(J\\) uniquely identify rows. The first row has \\(I = 1\\) and \\(J = 1\\), and there is no other row with that combination. We could also say that \\(X\\) uniquely identifies rows, but in this example \\(X\\) is not a case-identifying variable, it’s actual data. It is common to want to collapse a data set from one level to another, coarser level. For example, perhaps instead of one row per combination of \\(I\\) and \\(J\\), we simply want one row per \\(I\\), perhaps with the average \\(X\\) across all \\(I\\) observations. This would result in: . | I | X | . | 1 | 3.25 | . | 2 | 3.25 | . This can be one useful way to produce summary statistics, but can also be used to rid the data of unnecessary or unusable detail, or to change one data set to match the observation level of another. ",
    "url": "/Data_Manipulation/collapse_a_data_set.html",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html"
  },"259": {
    "doc": "Collapse a Data Set",
    "title": "Keep in Mind",
    "content": ". | Collapsing a data set almost by definition requires losing some information. Make sure that you actually want to lose this information, rather than, for example, doing a horizontal merge, which can match data sets with different observation levels without losing information. | Make sure that, for each variable you plan to retain in your new, collapsed data, you know the correct procedure that should be used to figure out the new, summarized value. Should the collapsed data for variable \\(X\\) use the mean of all the observations you started with? The median? The mode? The first value found in the data? Think through these decisions. | . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#keep-in-mind"
  },"260": {
    "doc": "Collapse a Data Set",
    "title": "Also Consider",
    "content": ". | For more information about observation levels and how to determine what the current observation level is, see determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#also-consider",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#also-consider"
  },"261": {
    "doc": "Collapse a Data Set",
    "title": "Implementations",
    "content": "For our implementation examples, we’ll use the “storms” dataset that comes bundled with the dplyr R package and is also available as a downloadable CSV here. This dataset contains hourly track data for 198 tropical storms from 1993 to 2006, which amounts to just over 10,000 observations in total. Our task for each of the implementation examples will be the same: We want to collapse this hourly dataset to the daily level, and obtain the mean wind speed and pressure reading for each storm. Moreover, we’ll also get the first category listing of each storm, thereby demonstrating how we can combine multiple aggregation functions (mean and first) in a single operation. ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#implementations",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#implementations"
  },"262": {
    "doc": "Collapse a Data Set",
    "title": "Julia",
    "content": "Julia adopts a highly modular approach to package functionality. The main data wrangling operations are all provided by the DataFrames.jl package. But for this example, we’ll also need to load the Statistics.jl library (that comes bundled with the base Julia installation) for the mean and first aggregating functions. We’ll also be using the CSV.jl package, but that’s just to import the dataset from the web. #] add DataFrames, CSV using DataFrames, Statistics, CSV # Read in the file from the web and convert to a DataFrame url = \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\" storms = CSV.read(download(url), DataFrame) storms_collapsed = combine(groupby(storms, [:name, :year, :month, :day]), [:wind, :pressure] .=&gt; mean, [:category] .=&gt; first) . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#julia",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#julia"
  },"263": {
    "doc": "Collapse a Data Set",
    "title": "Python",
    "content": "For our Python implementation we’ll use the very popular pandas library. Note that, after specifying the grouping variables, we’ll use dictionary of functions (of the form 'original column': 'function') to aggregate the other variables by. import pandas as pd # Pull in data on storms storms = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv') # We'll save the collapsed data as a new object called `storms_collapsed` (this is optional) storms_collapsed = (storms .groupby(['name', 'year', 'month', 'day']) .agg({'wind': 'mean', 'pressure': 'mean', 'category': 'first'})) . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#python",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#python"
  },"264": {
    "doc": "Collapse a Data Set",
    "title": "R",
    "content": "R provides several package ecoystems for data wrangling and collapsing. We’ll show you three (in increasing order of speed for this particular task, although they’ll all complete quickly). First, dplyr: . # If necessary, install dplyr # install.packages('dplyr') library(dplyr) # The storms dataset comes bundled with dplyr, so we can load it directly data(\"storms\") # We'll save the collapsed data as a new object called `storms_collapsed` (this is optional) storms_collapsed = storms %&gt;% group_by(name, year, month, day) %&gt;% summarize(across(c(wind, pressure), mean), category = first(category)) . Second, data.table: . # install.packages('data.table') library(data.table) # Set the already-loaded storms dataset as a data.table setDT(storms) storms_collpased = storms[, .(wind = mean(wind), pressure = mean(pressure), category = first(category)), by = .(name, year, month, day)] . Third: collapse: . # install.packages('collapse') library(collapse) storms_collapsed = collap(storms, by = ~name + year + month + day, custom = list(fmean=c('wind', 'pressure'), ffirst='category')) . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#r",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#r"
  },"265": {
    "doc": "Collapse a Data Set",
    "title": "Stata",
    "content": "For Stata, we’ll use the generic collapse command. ** Read in the data import delimited https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv collapse (mean) wind (mean) pressure (first) category, by(name year month day) . With big datasets, Stata can be slow compared to other languages, though they do seem to be trying to change that a bit. The community-contributed gtools suite can help a lot with speedups and, fortunately, has a faster version of collapse, called gcollapse. Note that we won’t necessarily see a benefit for small(ish) datasets like the one that we are using here. But note that the syntax is otherwise identical. * ssc install gtools * gtools, upgrade gcollapse (mean) wind (mean) pressure (first) category, by(name year month day) . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#stata",
    
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#stata"
  },"266": {
    "doc": "Color Palettes",
    "title": "Color Palettes",
    "content": "Color palettes are a simple way to customize graphs and improve data visualization through increased color ranges and options. There are a multitude of packages that add color palette options for all types of data, including continuous and discrete color palettes. ",
    "url": "/Presentation/Figures/color_palettes.html",
    
    "relUrl": "/Presentation/Figures/color_palettes.html"
  },"267": {
    "doc": "Color Palettes",
    "title": "Keep in Mind",
    "content": ". | Not all color palettes can be used in every situation. Some are better suited for continuous data and others for discrete data. | There are different functions for scale colors and fill colors, pick the right function for your specific case. | Check the help file or manual for any color palette package for information on arguments and color options available. | . ",
    "url": "/Presentation/Figures/color_palettes.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/color_palettes.html#keep-in-mind"
  },"268": {
    "doc": "Color Palettes",
    "title": "Also Consider",
    "content": ". | Remember, not all color palette packages can be used in every scenario. Some are better suited for discrete variables while others are better for continuous variables. | There are other ways to customize your graphs, such as adding themes, which conforms your graph to a pre-determined aesthetic based on the function used. | . ",
    "url": "/Presentation/Figures/color_palettes.html#also-consider",
    
    "relUrl": "/Presentation/Figures/color_palettes.html#also-consider"
  },"269": {
    "doc": "Color Palettes",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/color_palettes.html#implementations",
    
    "relUrl": "/Presentation/Figures/color_palettes.html#implementations"
  },"270": {
    "doc": "Color Palettes",
    "title": "R",
    "content": "There are a huge array of color palette options and user-defined themes available in R. In the examples that follow, we’ll limit ourselves to demonstrating with ggplot2 (link), although the same principles apply to base R plots, and only one or two additional palette themes. First, let’s load in some data that we can use to demonstrate color palette options. For these examples, we will use the palmer penguins data from the palmerpenguins package. # load in palmerpenguins package library(palmerpenguins) # Load the penguins data into memory data(\"penguins\") . Discrete Data . The first two examples will deal with discrete color palettes for modeling discrete data. Let’s first demonstrate a color palette option that comes with the ggplot2 package, which will be used throughout this post to construct graphs. Let’s see what ggplot2 offers in terms of color customization by graphing penguin bill length and bill depth coloring by species. # load ggplot2 library(ggplot2) # graph bill length vs depth coloring by species ggplot(data = penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=species)) + geom_point() . ggplot2 provides default color options for quick graph making. However, we can change the colors for the species by using the scale_color_brewer() function from the ggplot2 package . # recreate previous graph, now using the scale_color_brewer() to change color palette last_plot() + scale_color_brewer(palette = \"RdYlGn\") . By using the scale_color_brewer() function (which again comes bundled with ggplot2), we are able to change the color palette used to color the data points. This is particularly helpful when we need to color many different groups, as we can specify a palette with lots of colors available to assign to each group. There are many different palette options we can specify, check out the ggplot2 website for more available options. In addition to changing the color of data points, we can use color palettes to change the fill color of a graph object, which is particularly useful when creating bar charts, line graphs, boxPlots, or filling in confidence intervals. Let’s create a new example where we make a bar chart of the number of penguins from each species in the data set: . # create bar chart of number of penguins from each species, coloring the fill of the bars by species ggplot(data = penguins, aes(x=species)) + geom_bar(aes(fill = species)) . Again, ggplot2 uses default colors to fill in the bars. However, we can change these colors using a dedicated color palette package. Let’s try the ggsci package (link) this time, which comes with many different fill options inspired by those used in science journals and popular tv shows. For the example below, we’ll use the scale_fill_simpsons() function to change the bar colors to a set inspired by the long-running comedy show. # load in ggsci package library(ggsci) # add the scale_fill_simpsons() function to change bar colors last_plot() + scale_fill_simpsons() . Now the bars are a different set of colors from the default set! If you want to learn more about all the options available from the ggsci package, visit its cran page. Continuous Data . The previous examples use a discrete variable to color the data by. We can also apply color palettes to continuous variables for a color gradient scale. Let’s return to the first example, but change the variable we are coloring by from “species” to body_mass_g and see what we get. # Recreate first example, but switch color to body_mass_g variable ggplot(data = penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + geom_point() . Now, instead of having a set number of colors modeling our data, we see a gradient scale in the legend on the left-hand side of the chart. ggplot2 defaults to a blue color gradient, with lighter shades of blue representing heavier penguins. It is a bit difficult to discern the different shades, however, making this graph hard to read. A popular alterative these days is the viridis palette, which ggplot2 includes as an option.. # recall previous plot, but this time using scale_color_viridis_c() # note that the \"_c\" suffix indicates \"continuous\" data last_plot() + scale_color_viridis_c() . Now we have a better idea about how penguin body mass relates to bill length and depth. The scale_color_viridis_C() function is well known for having a wide range of color shades as well as being visible to color blind people. These simple examples are certainly not the end-all be-all, but they should hopefully demonstrate that there are lots of customization options available for your graphs. Ultimately, it comes down to personal preference and what type of data you are modeling. Make sure to keep in mind too how you should call attention to specific parts of your charts to best convey the information within the data. With that being said, the options are seemingly endless when it comes to color palettes in R, so play around with your favorite color palette packages to find the best option for you! . Some other fun, pre-defined color palette packages in R include: . | colorspace (link) | wesanderson (link) | . ",
    "url": "/Presentation/Figures/color_palettes.html#r",
    
    "relUrl": "/Presentation/Figures/color_palettes.html#r"
  },"271": {
    "doc": "Color Palettes",
    "title": "Stata",
    "content": "Stata has more limited native options to use colors and color palettes to graphs. However, thanks to Ben Jann package colrspace and palettes, it is relatively easy to extract and translate color palettes to be used in Stata. In addition, it is also possible to combine this packages with grstyle (also by Ben Jann), to modify the colors of the scheme in memory, to easily change the colors of most of your graphs. To facilitate further the use of colors in your graphs, I put together a wrapper that will combine the use of Ben Jann’s packages, to easily combine palettes and schemes, for most of your color needs. Setup . First, we need to install a few packages from ssc. * To modify schemes ssc install grstyle * To add color palettes and palettes translators in Stata net install palettes , replace from(\"https://raw.githubusercontent.com/benjann/palettes/master/\") net install colrspace, replace from(\"https://raw.githubusercontent.com/benjann/colrspace/master/\") * A wrapper for the commands above, plus adding other predefined palettes ssc install color_style . Second, lets load some data that we can use to demonstrate color palette options. For these examples, I will use iris.dta dataset. This has similar characteristics as the penguins dataset used in the R example above. The examples that follow will try to replicate the figures described in R. I will also use the scheme white, which is clear than the Stata default scheme. webuse iris, clear . Discrete Data . The first two examples will deal color palettes for discrete data, emphasizing on the use of color_style. We start by creating a simple scatter of sepal length and with by iris type, using default color options in Stata. Note that there is a new comand (mscatter in ssc) that can create scatterplots by groups more directly. However, I will stay with the simpler scatter for this example. two (scatter seplen sepwid if iris==1) /// (scatter seplen sepwid if iris==2) /// (scatter seplen sepwid if iris==3), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . This, however, generates dots with colors that are hard to differentiate. We could, instead use “tableau” colors: . color_style tableau two (scatter seplen sepwid if iris==1) /// (scatter seplen sepwid if iris==2) /// (scatter seplen sepwid if iris==3), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . Alternatively, we could use a more colorful palette option, such as RdYlGn. However, because this palette is made for continous data, you would do better if instead use color_style option n(#). color_style RdYlGn, n(3) two (scatter seplen sepwid if iris==1) /// (scatter seplen sepwid if iris==2) /// (scatter seplen sepwid if iris==3), /// legend(order(1 \"Sectosa\" 2 \"Versicolor\" 3 \"Virginica\")) . By using color_style function, we can change the colors used for the data points. However, using this method has the hard constrain of defining up to 15 different colors. Check the helpfile of colorpalette to see all predefined colorpalette options, and type color_style, list for additional palettes that come with this command. In addition to changing the color of data points, we can use color palettes to change the fill color of a graph object, which is particularly useful when creating bar charts, line graphs, boxPlots, or filling in confidence intervals. Let’s create a new example where we plot the means of sepal and petal length and width. color_style s2 graph bar (mean) seplen sepwid petlen petwid, title(\"S2color-Stata Default\") name(m1) color_style RdYlGn graph bar (mean) seplen sepwid petlen petwid, title(\"RdYlGn-up to 15\") name(m2) color_style RdYlGn, n(4) graph bar (mean) seplen sepwid petlen petwid, title(\"RdYlGn-for 4 groups\") name(m3) color_style google graph bar (mean) seplen sepwid petlen petwid, title(\"Google-Palette\") name(m4) graph combine m1 m2 m3 m4, nocopies . There could be, however, that a particular palette is not yet available in color_style, or colorpalette. If you have the HEX colors, you can still use them!. Here I use “Simpsons” and “Tron” hex colors for the bar graphs. ** Like Simpsons (ggsci) color_style #FED439 #709AE1 #8A9197 #D2AF81 graph bar (mean) seplen sepwid petlen petwid, title(\"S2color-Stata Default\") name(m5) ** Like Tron (ggsci) color_style #FF410D #6EE2FF #F7C530 #95CC5E graph bar (mean) seplen sepwid petlen petwid, title(\"RdYlGn-up to 15\") name(m6) graph combine m5 m6, nocopies . Continuous Data . The previous examples use a discrete variable to color the data by. While colorpalette allows you to set colors for continuous data as well, using interpolation of colors when appropriate, schemes can only manage up to 15 different groups. There are other user written commands, however that work with colorpalette on the background, and may allow you to do this kind of graphs . ** Like Scatter but for multiple groups. ssc install mscatter mscatter seplen sepwid, over(petlen) colorpalette(viridis) alegend msize(2) title(viridis) name(m7, replace) legend(col(2)) mscatter seplen sepwid, over(petlen) colorpalette(egypt) alegend msize(2) title(egypt) name(m8, replace) legend(col(2)) mscatter seplen sepwid, over(petlen) colorpalette(magma) alegend msize(2) title(magma) name(m9, replace) legend(col(2)) mscatter seplen sepwid, over(petlen) colorpalette(google) alegend msize(2) title(google) name(m10, replace) legend(col(2)) graph combine m7 m8 m9 m10, nocopies altshrink . While this approach provides an option for multiple levels of colors, some work is needed to have better formatted labels. These simple examples should demonstrate that there are lots of customization options available for your graphs. Ultimately, it comes down to personal preference and what type of data you are modeling. Make sure to keep in mind too how you should call attention to specific parts of your charts to best convey the information within the data. What is happening in the background . To understand what is happening in the background. color_style calls on colorpalette to identify colors for a particular palette. colorpalette comes loaded with many predefined palettes, which are constantly being updated. Once the colors are obtained, color_style calls on grstyle to modify all color attributes in the current scheme. color_style also comes with a good selection of palettes, that one may want to explore. By default, color_style uses all colors in a particular palette, recycling them if more colors are needed. However, one can request use fewer colors (say 3), for better color contrast. This may depend on the type of palette one is using. Using this method only allows you to change colors for up to 15 groups. Other programs exist, however, when one wants to use more than 15 groups of colors. ",
    "url": "/Presentation/Figures/color_palettes.html#stata",
    
    "relUrl": "/Presentation/Figures/color_palettes.html#stata"
  },"272": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Combining Datasets: Horizontal Combination (Deterministic)",
    "content": "A deterministic merge is when there is some variable(s) that uniquely and dependably identifies individual units (be it people, firms, teams, etc.) across datasets. For example, we might have two datasets containing information about the same set of people, one with their financial information, the other with their educational information. To analyze the relationship between the education and financial measures, we need them in the same dataset and so would want to combine them. If both datasets had a unique identification field for each person, such as a social security number or other national id, we could use this to match the records, so that all information from the same person appeared on the same line. Because we expect such identifiers to be unique to an individual (unlike many names, such as John Smith) and appear exactly the same in each dataset, we can use just this field to do the match, and don’t anticipate in ambiguity in determining which records match to each other. Thus, it is a deterministic merge. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#combining-datasets-horizontal-combination-deterministic",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#combining-datasets-horizontal-combination-deterministic"
  },"273": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Keep in mind",
    "content": ". | For any number of reasons, one or both of the datasets may have more than one observation per unit or individual. That may be for a good reason – such as havinng multiple test scores for the same student because they took exams at different points in time – or it may be redundant information. Understanding the structure of your data is key before embarking on a deterministic merge. | It is a good idea to have a clear sense of how much overlap you anticipate across your datasets. It is important to examine the results of your merge and see if it matches the amount the overlap you expected. Subtle differences in a matching variable (e.g. if leading zeroes are present in an ID variable for one variable but not another) can be a source of major headaches for your analysis if not caught early. If something looks weird in your results later in the project, trouble with a merge is a common cause. So check your merge results early and often. | . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#keep-in-mind"
  },"274": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Also Consider",
    "content": ". | Determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#also-consider",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#also-consider"
  },"275": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#implementations",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#implementations"
  },"276": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Julia",
    "content": "Julia provides a variety of join functions to combine two dataframes, including innerjoin, leftjoin, rightjoin, outerjoin, semijoin, antijoin, and crossjoin. We will consider the most common use cases, involving the leftjoin and the antijoin. using DataFrames gdp2018 = DataFrame(country=[\"UK\", \"USA\", \"France\"], currency=[\"GBP\", \"USD\", \"EUR\"], gdp_trillions=[2.1, 20.58, 2.78]) dollarvalue2018 = DataFrame(currency=[\"EUR\", \"GBP\", \"YEN\", \"USD\"], in_dollars=[1.104, 1.256, 0.00926, 1.0]) gdpandexchange = leftjoin(gdp2018, dollarvalue2018, on = :currency) # The antijoin is often useful as a way to obtain which rows in one dataframe are not in a another. # In this case, for which currencies do we have dollar value, but not country and GDP? missingcountries = antijoin(dollarvalue2018, gdp2018, on = :currency) . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#julia",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#julia"
  },"277": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Python",
    "content": "There are three main ways to join datasets horizontally in python using the merge function in pandas: one-to-one joins (e.g. two DataFrames joined on unique indexes), many-to-one joins (e.g. joining a unique index to one or more columns in a different DataFrame), and many-to-many joins (joining columns on columns). The column(s) to use as keys for the merge are specified with the on= keyword argument. The merges are different depending on if the merge is inner (use only those keys in both DataFrames), outer (use the cartesian product of all keys), left (use only keys in the left DataFrame), or right (use only keys in the right DataFrame). Outer joins will include entries for all possible combinations of columns. Further details can be found in the pandas documentation. import pandas as pd gdp_2018 = pd.DataFrame({'country': ['UK', 'USA', 'France'], 'currency': ['GBP', 'USD', 'EUR'], 'gdp_trillions': [2.1, 20.58, 2.78]}) dollar_value_2018 = pd.DataFrame({'currency': ['EUR', 'GBP', 'YEN', 'USD'], 'in_dollars': [1.104, 1.256, .00926, 1]}) # Perform a left merge, which discards 'YEN' GDPandExchange = pd.merge(gdp_2018, dollar_value_2018, how='left', on='currency') . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#python",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#python"
  },"278": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "R",
    "content": "There are several ways to combine data sets horizontally in R, including base-R merge and several different approaches in the data.table package. We will be using the join functions in the dplyr package. # If necessary, install dplyr # install.packages('dplyr') library(dplyr) # This data set contains information on GDP in local currency GDP2018 &lt;- data.frame(Country = c(\"UK\", \"USA\", \"France\"), Currency = c(\"Pound\", \"Dollar\", \"Euro\"), GDPTrillions = c(2.1, 20.58, 2.78)) # This data set contains dollar exchange rates DollarValue2018 &lt;- data.frame(Currency = c(\"Euro\", \"Pound\", \"Yen\", \"Dollar\"), InDollars = c(1.104, 1.256, .00926, 1)) . Next we want to join together GDP2018 and DollarValue2018 so we can convert all the GDPs to dollars and compare them. There are three kinds of observations we could get - observations in GDP2018 but not DollarValue2018, observations in DollarValue2018 but not GDP2018, and observations in both. Use help(join) to pick the variant of join that keeps the observations we want. The “Yen” observation won’t have a match, and we don’t need to keep it. So let’s do a left_join and list GDP2018 first, so it keeps matched observations, plus any observations only in GDP2018. GDPandExchange &lt;- left_join(GDP2018, DollarValue2018) . The join function will automatically detect that the Currency variable is shared in both data sets and use it to join them. Generally, you will want to be sure that the set of variables you are joining by uniquely identifies observations in at least one of the data sets you are joining. If you’re not sure whether that’s true, see Determine the observation level of a data set, or run join through the safe_join from the pmdplyr package. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#r",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#r"
  },"279": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Stata",
    "content": "A Quick Prelude About “Master” And “Using” Datasets . When merging two datasets together, there are two relevant datasets to consider. The first is the one currently in Stata’s memory, the other is whatever dataset (not currently loaded into Stata) that you would like to merge onto the dataset in memory. For ease of reference, Stata calls the dataset in memory the “master” dataset and the other file the “using” dataset. When you see the syntax of the merge command, the reason for calling it the “using” dataset will become clear. In Stata, there are 3 types of deterministic merges: . 1-to-1 . A one-to-one merge expects there to be no more than one row in each dataset to have a matched pair in the other dataset. If there is more than one observation with the same identifying variable(s) in either the master or using datasets when attempting to do a one-to-one merge, Stata will throw an error. (Note: you can check to see if there is more than one observation per identifying variable by using the “duplicates report” command, or the Gtools variant for especially large datasets, called “gduplicates report.”) . *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse autosize.dta, clear merge 1:1 make using http://www.stata-press.com/data/r14/autoexpense.dta . Note that the syntax specifies “make” as the identifying variable after the merge type (1:1) and before the using statement (thus why we call the data not in memory the “using” data. The result of this merge shows 5 successful matches and one observation from the master dataset that did not have a match. | Result | # of obs. | _merge value | . | not matched from master | 1 | (_merge==1) | . | not matched from using | 0 | (_merge==2) | . |   |   |   | . | matched | 5 | (_merge==3) | . Note that Stata creates a new variable (_merge) during the merge that stores the merge status of each observation, where a value of 1 means that the observation was only found in the master dataset, 2 means it was found only in the using dataset, and 3 means it was found in both and successfully merged. Many-to-1 . A many-to-one merge occurs when the master dataset contains multiple observations of the same unit or individual (say, multiple test scores for the same student), while the using dataset has only one observation per unit or individual (say, the age of each student). Here is the syntax for a many-to-1 merge. *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse sforce.dta, clear merge m:1 region using http://www.stata-press.com/data/r14/dollars.dta . Note that in this case that the syntax changes from merge 1:1 to merge m:1 where m stands for many. In this case, the identifying variable is “region.” . 1-to-Many . A one-to-many merge is the opposite of a many to one merge, with multiple observations for the same unit or individual in the using rather than the master data. The only different in the syntax is that it becomes merge 1:m rather than merge m:1. Many-to-Many . A many-to-many merge is intended for use when there are multiple observations for each combination of the set of merging variables in both master and using data. However, merge m:m has strange behavior that is effectively never what you want, and it is not recommended. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#stata",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#stata"
  },"280": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Horizontal Combination (Deterministic)",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html"
  },"281": {
    "doc": "Combining Datasets",
    "title": "Combining Datasets Overview",
    "content": "There are two main ways to combine data: vertically and horizontally. That is, you can want to combine observations (adding new variables) or combine variables (adding new observations). This is perhaps easiest to show visually: . Individual Name Info . | Name | ID | . | John Smith | A63240 | . | Desiree Thomas | B78242 | . Individual Age Info . | ID | Age | . | B78242 | 22 | . | A63240 | 27 | . In the case above, we would like to combine two datasets, the Individual Name Info and the Individual Date Info, that have different information about the same people, who are identified by the ID variable. The result from the merge would be to have a new dataset with more columns than the original datasets because it contains all of the information for each individual from both of the original datasets. Here we have to combine the files according to the ID variable, placing the information from observations with the same ID on the same row in the combined dataset. Alternatively, the below example has two datasets that collect the same information about different people. We would like to combine these datasets vertically, with the result containing more rows than the original dataset, because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. | Name | ID | Age | . | John Smith | A63240 | 22 | . | Desiree Thomas | B78242 | 27 | . | Name | ID | Age | . | Teresa Suarez | Y34208 | 19 | . | Donald Akliberti | B72197 | 34 | . These ways of combining data are referred to by different names across different programming languages, but will largely be referred to by one common set of terms (used by Stata and Python’s Pandas): merge for horizontal combination and append for for vertical combination. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html#combining-datasets-overview",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html#combining-datasets-overview"
  },"282": {
    "doc": "Combining Datasets",
    "title": "Combining Datasets",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html"
  },"283": {
    "doc": "Vertical Combination",
    "title": "Combining Datasets: Vertical Combination",
    "content": "When combining two datasets that collect the same information about different people, they get combined vertically because they have variables in common but different observations. The result of this combination will more rows than the original dataset because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. It is a “vertical” combination in the sense that one set of observations gets added to the bottom of the other set of observations. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#combining-datasets-vertical-combination",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#combining-datasets-vertical-combination"
  },"284": {
    "doc": "Vertical Combination",
    "title": "Keep in Mind",
    "content": ". | Vertical combinations require datasets to have variables in common to be of much use. That said, it may not be necessary for the two datasets to have exactly the same variables. Be aware of how your statistical package handles observations for a variable that is in one dataset but not another (e.g. are such observations set to missing?). | It may be the case that the datasets you are combining have the same variables but those variables are stored differently (numeric vs. string storage types). Be aware of how the variables are stored across datasets and how your statistical package handles attempts to combine the same variable with different storage types (e.g. Stata throws an error and will now allow the combination, unless the “, force” option is specified.) | . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#keep-in-mind"
  },"285": {
    "doc": "Vertical Combination",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#implementations",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#implementations"
  },"286": {
    "doc": "Vertical Combination",
    "title": "Julia",
    "content": "The primary library used for datasets in Julia is DataFrames, which is the equivalent of python’s pandas. The simplest way to combine two dataframes is via the cat function, where we specify the axis. However, most of the time people use the vcat version, which combines rows by default. using DataFrames, RDatasets # Load the mtcars dataset to match the R example cars = dataset(\"dataset\", \"mtcars\") # split into two. Indices are 1-based. mtcars1 = cars[1:10, :] mtcars2 = cars[11:32, :] # combine them back together the simplest way. Works with any number mtcarswhole = vcat(mtcars1, mtcars2) . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#julia",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#julia"
  },"287": {
    "doc": "Vertical Combination",
    "title": "Python",
    "content": "pandas is by far the most widely-used library for data manipulation in python. The concat function concatenates datasets vertically and combines datasets even if they don’t contain the exact same set of variables. It’s also possible to concatenate dataframes horizontally by passing the function the keyword argument axis=1. import pandas as pd # Load California Population data from the internet df_ca = pd.read_stata('http://www.stata-press.com/data/r14/capop.dta') df_il = pd.read_stata('http://www.stata-press.com/data/r14/ilpop.dta') # Concatenate a list of the dataframes (works on any number of dataframes) df = pd.concat([df_ca, df_il]) . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#python",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#python"
  },"288": {
    "doc": "Vertical Combination",
    "title": "R",
    "content": "There are several ways to vertically combine data sets in R, including rbind. We will use the dplyr package function bind_rows, which allows the two data sets to combine even if they don’t contain the exact same set of variables. # If necessary, install dplyr # install.packages('dplyr') library(dplyr) # Load in mtcars data data(mtcars) # Split it in two, so we can combine them back together mtcars1 &lt;- mtcars[1:10,] mtcars2 &lt;- mtcars[11:32,] # Use bind_rows to vertically combine the data sets mtcarswhole &lt;- bind_rows(mtcars1, mtcars2) . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#r",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#r"
  },"289": {
    "doc": "Vertical Combination",
    "title": "Stata",
    "content": "* Load California Population data webuse http://www.stata-press.com/data/r14/capop.dta // Import data from the web append using http://www.stata-press.com/data/r14/ilpop.dta // Merge on Illinois population data from the web . You can also append multiple datasets at once, by simply listing both datasets separated by a space: . * Load California Population data * Import data from the web webuse http://www.stata-press.com/data/r14/capop.dta * Merge on Illinois and Texas population data from the web append using http://www.stata-press.com/data/r14/ilpop.dta http://www.stata-press.com/data/r14/txpop.dta . Note that, if there are columns in one but not the other of the datasets, Stata will still append the two datasets, but observations from the dataset that did not contain those columns will have their values for that variable set to missing. * Load Even Number Data webuse odd.dta, clear append using http://www.stata-press.com/data/r14/even.dta . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#stata",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#stata"
  },"290": {
    "doc": "Vertical Combination",
    "title": "Vertical Combination",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html",
    
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html"
  },"291": {
    "doc": "Create a Conda Package (Python)",
    "title": "Create a Conda Package (Python)",
    "content": "Warning This is a page aimed at git experts at the moment. We welcome suggestions for pushing its difficulty down. Sometimes there are packages that are available in the Python ecosystem generally but which have not yet been incorporated into the conda-forge package repository for Anaconda. This page shows you the correct way to install that package into your environment in the interim and how to get that package added to the conda-forge repository. ",
    "url": "/Other/create_a_conda_package.html",
    
    "relUrl": "/Other/create_a_conda_package.html"
  },"292": {
    "doc": "Create a Conda Package (Python)",
    "title": "Installing the package locally",
    "content": "First, create a temporary directory where we’ll do all of our work. In the following snippet we call that directory TMP_DIR. Second, identify the package you’d like to install that is currently available via pip. In what follows, we’ll use linearmodels as an example. PACKAGE=linearmodels TMP_DIR=\"~/tmp/{PACKAGE}_build\" # Create the temporary directory mkdir -p $TMP_DIR &amp;&amp; cd $TMP_DIR # Conda boilerplate conda skeleton pypi $PACKAGE conda build -c conda-forge $PACKAGE # Install the package conda install --use-local -c conda-forge linearmodels . ",
    "url": "/Other/create_a_conda_package.html#installing-the-package-locally",
    
    "relUrl": "/Other/create_a_conda_package.html#installing-the-package-locally"
  },"293": {
    "doc": "Create a Conda Package (Python)",
    "title": "Setting up an Anaconda feedstock for your package",
    "content": "The above solves the problem for you locally, but not for the broader community! In this section, we’ll describe generic instructions for creating an Anaconda feedstock for a Python 3 package. There are detailed instructions here, but we found them a bit confusing for relatively simple packages. First, an overview of the steps: . | Fork the conda-forge/staged-recipes repo on GitHub. | Create the skeleton recipe for your package. | Edit the skeleton recipe. | Create a PR for your recipe. | Work with conda-forge maintainers until your recipe is merged. | . Forking the conda-forge/staged-recipes repo . On GitHub, go to conda-forge/staged-recipes and fork the repository. If you already have a fork, I recommend deleting your fork and reforking. This repository is highly volatile. Creating the skeleton recipe . As above, we’ll assume you’re working with the linearmodels package. We’ll also assume you’re working in REPO_DIR as defined below. You’ll need to run the below scripts. REPO_DIR=~/repo GITHUB_USER=khwilson PACKAGE=linearmodels LOCAL_REPO_DIR=\"${REPO_DIR}/staged-recipes\" # Make sure your REPO_DIR exists mkdir -p $REPO_DIR &amp;&amp; cd $REPO_DIR # Pull your fork git clone \"https://github.com/${GITHUB_USER}/staged-recipes.git\" cd $LOCAL_REPO_DIR # Create the skeleton recipe cd \"recipes\" conda skeleton pypi $PACKAGE . You should now see a file in “${PACKAGE}/meta.yaml”. This is your skeleton recipe. Edit the recipe . You’ll need to make several changes to the skeleton. These can be divided into: . | Make the recipe py3k only. | If your recipe involves Cython, adding some build requirements. | Getting the license setup. | Other “about” metadata. | Add your name to the list of maintainers. | . Make the recipe py3k only . In the build section, add a key skip: True # [py2k] . If your recipe involves Cython . In the requirements section: . | Add a build section that looks like this: ```yaml | . build: . | {{ compiler(‘c’) }} | {{ compiler(‘cxx’) }} | . * In the `host` section, change the line involving `pip` to `pip &gt;=10`. * In the `run` section, remove any references to Cython and pip. #### Getting the license setup This is probably the most annoying part. Find your packages online home. Copy the LICENSE file to your recipe's directory. Then in the `about` section, add the line `license_file: NAME_OF_LICENSE_FILE`, where `NAME_OF_LICENSE_FILE` is the file you saved the license to. Also, you'll need to describe the license by type. Sometimes, conda can figure this out for you. However, sometimes you'll need to figure it out. To do so, you can typically dump large parts of the text into Google and it will tell you the name. In the worst case, you may need to physically search [this page](https://opensource.org/licenses/alphabetical). Once you've found the license, add `license: SPDX_SHORT_CODE` to the `about` block. N.B. You may also need to add a `license_family` to the `about` block. However, the errors should tell you how to do this. :-) #### Other `about` metatdata There are some other stubs in the `about` section that conda should have setup for you, specifically around docs. If you can find these online, then add them here. #### Add your name to the list of maintainers Finally, at the bottom of the `meta.yaml` file, you will need a section that looks like: ```yaml extra: recipe-maintainers: - khwilson . Here you should obviously replace khwilson with your own GitHub username. This completes the main part of recipe editing. Create a PR for your recipe . Before creating a PR for your recipe, you probably want to test your recipe locally. To do so, you’ll need to have docker installed. Then run the following: . cd $LOCAL_REPO_DIR # Remove extra recipes rm -rf recipes/example recipes/spyrmsd . Then in the .circleci/build_steps.sh file, comment out the line that starts git ls-tree --name-only master -- .. Then you should be able to run ./.circleci/run_docker_build.sh. You’ll probably see some errors which you’ll need to fix. Once these errors are sorted out, you can push your recipe to GitHub and create a PR. Make sure to name the PR something memorable, e.g., Adding linearmodels recipe. ",
    "url": "/Other/create_a_conda_package.html#setting-up-an-anaconda-feedstock-for-your-package",
    
    "relUrl": "/Other/create_a_conda_package.html#setting-up-an-anaconda-feedstock-for-your-package"
  },"294": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "Creating a Variable with Group Calculations",
    "content": "Many data sets have hierarchical structures, where individual observations belong within certain groups. For example, data with students inside classrooms inside schools. Or companies inside countries. See the below table for one example . | I | J | X | . | 1 | 1 | 3 | . | 1 | 2 | 3.5 | . | 2 | 1 | 2 | . | 2 | 2 | 5 | . Here, we have data where each group \\(I\\) has multiple rows, one for each \\(J\\). We often might want to create a new variable that performs a calculation within each group, and assigns the result to each value in that group. For example, perhaps we want to calculate the mean of \\(X\\) within each group \\(I\\), so we can know how far above or below the group average each observation is. Our goal is: . | I | J | X | AvgX | . | 1 | 1 | 3 | 3.25 | . | 1 | 2 | 3.5 | 3.25 | . | 2 | 1 | 2 | 3.5 | . | 2 | 2 | 5 | 3.5 | . ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html"
  },"295": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "Also Consider",
    "content": ". | If the goal is to produce a data set with one row per group rather than a new variable assigning the calculation to each observation in the group, then instead look at Collapse a Data Set. | . ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#also-consider",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#also-consider"
  },"296": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#implementations",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#implementations"
  },"297": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "Julia",
    "content": "The DataFrames.jl package makes data aggregation and manipulaion relatively straigthforward. # Load required packages using CSV # Import .csv files using DataFrames # Working with data frames using Statistics # Required to calculate a mean # Import .csv file from GitHub and store as a DataFrame storms = CSV.read(download(\"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\"), DataFrame) # Use 'groupby' to aggregate data by groups (namely: name, year, month and day # columns) and use 'transform!' to add a new column called 'mean_wind' # containing the mean of the existing 'wind' column. (The `!` means that this # change will be made in-place.) transform!(groupby(storms, [:name, :year, :month, :day]), :wind=&gt; mean =&gt; :mean_wind) . ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#julia",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#julia"
  },"298": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "Python",
    "content": "pandas accomplishes this by using the groupby-transform approach. We can either call numpy’s mean function or use a lambda and apply the .mean() method to each group . import pandas as pd # Pull in data on storms storms = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv') # Use groupby and agg to perform a group calculation # Here it's a mean, but it could be any function storms['mean_wind'] = storms.groupby(['name','year','month','day'])['wind'].transform(lambda x: x.mean()) # this tends to be a bit faster because it uses an existing function instead of a lambda import numpy as np storms['mean_wind'] = storms.groupby(['name','year','month','day'])['wind'].transform(np.mean) . Though the above may be a great way to do it, it certainly seems complex. There is a much easier way to achieve similar results that is easier on the eyes (and brain!). This is using panda’s aggregate() method with tuple assignments. This results in the most easy-to-understand way, by using the aggregate method after grouping since this would allow us to follow a very simple format of new_column_name = ('old_column', 'agg_funct'). So, for example: . import pandas as pd # Pull in data on storms storms = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv') # Use groupby and group the columns and perform group calculations # The below calculations aren't particularly indicative of a good analysis, # but give a quick look at a few of the calculations you can do df = ( storms .groupby(by=['name', 'year', 'month', 'day']) #group .aggregate( avg_wind = ('wind', 'mean'), max_wind = ('wind', 'max'), med_wind = ('wind', 'median'), std_pressure = ('pressure', 'std'), first_year = ('year', 'first') ) .reset_index() # Somewhat similar to ungroup. Removes the grouping from the index ) . ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#python",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#python"
  },"299": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "R",
    "content": "In R, we can use either the dplyr or data.table package to do this. Here’s how to do it with dplyr… . library(dplyr) data(\"storms\") # The dataset is bundled with dplyr, so we'll just open directly # Use 'group_by' to designate the groups and 'mutate' to create new column(s). # Note that dplyr doesn't modify in-place, so we need to reassign the result. storms = storms %&gt;% group_by(name, year, month, day) %&gt;% mutate(mean_wind = mean(wind)) . …and here’s how to do it with data.table. library(data.table) # storms = fread(\"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\") setDT(storms) # Set the already-loaded storms DF as a data.table # Use ':=' for in-place modification storms[, mean_wind := mean(wind), by = .(name, year, month, day)] . ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#r",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#r"
  },"300": {
    "doc": "Creating a Variable with Group Calculations",
    "title": "Stata",
    "content": "This process is made easy in Stata using the egen command. egen is not entirely flexible and is limited to a set of predetermined group calculations (see help egen), although this can be expanded using the egenmore package (ssc install egenmore then help egenmore). However, the list of predetermined functions is long enough that you’ll rarely be caught out! . import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", clear * Use bysort to determine the grouping, and egen to do the calculation bysort name year month day: egen mean_wind = mean(wind) . ",
    "url": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#stata",
    
    "relUrl": "/Data_Manipulation/creating_a_variable_with_group_calculations.html#stata"
  },"301": {
    "doc": "Creating Categorical Variables",
    "title": "Creating Categorical Variables",
    "content": "Many variables are categorical in nature. Each observation takes one of a set list of values, which are mutually exclusive. We may want to create these variables from raw data, assigning the category based on the values of other variables. For example, we may create a simplified four or five-category race variable based on a self-reported open-ended “race” question on a survey. Or we may want to create income bins based on splitting up a continuous variable. This page will explore how to consider a set of conditions and assigning a category based on those conditions. For example, maybe everyone with a value of birthplace == 'USA' and citizenship == 'USA' is in the ‘natural-born USA citizen’ category. ",
    "url": "/Data_Manipulation/creating_categorical_variables.html",
    
    "relUrl": "/Data_Manipulation/creating_categorical_variables.html"
  },"302": {
    "doc": "Creating Categorical Variables",
    "title": "Keep in Mind",
    "content": ". | Make sure that the categories you’re going for are truly mutually exclusive, or you’ll have to figure out what to do with overlaps later when you find them! | . ",
    "url": "/Data_Manipulation/creating_categorical_variables.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/creating_categorical_variables.html#keep-in-mind"
  },"303": {
    "doc": "Creating Categorical Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/creating_categorical_variables.html#implementations",
    
    "relUrl": "/Data_Manipulation/creating_categorical_variables.html#implementations"
  },"304": {
    "doc": "Creating Categorical Variables",
    "title": "Python",
    "content": "We can use the filtering operation in pandas to only assign the categorical value to the rows that satisfy the condition. import pandas as pd # and purely for the dataset import statsmodels.api as sm mtcars = sm.datasets.get_rdataset('mtcars').data # Now we go through each pair of conditions and group assignments, # using loc to only send that group assignment to observations # satisfying the given condition mtcars.loc[(mtcars.mpg &lt;= 19) &amp; (mtcars.hp &lt;= 123), 'classification'] = 'Efficient and Non-Powerful' mtcars.loc[(mtcars.mpg &gt; 19) &amp; (mtcars.hp &lt;= 123), 'classification'] = 'Inefficient and Non-Powerful' mtcars.loc[(mtcars.mpg &lt;= 19) &amp; (mtcars.hp &gt; 123), 'classification'] = 'Efficient and Powerful' mtcars.loc[(mtcars.mpg &gt; 19) &amp; (mtcars.hp &gt; 123), 'classification'] = 'Inefficient and Powerful' . There’s another way to achieve the same outcome using lambda functions. In this case, we’ll create a dictionary of pairs of classification names and conditions, for example 'Efficient': lambda x: x['mpg'] &lt;= 19. We’ll then find the first case where the condition is true for each row and create a new column with the paired classification name. # Dictionary of classification names and conditions expressed as lambda functions conds_dict = { 'Efficient and Non-Powerful': lambda x: (x['mpg'] &lt;= 19) &amp; (x['hp'] &lt;= 123), 'Inefficient and Non-Powerful': lambda x: (x['mpg'] &gt; 19) &amp; (x['hp'] &lt;= 123), 'Efficient and Powerful': lambda x: (x['mpg'] &lt;= 19) &amp; (x['hp'] &gt; 123), 'Inefficient and Powerful': lambda x: (x['mpg'] &gt; 19) &amp; (x['hp'] &gt; 123), } # Find name of first condition that evaluates to True mtcars['classification'] = mtcars.apply(lambda x: next(key for key, value in conds_dict.items() if value(x)), axis=1) . There’s quite a bit to unpack here! .apply(lambda x: ..., axis=1) applies a lambda function rowwise to the entire dataframe, with individual columns accessed by, for example, x['mpg']. (You can apply functions on an index using axis=0.) The next keyword returns the next entry in a list that evaluates to true or exists (so in this case it will just return the first entry that exists). Finally, key for key, value in conds_dict.items() if value(x) iterates over the pairs in the dictionary and returns only the condition names (the ‘keys’ in the dictionary) for conditions (the ‘values’ in the dictionary) that evaluate to true. Once again, just like R, Python has many ways of doing the same thing. Some with more complex, but efficient (runtime) manners, while others being slightly slower but many times easier to understand and follow-along with it’s closeness of natural-language syntax. So, for this example, we will use numpy and pandas together, to achieve both an efficient runtime and a relatively simple syntax. from seaborn import load_dataset import pandas as pd import numpy as np mtcars = load_dataset('mpg') # Create our list of index selections conditionList = [ (mtcars['mpg'] &lt;= 19) &amp; (mtcars['horsepower'] &lt;= 123), (mtcars['mpg'] &gt; 19) &amp; (mtcars['horsepower'] &lt;= 123), (mtcars['mpg'] &lt;= 19) &amp; (mtcars['horsepower'] &gt; 123), (mtcars['mpg'] &gt; 19) &amp; (mtcars['horsepower'] &gt; 123) ] # Create the results we will pair with the above index selections resultList = [ 'Efficient and Non-powerful', 'Inefficient and Non-powerful', 'Efficient and Powerful', 'Inefficient and Powerful' ] df = ( mtcars .assign( # Run the numpy select classification = np.select(condlist=conditionList, choicelist=resultList, default='Not Considered' ) ) # Convert from object to categorical .astype({'classification' :'category'}) ) \"\"\" Be a more purposeful programmer/analyst/data scientist: Using the default parameter in np.select() allows you to fill in the values with that specific text wherever your criteria is not considered. For example, if you search this data, you will see there are a few rows where horesepower is null. The original criteria we built does not considering null, so it would be populated with \"Not Considered\" allowing you to find those values and correct them, or set checks for them in a pipeline. \"\"\" . ",
    "url": "/Data_Manipulation/creating_categorical_variables.html#python",
    
    "relUrl": "/Data_Manipulation/creating_categorical_variables.html#python"
  },"305": {
    "doc": "Creating Categorical Variables",
    "title": "R",
    "content": "We will create a categorical variable in two ways, first using case_when() from the dplyr package, and then using the faster fcase() from the data.table package. library(dplyr) data(mtcars) mtcars &lt;- mtcars %&gt;% mutate(classification = case_when( mpg &lt;= 19 &amp; hp &lt;= 123 ~ 'Efficient and Non-Powerful', # Here we list each pair of conditions and group assignments mpg &gt; 19 &amp; hp &lt;= 123 ~ 'Inefficient and Non-Powerful', mpg &lt;= 19 &amp; hp &gt; 123 ~ 'Efficient and Powerful', mpg &gt; 19 &amp; hp &gt; 123 ~ 'Inefficient and Powerful' )) %&gt;% mutate(classification = as.factor(classification)) # Storing categorical variables as factors often makes sense . Now, using fcase() in data.table, which has similar syntax to case_when() except it uses more commas instead of ~. library(data.table) data(mtcars) mtcars &lt;- as.data.table(mtcars) mtcars[, classification := fcase( mpg &lt;= 19 &amp; hp &lt;= 123, 'Efficient and Non-Powerful', # Here we list each pair of conditions and group assignments mpg &gt; 19 &amp; hp &lt;= 123, 'Inefficient and Non-Powerful', mpg &lt;= 19 &amp; hp &gt; 123, 'Efficient and Powerful', mpg &gt; 19 &amp; hp &gt; 123, 'Inefficient and Powerful' )] # Storing categorical variables as factors often makes sense mtcars[, classification := as.factor(classification)] . ",
    "url": "/Data_Manipulation/creating_categorical_variables.html#r",
    
    "relUrl": "/Data_Manipulation/creating_categorical_variables.html#r"
  },"306": {
    "doc": "Creating Categorical Variables",
    "title": "Stata",
    "content": "There are several ways to turn conditionals into a categorical variable in Stata. However, the easiest way that requires learning the least lingo is just to use good ol’ replace with if. sysuse auto.dta, clear * Use if on each step, including generate, so that * observations that satisfy none of the conditions end up missing g classification = \"Efficient and Non-Powerful\" if mpg &lt;= 19 &amp; gear_ratio &lt;= 2.9 replace classification = \"Inefficient and Non-Powerful\" if mpg &gt; 19 &amp; gear_ratio &lt;= 2.9 replace classification = \"Efficient and Powerful\" if mpg &lt;= 19 &amp; gear_ratio &gt; 2.9 replace classification = \"Inefficient and Powerful\" if mpg &gt; 19 &amp; gear_ratio &gt; 2.9 . ",
    "url": "/Data_Manipulation/creating_categorical_variables.html#stata",
    
    "relUrl": "/Data_Manipulation/creating_categorical_variables.html#stata"
  },"307": {
    "doc": "Creating Dummy Variables",
    "title": "Introduction",
    "content": "Creating a dummy variable can be just like creating any other variable but dummy variables can only take the value of 0 or 1 (or false or true). This gives us even more options in how we decide to add dummies. Dummy variables are often used as a way of including categorical variables in a model. ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#introduction",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#introduction"
  },"308": {
    "doc": "Creating Dummy Variables",
    "title": "Keep in Mind",
    "content": "Factor class vectors are automatically treated as dummies in regression models in R (Stata and SW languages have similar capabilities). In order to transform a categorical vector to a factor class you can simply use factor() on the variable in regression in R, or i. in Stata. This means you don’t have to create a different dummy vector for every value. If you are interested in looking behind the scenes you can use model.matrix() to see how R is creating dummies from these factor class variables. Note: model.matrix() creates a separate dummy column for all values in the vector. This is called one-hot encoding and, if you aren’t careful, can lead to the dummy variable trap if an intercept is also included in the regression. The dummy variable trap arises because of perfect multicollinearity between the intercept term and the dummy variables (which row-wise all add up to 1). So one of the columns needs to be dropped from the regression in order for it to run. Typically, the first variable is the one which is dropped and effectively absorbed into the intercept term. If this happens then all the dummy estimates will be in reference to the dropped dummy. ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#keep-in-mind"
  },"309": {
    "doc": "Creating Dummy Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#implementations",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#implementations"
  },"310": {
    "doc": "Creating Dummy Variables",
    "title": "Python",
    "content": "Several python libraries have functions to turn categorical variables into dummies, including pandas, scikit-learn (where it is called OneHotEncoder), and statsmodels (where it is called categorical). This example uses pandas get_dummies function. import pandas as pd # Create a dataframe df = pd.DataFrame({'colors': ['red', 'green', 'blue', 'red', 'blue'], 'numbers': [5, 13, 1, 7, 5]}) # Replace the colors column with a dummy column for each color df = pd.get_dummies(df, columns=['colors']) . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#python",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#python"
  },"311": {
    "doc": "Creating Dummy Variables",
    "title": "Julia",
    "content": "Here’s an implementation example in Julia for creating dummy variables using the OneHotEncoder function from the MLBase package: . ```using MLBase . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#julia",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#julia"
  },"312": {
    "doc": "Creating Dummy Variables",
    "title": "Create an array of categorical data",
    "content": "colors = [“red”, “green”, “blue”, “red”, “blue”] . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#create-an-array-of-categorical-data",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#create-an-array-of-categorical-data"
  },"313": {
    "doc": "Creating Dummy Variables",
    "title": "Encode the categorical data into dummy variables",
    "content": "encoded_colors = MLBase.OneHotEncoder()(colors) . In this example, we start by importing the MLBase package, which provides the OneHotEncoder function for creating dummy variables. Next, we define an array colors that represents the categorical variable we want to encode. In this case, it contains different colors. To create the dummy variables, we use the OneHotEncoder() function and apply it to the colors array. The function automatically encodes the categorical data into a matrix of dummy variables. After executing this code, the encoded_colors variable will contain a matrix where each row represents an observation, and each column represents a category from the original colors array. The values in the matrix are either 0 or 1, indicating whether a particular category is present for each observation. For example, if we print the encoded_colors matrix, it might look like this: . 3×3 Matrix{Float64}: 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 . In this case, the first column represents the category \"red\", the second column represents \"green\", and the third column represents \"blue\". The value of 1 in each row indicates the presence of that category for that particular observation, while the value of 0 indicates the absence of that category. This encoding allows you to use the resulting matrix of dummy variables in machine learning models or other data analysis tasks where numerical data is required. ## R Turning a categorical variable into a set of dummies ```r data(iris) # To retain the column of dummies for the first # categorical value we remove the intercept model.matrix(~-1+Species, data=iris) # Then we can add the dummies to the original data iris &lt;- cbind(iris, model.matrix(~-1+Species, data=iris)) # Of course, in a regression we can skip this process summary(lm(Sepal.Length ~ Petal.Length + Species, data = iris)) . If you are only creating one dummy at a time rather than a set from a factor variable, creating a dummy variable doesn’t have to be any different than creating any other variable. Below are several ways to create a new variable in R. dplyr::mutate . Let’s say that we want our dummy to indicate if variable_1 &gt; variable_2. To do this we can use mutate: . # If necessary, install dplyr # install.packages('dplyr') library(dplyr) data(iris) # The below takes existing data (iris) and adds # a new variable (Long.Petal) based on existing variables # (Petal.Length and Petal.Width) and saves the result as # mutated_data. # Note: new variables do not have to be based on old # variables mutated_data = iris %&gt;% mutate(Long.Petal = Petal.Length &gt; Petal.Width) . This will create a new column of logical (TRUE/FALSE) variables. This works just fine for most uses of dummy variables. However if you need the variables to be 1s and 0s you can now take . mutated_data &lt;- mutated_data %&gt;% mutate(Long.Petal = Long.Petal*1) . You could also nest that operation inside the original creation of new_dummy like so: . mutated_data = iris %&gt;% mutate(Long.Petal = (Petal.Length &gt; Petal.Width)*1) . Base R . #the following creates a 5 x 2 data frame letters = c(\"a\",\"b\",\"c\", \"d\", \"e\") numbers = c(1,2,3,4,5) df = data.frame(letters,numbers) . Now I’ll show several different ways to create a dummy indicating if the numbers variable is odd. df$dummy = df$numbers%%2 df$dummy = ifelse(df$numbers%%2==1,1,0) df$dummy = df$numbers%%2==1 # the last one created a logical outcome to convert to numerical we can either df$dummy = df$dummy * 1 # or df$dummy = (df$numbers%%2==1) *1 . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#encode-the-categorical-data-into-dummy-variables",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#encode-the-categorical-data-into-dummy-variables"
  },"314": {
    "doc": "Creating Dummy Variables",
    "title": "MATLAB",
    "content": "Categorical to Dummy . The equivalent of model.matrix() in MATLAB is dummyvar which creates columns of one-hot encoded dummies from categorical variables. The following example is taken from MathWorks documentation. Colors = {'Red';'Blue';'Green';'Red';'Green';'Blue'}; Colors = categorical(Colors); D = dummyvar(Colors) . Other Dummies . In MATLAB you can store variables as columns in arrays. If you know you are going to add columns multiple times to the same array it is best practice to pre-allocate the final size of the array for computational efficiency. If you do this you can simply select the column you are designating for your dummy variable and story the dummys in that column. arr = [1,2,3;5,2,6;1,8,3]; dum = sum(data(:,:),2) &lt;10; data = horzcat(arr,dum); . In the above script I make a 3 by 3 array, then create a 3 x 1 array of dummy variables indicating if the sum of the rows are less than 10. Then I horizontally concatenate the arrays together. I should note that in MATLAB logicals are automatically stored as 1s and 0s instead of T/F like in R. ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#matlab",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#matlab"
  },"315": {
    "doc": "Creating Dummy Variables",
    "title": "Stata",
    "content": "In Stata, if we have a categorical variable stored as a number, we can use i. to turn it into a set of dummies, or include it directly in a regression. sysuse auto.dta, clear * Let's get the brand of the car g brand = word(make,1) * Turn it into a numerically coded categorical encode brand, g(brand_n) * include in a regression regress mpg weight i.brand_n * Or create a set of dummies * specifying the prefix so it's easy to refer to * Note this actually does not require * numeric encoding xi i.brand, pre(b_) regress mpg weight b_* * Create a logical variable gen highmpg = mpg &gt; 30 . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#stata",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#stata"
  },"316": {
    "doc": "Creating Dummy Variables",
    "title": "Creating Dummy Variables",
    "content": " ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html",
    
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html"
  },"317": {
    "doc": "Creating a Time Series Dataset",
    "title": "Creating a Time Series Dataset",
    "content": "Time-series estimators are, by definition, a function of the temporal ordering of the observations in the estimation sample. So a number of programmed time-series econometric routines can only be used if the software is instructed ahead of time that it is working with a time-series dataset. ",
    "url": "/Time_Series/creating_time_series_dataset.html",
    
    "relUrl": "/Time_Series/creating_time_series_dataset.html"
  },"318": {
    "doc": "Creating a Time Series Dataset",
    "title": "Keep in Mind",
    "content": ". | Date data can be notoriously difficult to work with. Be sure before declaring your data set as a time series that your date variable has been imported properly. | As an example, we will use data on U.S. quarterly real Gross Domestic Product (GDP). To get an Excel spreadsheet holding the GDP data, go to the Saint Louis Federal Reserve Bank FRED website. | . ",
    "url": "/Time_Series/creating_time_series_dataset.html#keep-in-mind",
    
    "relUrl": "/Time_Series/creating_time_series_dataset.html#keep-in-mind"
  },"319": {
    "doc": "Creating a Time Series Dataset",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/creating_time_series_dataset.html#implementations",
    
    "relUrl": "/Time_Series/creating_time_series_dataset.html#implementations"
  },"320": {
    "doc": "Creating a Time Series Dataset",
    "title": "Python",
    "content": "pandas supports time series data. Here is an example which downloads quarterly data, casts the date column (read in as an object series) as a datetime series, and creates a year-quarter column. import pandas as pd # Read in data gdp = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") # Convert date column to be of data type datetime64 gdp['DATE'] = pd.to_datetime(gdp['DATE']) # Create a column with quarter-year combinations gdp['yr-qtr'] = gdp['DATE'].apply(lambda x: str(x.year) + '-' + str(x.quarter)) . ",
    "url": "/Time_Series/creating_time_series_dataset.html#python",
    
    "relUrl": "/Time_Series/creating_time_series_dataset.html#python"
  },"321": {
    "doc": "Creating a Time Series Dataset",
    "title": "R",
    "content": "There are many different kinds of time series data set objects in R. Instead of R-based time series objects such as ts, zoo and xts, here we will use tsibble, will preserves time indices as the essential data column and makes heterogeneous data structures possible. The tsibble package extends the tidyverse to temporal data and built on top of the tibble, and so is a data- and model-oriented object. For more detail information for using tsibble such as key and index, check the tsibble page and the Introduction to tsibble. STEP 1) Load necessary packages . # If necessary # install.packages(c('tsibble','tidyverse')) library(tsibble) library(tidyverse) . STEP 2) Import data into R. gdp &lt;- read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") # read.csv() has read in our date variable as a factor. We need a date! gdp$DATE &lt;- as.Date(gdp$DATE) # If it were a little less well-behaved than this, we could use the lubridate package to fix it. STEP 3) Convert a date variable formats to quarter . gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) . By applying yearmonth() to the index variable (referred to as .), it creates new variable named qtr with a quarter interval which corresponds to the year-quarter for the original variable DATE. Since the tsibble handles regularly-spaced temporal data whereas our data (GDPC1) has an irregular time interval (since it’s not the exact same number of days between quarters every time), we set the option regular = FALSE. Now, we have a quarterly time-series dataset with the new variable date. References for more information: . | If you want to learn how to build various types of time-series forecasting models, Forecasting: Principles and Practice provides very useful information to deal with time-series data in R. | If you need more detail information on tssible, visit the tsibble page or tsibble on RDRR.io. | The fable packages provides a collection of commonly used univariate and multivariate time-series forecasting models. For more information, visit fable. | . ",
    "url": "/Time_Series/creating_time_series_dataset.html#r",
    
    "relUrl": "/Time_Series/creating_time_series_dataset.html#r"
  },"322": {
    "doc": "Creating a Time Series Dataset",
    "title": "Stata",
    "content": "STEP 1) Import Data to Stata . import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", clear . STEP 2) Generate the new date variable . generate date_index = tq(1947q1) + _n-1 . The function tq() converts a date variable for each of the above formats to an integer value (starting point of our data is 1947q1). _n is a Stata command gives the index number of the current row. STEP 3) Index the new variable format as quarter . format date_index %tq . This command will format date_index as a vector of quarterly dates which corresponds to our original date variable observation date. STEP 4) Convert a variable into time-series data . tsset date_index . Now, we have a quarterly Stata time-series dataset. Any data you add to this file in the future will be interpreted as time-series data. ",
    "url": "/Time_Series/creating_time_series_dataset.html#stata",
    
    "relUrl": "/Time_Series/creating_time_series_dataset.html#stata"
  },"323": {
    "doc": "Data Manipulation",
    "title": "Data Manipulation Techniques",
    "content": " ",
    "url": "/Data_Manipulation/data_manipulation.html#data-manipulation-techniques",
    
    "relUrl": "/Data_Manipulation/data_manipulation.html#data-manipulation-techniques"
  },"324": {
    "doc": "Data Manipulation",
    "title": "Data Manipulation",
    "content": " ",
    "url": "/Data_Manipulation/data_manipulation.html",
    
    "relUrl": "/Data_Manipulation/data_manipulation.html"
  },"325": {
    "doc": "Decision Trees",
    "title": "Decision Trees",
    "content": "Decision trees are among the most common and useful machine learning methodologies. While they are a relatively simple method, they are incredibly easy to understand and implement for both classification and regression problems. A decision tree “grows” by creating a cutoff point (often called a split) at a single point in the data that maximizes accuracy. The tree’s prediction is then based on the mean of the region that results from the input data. For both regression and classification trees, it is important to optimize the number of splits that we allow the tree to make. If there is no limit, the trees would be able to create as many splits as the data will allow. This would mean the tree could perfectly “predict” every value from the training dataset, but would perform terribly out of sample (i.e., overfit the data). As such, it is important to keep a reasonable limit on the number of splits. This is achieved by creating a penalty that the algorithm has to pay in order to perform another split. If the increase in accuracy is worth more than the penalty, it will make the split. For regression trees, the decision to split along a continuum of values is often made by minimizing the residual sum of squares: . \\[minimize \\sum(y-prediction)^2\\] This should be highly reminiscent of ordinary least squares. Where this differs is in the number of splits created, the binary nature of the splits, and its nonlinear nature. The methodology behind classificiation is very similar, except the splits are decided by minimizing purity, such as the Gini index: . \\[G= 1 - \\sum_{i = 1}^{C} (p_{i})^2\\] The goal here is to create regions with as of classifications as possible, as such, a smaller Gini index implies a more pure region. ",
    "url": "/Machine_Learning/decision_trees.html",
    
    "relUrl": "/Machine_Learning/decision_trees.html"
  },"326": {
    "doc": "Decision Trees",
    "title": "Keep in Mind",
    "content": ". | While decision trees are easy to interpret and understand, they often underpreform relative to other machine learning methodologies. | Even though they may not offer the best predictions, decision trees excel at identifying key variables in the data. | . ",
    "url": "/Machine_Learning/decision_trees.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/decision_trees.html#keep-in-mind"
  },"327": {
    "doc": "Decision Trees",
    "title": "Also Consider",
    "content": ". | Decision trees are the basis for all tree-based methodologies. More robust methods, such as Random Forests, are a collection of decision trees that aggregate their decisions into a single prediction. These forests are often more useful for predictive modeling. | . ",
    "url": "/Machine_Learning/decision_trees.html#also-consider",
    
    "relUrl": "/Machine_Learning/decision_trees.html#also-consider"
  },"328": {
    "doc": "Decision Trees",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/decision_trees.html#implementations",
    
    "relUrl": "/Machine_Learning/decision_trees.html#implementations"
  },"329": {
    "doc": "Decision Trees",
    "title": "Python",
    "content": "The easiest way to get started with decision trees in Python is to use the scikit-learn package. In the example below, we’ll use data on the passengers of the Titanic to build a classification tree that predicts whether passengers survived or not (binary outcome) based on properties such as passenger age, gender as recorded in the data, and class of cabin. As ever with machine learning, it’s essential that an out-of-sample set, also known as a test set, is retained and used to score the final model. # Install sklearn and pandas using pip or conda, if you don't have them already. # Note that the 'f-strings' used in the print statements below are only available in Python&gt;=3.6. from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.metrics import plot_confusion_matrix import pandas as pd titanic = pd.read_csv(\"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv\", index_col=0) # Let's ensure the columns we want to treat as continuous are indeed continuous by using pd.to_numeric # The errors = 'coerce' keyword argument will force any values that cannot be # cast into continuous variables to become NaNs. continuous_cols = ['age', 'fare'] for col in continuous_cols: titanic[col] = pd.to_numeric(titanic[col], errors='coerce') # Set categorical cols &amp; convert to dummies cat_cols = ['sex', 'pclass'] for col in cat_cols: titanic[col] = titanic[col].astype('category').cat.codes # Clean the dataframe. An alternative would be to retain some rows with missing values by giving # a special value to nan for each column, eg by imputing some values, but one should be careful not to # use information from the test set to impute values in the training set if doing this. Strictly speaking, # we shouldn't be dropping the nans from the test set here (as we pretend we don't know what's in it) - but # for the sake of simplicity, we will. titanic = titanic.dropna() # Create list of regressors regressors = continuous_cols + cat_cols # Predicted var y_var = ['survived'] # Create a test (25% of data) and train set train, test = train_test_split(titanic, test_size=0.25) # Now let's create an empty decision tree to solve the classification problem: clf = tree.DecisionTreeClassifier(max_depth=10, min_samples_split=5, ccp_alpha=0.01) # The last option, ccp_alpha, prunes low-value complexity from the tree to help # avoid overfitting. # Fit the tree with the data clf.fit(train[regressors], train[y_var]) # Let's take a look at the tree: tree.plot_tree(clf) # How does it perform on the train and test data? train_accuracy = round(clf.score(train[regressors], train[y_var]), 4) print(f'Accuracy on train set is {train_accuracy}') test_accuracy = round(clf.score(test[regressors], test[y_var]), 4) print(f'Accuracy on test set is {test_accuracy}') # Show the confusion matrix plot_confusion_matrix(clf, test[regressors], test[y_var]) # Although it won't be the same from run to run, this model scored around 80% # out of sample, and has slightly more false positives than false negatives. ",
    "url": "/Machine_Learning/decision_trees.html#python",
    
    "relUrl": "/Machine_Learning/decision_trees.html#python"
  },"330": {
    "doc": "Decision Trees",
    "title": "R",
    "content": "# Load packages library(rpart) library(rpart.plot) library(caret) library(rattle) # We will utilize data regarding passengers on their survival. We have multiple pieces of information on every passenger, including passenger age, sex, cabin number, and class. # Our goal is to build a decision tree that can predict whether or not passengers survived the wreck, making it a classification tree. These same methodologies can be used and applied to a regression tree framework. # Read in the data titanic &lt;- read.csv(\"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv\") # Set a seed for reproducability set.seed(1234) # The data is clean for the most part, but some variables have been read in as factors instead of numeric variables, so we can fix that with the following code. titanic$age &lt;- as.numeric(titanic$age) titanic$fare &lt;- as.numeric(titanic$fare) # As with all machine learning methodologies, we want to create a test and a training dataset # Take a random sample of the data, here we have chosen to use 75% for training and 25% for validation samp_size &lt;- floor(0.75*nrow(titanic)) train_index &lt;- sample(seq_len(nrow(titanic)),size=samp_size,replace=FALSE) train &lt;- titanic[train_index, ] test &lt;- titanic[-train_index, ] # Now that we have our test and train datasets, we can build our trees. Here, we will use the package \"rpart\". Other packages, such as \"ranger\" are also viable options. # Here we can pick some variables we think would be good, the tree will decide which ones are best. Some data we have isn't useful, such as an individual's name or the random ID we assigned passengers, so there is no need to include them. basic_tree &lt;- rpart( survived ~ pclass + sex + age + fare + embarked, # our formula data=train, method = \"class\", # tell the model we are doing classification minsplit=2, # set a minimum number of splits cp=.02 # set an optional penalty rate. It is often useful to try out many different ones, use the caret package to test many at once ) basic_tree # plot it using the packages we loaded above fancyRpartPlot(basic_tree,caption=\"Basic Decision Tree\") # This plot gives a very intuitive visual representation on what is going on behind the scenes. # Now we should predict using the test data we left out! predictions &lt;- predict(basic_tree,newdata=test,type=\"class\") # Make the numeric responses as well as the variables that we are testing on into factors predictions &lt;- as.factor(predictions) test$survived &lt;- as.factor(test$survived) # Create a confusion matrix which tells us how well we did. confusionMatrix(predictions,test$survived) # This particular model got ~80% accuracy. This varies each time if you do not set a seed. Much better than a coin toss, but not great. With some additional tuning a decision tree can be much more accurate! Try it for yourself by changing the factors that go into the prediction and the penalty rates. ",
    "url": "/Machine_Learning/decision_trees.html#r",
    
    "relUrl": "/Machine_Learning/decision_trees.html#r"
  },"331": {
    "doc": "Density Discontinuity Tests for Regression Discontinuity",
    "title": "Density Discontinuity Tests for Regression Discontinuity",
    "content": "The Regression Discontinuity Design can be applied in cases where a running variable (a.k.a. forcing variable) determines treatment, at least partially, and where the treatment assignment changes significantly at a cutoff variable. In sharp designs, there is no treatment if the running variable is to one side of the cutoff, but everyone with a running variable to the other side of the cutoff is treated. Regression discontinuity in all its forms (sharp, fuzzy, kink, etc.) relies on an assumption that the running variable is not being manipulated around the cutoff value. For example, imagine the running variable is a math test score, the treatment is getting to skip a grade in math, and treatment assignment is that you get to skip a grade if your score is above 90 (cutoff). If someone knows they’re about to get an 89 and so works extra hard because they know how close they are to the cutoff, or if a grader observes someone getting an 89, decides they’re close enough, and fudges their score to be a 90 so they can skip a grade, then the running variable is being manipulated. You can no longer assume that people just on either side of the cutoff are comparable, and the research design falls apart. Density discontinuity tests are intended to check for the presence of manipulation at the cutoff. Generally, they examine the density distribution of the running variable, and look for a discontinuity in the density function just to either side of the cutoff. If there is one (in our math score example, perhaps we see hardly anyone with an 88 or 89, but lots of people with 90 or 91), then the running variable is probably manipulated. There are several ways to perform this test, all of them descending from McCrary (2008). This page will also show the test based on Cattaneo, Jansson, and Ma (2020), which requires fewer choices of tuning parameters, and takes fuller advantage of local polynomial regression. ",
    "url": "/Model_Estimation/Research_Design/density_discontinuity_test.html",
    
    "relUrl": "/Model_Estimation/Research_Design/density_discontinuity_test.html"
  },"332": {
    "doc": "Density Discontinuity Tests for Regression Discontinuity",
    "title": "Keep in Mind",
    "content": ". | Density discontinuity tests can only detect the presence of manipulation that would change the distribution of the running variable. If manipulation takes the form of the person who chooses the cutoff intentionally placing it for their own benefit, or the running variable being manipulated in both directions (in our test score example, some strange test grader doesn’t just kick a few 88-89 scores up to 90-91, but instead manipulates people both in and out of treatment, also replacing some 90-91 scores with 88-89s), then the density distribution test will not detect this. As with any research design, carefully consider if your assumptions make sense, don’t rely only on a statistical test. | . ",
    "url": "/Model_Estimation/Research_Design/density_discontinuity_test.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Research_Design/density_discontinuity_test.html#keep-in-mind"
  },"333": {
    "doc": "Density Discontinuity Tests for Regression Discontinuity",
    "title": "Also Consider",
    "content": ". | The page on Regression Discontinuity Design and Regression Kink Design | . ",
    "url": "/Model_Estimation/Research_Design/density_discontinuity_test.html#also-consider",
    
    "relUrl": "/Model_Estimation/Research_Design/density_discontinuity_test.html#also-consider"
  },"334": {
    "doc": "Density Discontinuity Tests for Regression Discontinuity",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/density_discontinuity_test.html#implementations",
    
    "relUrl": "/Model_Estimation/Research_Design/density_discontinuity_test.html#implementations"
  },"335": {
    "doc": "Density Discontinuity Tests for Regression Discontinuity",
    "title": "R",
    "content": "# If necessary # install.packages(c('rdd','rddensity')) # Load RDD of house elections from the R package rddtools, # and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 df &lt;- read.csv(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Regression_Discontinuity_Design/house.csv\") ## Run McCrary (2008) version of the test library(rdd) # Give it the running variable and the cutpoint # it will automatically produce a plot and select the number of bins and the bandwidth # The output will be hte p-value for the presence of a discontinuity DCdensity(df$x, c = 0) ## Run the Cattaneo, Jansson, and Ma (2020) estimator library(rddensity) # Give it the running variable and the cutoff # It will pick the bandwidth, and has default polynomials, kernel, and bias correction # It doesn't have bins to pick denstest &lt;- rddensity(df$x, c = 0) summary(denstest) # Not plot the density discontinuity # It needs the density test object we just made rdplotdensity(denstest, df$x) . ",
    "url": "/Model_Estimation/Research_Design/density_discontinuity_test.html#r",
    
    "relUrl": "/Model_Estimation/Research_Design/density_discontinuity_test.html#r"
  },"336": {
    "doc": "Density Discontinuity Tests for Regression Discontinuity",
    "title": "Stata",
    "content": "In Stata, code for the original McCrary (2008) test cannot be installed from the command line like most packages. If you wish to perform the original version of the test, you can find installation instructions and example code at McCrary’s website. The Cattaneo, Jansson, and Ma (2020) version is easier to install, and code follows: . * if necessary * ssc install rddensity * Load RDD of house elections from the R package rddtools, * and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Regression_Discontinuity_Design/house.csv\", clear * We need a running variable and a cutoff, other defaults can be seen in hte help file * the \"plot\" option adds a plot while we're at it rddensity x, c(0) plot . ",
    "url": "/Model_Estimation/Research_Design/density_discontinuity_test.html#stata",
    
    "relUrl": "/Model_Estimation/Research_Design/density_discontinuity_test.html#stata"
  },"337": {
    "doc": "Density Plots",
    "title": "Introduction",
    "content": "A density plot visualises the distribution of data over a continuous interval (or time period). Density Plots are not affected by the number of bins (each bar used in a typical histogram) used, thus, they are better at visualizing the shape of the distribution than a histogram unless the bins in the histogram have a theoretical meaning. ",
    "url": "/Presentation/Figures/density_plots.html#introduction",
    
    "relUrl": "/Presentation/Figures/density_plots.html#introduction"
  },"338": {
    "doc": "Density Plots",
    "title": "Keep in Mind",
    "content": ". | Notice that the variable on the x-axis should be continuous. Density plots are not designed for use with discrete variables. | . ",
    "url": "/Presentation/Figures/density_plots.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/density_plots.html#keep-in-mind"
  },"339": {
    "doc": "Density Plots",
    "title": "Also Consider",
    "content": "= You might also want to know how to make a histogram or a line graph, click Histogram or Line graph for more information. ",
    "url": "/Presentation/Figures/density_plots.html#also-consider",
    
    "relUrl": "/Presentation/Figures/density_plots.html#also-consider"
  },"340": {
    "doc": "Density Plots",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/density_plots.html#implementations",
    
    "relUrl": "/Presentation/Figures/density_plots.html#implementations"
  },"341": {
    "doc": "Density Plots",
    "title": "Python",
    "content": "In this example, we’ll use seaborn, a declarative plotting library that provides a quick and easy way to produce density plots. It builds on matplotlib. # You may need to install seaborn on the command line using 'pip install seaborn' or 'conda install seaborn' import seaborn as sns # Set a theme for seaborn sns.set_theme(style=\"darkgrid\") # Load the example diamonds dataset diamonds = sns.load_dataset(\"diamonds\") # Take a look at the data print(diamonds.head()) . | | carat | cut | color | clarity | depth | table | price | x | y | z | . | 0 | 0.23 | Ideal | E | SI2 | 61.5 | 55.0 | 326 | 3.95 | 3.98 | 2.43 | . | 1 | 0.21 | Premium | E | SI1 | 59.8 | 61.0 | 326 | 3.89 | 3.84 | 2.31 | . | 2 | 0.23 | Good | E | VS1 | 56.9 | 65.0 | 327 | 4.05 | 4.07 | 2.31 | . | 3 | 0.29 | Premium | I | VS2 | 62.4 | 58.0 | 334 | 4.20 | 4.23 | 2.63 | . | 4 | 0.31 | Good | J | SI2 | 63.3 | 58.0 | 335 | 4.34 | 4.35 | 2.75 | . sns.kdeplot(data=diamonds, x=\"price\", cut=0); . This is basic, but there are lots of ways to adjust it through keyword arguments (you can see these by running help(sns.kdeplot)) or via calling functions on the matplotlib ax object that running sns.kdeplot returns when not followed by ;. In this simple example, the cut keyword argument forces the density estimate to end at the end-points of the data–which makes sense for a variable like price, which has a hard cut-off at 0. Let’s use further keyword arguments to enrich the plot, including different colours (‘hues’) for each cut of diamond. One keyword argument that may not be obvious is hue_order. The default function call would have arranged the cut types so that the ‘Fair’ cut obscured the other types, so the argument passed to the hue_order keyword below reverses the order of the unique list of diamond cuts via [::-1]. sns.kdeplot(data=diamonds, x=\"price\", hue=\"cut\", hue_order=diamonds['cut'].unique()[::-1], fill=True, alpha=.4, linewidth=0.5, cut=0.); . ",
    "url": "/Presentation/Figures/density_plots.html#python",
    
    "relUrl": "/Presentation/Figures/density_plots.html#python"
  },"342": {
    "doc": "Density Plots",
    "title": "R",
    "content": "For this R demonstration, we are going to use ggplot2 package to create a density plot. Additionally, we will use the dataset diamonds that is made available by the ggplot2 package. To begin with this R demonstration, make sure that we install and load all the useful packages that we need it. # load necessary packages library(ggplot2) library(viridis) library(RColorBrewer) library(tidyverse) library(ggthemes) library(ggpubr) library(datasets) . Next, in order to make a density plot, we are going to use the ggplot() and geom_density() functions. We will specify price as our x-axis. ggplot(diamonds, aes(x = price)) + geom_density() . We can always change the color of the density plot using the col argument and fill the color inside the density plot using fill argument. Furthermore, we can specify the degree of transparency density fill area using the argument alpha where alpha ranges from 0 to 1. ggplot(diamonds, aes(x = price))+ geom_density(fill = \"lightblue\", col = 'black', alpha = 0.6) . We can also change the type of line of the density plot as well by adding linetype= inside geom_density(). ggplot(diamonds, aes(x = price)) + geom_density(fill = \"lightblue\", col = 'black', linetype = \"dashed\") . Furthermore, you can also combine both histogram and density plots together. ggplot(diamonds, aes(x = price)) + geom_histogram(aes(y = ..density..), colour = \"black\", fill = \"grey45\") + geom_density(col = \"red\", size = 1,linetype = \"dashed\") . What happen if we want to make multiple densities? . For example, we want to make multiple densities plots for price based on the type of cut, all we need to do is adding fill=cut inside aes(). ggplot(data=diamonds, aes(x = price, fill = cut)) + geom_density(adjust = 1.5, alpha = .3) . ",
    "url": "/Presentation/Figures/density_plots.html#r",
    
    "relUrl": "/Presentation/Figures/density_plots.html#r"
  },"343": {
    "doc": "Density Plots",
    "title": "Stata",
    "content": "For this demonstration, we will use the plotplainblind scheme, a community-contributed color and grpah scheme for plots that greatly improves over Stata’s default plot color schemes especially for colorblind viewers. For more on using schemes in Stata, see here. * Install the blindschemes set of graph schemes, including plottig ssc install blindschemes * Shows the set of available schemes graph query, schemes * Load diamonds data import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv\", clear . We can build a basic density plot using the kdensity subcommand of twoway: . * Plot the kernel density with plotplain theming twoway kdensity price, scheme(plotplainblind) . To overlay densities for multiple variables or multiple groups, it is possible to use the standard twoway graph stacking syntax: . * Plot the density of two separate columns twoway (kdensity depth) (kdensity table), scheme(plotplainblind) * Plot the same variable separately by group, overlaid on a single set of axes twoway (kdensity price if cut == \"Fair\", lcolor(blue)) (kdensity price if cut == \"Good\", lcolor(red)), scheme(plotplainblind) legend(lab(1 \"Cut: Fair\") lab(2 \"Cut: Good\")) . However, the syntax for doing separate densities by group can get onerous very quickly with more than a handful of groups, noting that you’ll have to specify each group with an if by hand, be careful about the color/presentation of each line, and do the legend yourself. Much easier for by-group kernel densities is the mkdensity package, which still uses kdensity under the hood, but just handles some of this busywork for you. On the other hand it doesn’t accept a scheme() option. But you can still use it via set scheme. The downside of this approach, rather than doing it by hand, is that it relies on . set scheme plotplainblind * if necessary, install with ssc install mkdensity mkdensity price, over(cut) . ",
    "url": "/Presentation/Figures/density_plots.html#stata",
    
    "relUrl": "/Presentation/Figures/density_plots.html#stata"
  },"344": {
    "doc": "Density Plots",
    "title": "Density Plots",
    "content": " ",
    "url": "/Presentation/Figures/density_plots.html",
    
    "relUrl": "/Presentation/Figures/density_plots.html"
  },"345": {
    "doc": "Desired Nonexistent Pages",
    "title": "Desired Nonexistent Pages",
    "content": "This is a manually-maintained list of pages that have been linked from elsewhere, or that it would be nice to have, but do not exist yet. Feel free to edit with your own wishes! Also look below for a list of pages that exist but only have one language, and so could use some expanding. A page does not have to be listed here for you to add it! These are just the things that we thought of. There’s certainly plenty more in the world of statistical techniques to be added. If you create one of these pages, please remove it from this list. ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html"
  },"346": {
    "doc": "Desired Nonexistent Pages",
    "title": "Data Manipulation",
    "content": " ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#data-manipulation",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#data-manipulation"
  },"347": {
    "doc": "Desired Nonexistent Pages",
    "title": "Geo-Spatial",
    "content": ". | Handling Raster Data | Handling Vector Data | Spatial Moving Average Model | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#geo-spatial",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#geo-spatial"
  },"348": {
    "doc": "Desired Nonexistent Pages",
    "title": "Model Estimation",
    "content": "OLS . | MANOVA | . GLS . | Nonlinear Instrumental Variables Estimation | Fixed effects in Generalized Linear Models | Nonparametric regression | Conditional Logit | Endogenous Switching Model | Nonparametric Sample Selection Models | Treatment Effect Model | . Multilevel Models . | Hierarchical Bayes Conditional Logit | Multilevel Regression with Poststratification | Nonlinear Mixed Effects Models | . Matching . | Nearest-Neighbor Distance Matching | Coarsened Exact Matching | . Research Design . | Differences in Differences with treatment rollout (use updated methods!) | Event Study Estimation | . Statistical Inference . | Nonlinear hypothesis tests | . Nonstandard Errors . | Cluster Bootstrap Standard Errors | Jackknife Standard Errors | Non-Standard Bootstrap Standard Errors (bootstrap standard errors that aren’t built in to the command and must be done “by hand”, i.e. for matching) | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#model-estimation",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#model-estimation"
  },"349": {
    "doc": "Desired Nonexistent Pages",
    "title": "Machine Learning",
    "content": ". | A-B Testing | Artificial Neural Networks | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#machine-learning",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#machine-learning"
  },"350": {
    "doc": "Desired Nonexistent Pages",
    "title": "Presentation",
    "content": "Figures . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#presentation",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#presentation"
  },"351": {
    "doc": "Desired Nonexistent Pages",
    "title": "Time Series",
    "content": ". | Stationarity and Weak Dependence | Kalman Filtering/Smoothing | ARIMAX Model | TARCH Model | Recursive Regressions | State Dependent Regression | Structural Break (Chow Test) | Dynamic Panel | Ex-Post Forecasting | Ex-Ante Forecasting | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#time-series",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#time-series"
  },"352": {
    "doc": "Desired Nonexistent Pages",
    "title": "Other",
    "content": ". | Import a Fixed-Width Data File | Export Data to a Foreign Format | Power Analysis of Interaction Terms | How to create an ado package | How to create an R package | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#other",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#other"
  },"353": {
    "doc": "Desired Nonexistent Pages",
    "title": "List of One-Language Pages",
    "content": "This is a list of pages that exist but only have an example in one language, and so are ripe for expanding. Easy contributions, too, since the outline of the page is already written, and you can (should) use the same data as in the example already provided. ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#list-of-one-language-pages",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#list-of-one-language-pages"
  },"354": {
    "doc": "Desired Nonexistent Pages",
    "title": "Currently Python Only",
    "content": " ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#currently-python-only",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#currently-python-only"
  },"355": {
    "doc": "Desired Nonexistent Pages",
    "title": "Currently R Only",
    "content": ". | Geospatial: Merging Shape Files | Model Estimation: ANOVA | Model Estimation: Tobit Regression | Model Estimation: Stepwise Regression | Model Estimation: Propensity Score Matching | Presentation: Styling Line Graphs | Presentation: Graph Themes | Presentation: Animated Graphs | Presentation: Sankey Diagrams | Time Series: Granger Causality | Time Series: Linear Gaussian State Space Models | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#currently-r-only",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#currently-r-only"
  },"356": {
    "doc": "Desired Nonexistent Pages",
    "title": "Currently Stata Only",
    "content": " ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#currently-stata-only",
    
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#currently-stata-only"
  },"357": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Determine the Observation Level of a Data Set",
    "content": "The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . | I | J | X | . | 1 | 1 | 3 | . | 1 | 2 | 3.5 | . | 2 | 1 | 2 | . | 2 | 2 | 4.5 | . the variables \\(I\\) and \\(J\\) uniquely identify rows. The first row has \\(I = 1\\) and \\(J = 1\\), and there is no other row with that combination. We could also say that \\(X\\) uniquely identifies rows, but in this example \\(X\\) is not a case-identifying variable, it’s actual data. When working with data that has case-identifier variables, like panel data, it’s generally a good idea to know what set of them makes up the observation level of a data set. Otherwise you might perform merges or case-level calculations incorrectly. ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html"
  },"358": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Keep in Mind",
    "content": ". | As in the above example, it’s easy to uniquely identify rows using continuous data. But the goal is to figure out which case-identifying variables, like an individual’s ID code, or a country code, or a time code, uniquely identify rows. Make sure you only try these variables. | Even if you think you know what the observation level is, it’s good to check. Lots of data is poorly behaved! | . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#keep-in-mind"
  },"359": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Also Consider",
    "content": ". | You can collapse a data set to switch from one observation level to another, coarser one. | . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#also-consider",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#also-consider"
  },"360": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#implementations",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#implementations"
  },"361": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Python",
    "content": "To check for duplicate rows when using pandas dataframes, you can call duplicated or, to omit the duplicates, drop_duplicates. # Use conda or pip to install pandas if you don't already have it installed import pandas as pd storms = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv') # Find the duplicates by name, year, month, day, and hour level_variables = ['name', 'year', 'month', 'day', 'hour'] storms[storms.duplicated(subset=level_variables)] # Drop these duplicates, but retain the first occurrence of each storms = storms.drop_duplicates(subset=level_variables, keep='first') . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#python",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#python"
  },"362": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "R",
    "content": "# If necessary, install dplyr # install.packages('dplyr') # We do not need dplyr to detect the observation level # But we will use it to get data, and for our alternate approach library(dplyr) # Get data on storms from dplyr data(\"storms\") # Each storm should be identified by # name, year, month, day, and hour # anyDuplicated will return 0 if there are no duplicate combinations of these # so if we get 0, the variables in c() are our observation level. anyDuplicated(storms[,c('name','year','month','day','hour')]) # We get 2292, telling us that row 2292 is a duplicate (and possibly others!) # We can pick just the rows that are duplicates of other rows for inspection # (note this won't get the first time that duplicate shows up, just the subsequent times) duplicated_rows &lt;- storms[duplicated(storms[,c('name','year','month','day','hour')]),] # Alternately, we can use dplyr storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up mutate(number_duplicates = n()) %&gt;% # Then take that variable out pull(number_duplicates) %&gt;% # And get the maximum of it max() # If the result is 1, then we have found the observation level. If not, we have duplicates. # We can pick out the rows that are duplicated for inspection # by filtering on n(). This approach will give you every time the duplicate appears. duplicated_rows &lt;- storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up filter(n() &gt; 1) . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#r",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#r"
  },"363": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Stata",
    "content": "* Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * duplicates report followed by a variable list will show how many times * each combination shows up. * I think there is one observation level for each location, so I'll check that duplicates report latitude longitude * If I am correct, then the only number in the \"Copies\" column will be 1. * But it looks like I was not correct. * duplicates tag will create a binary variable with 1 for all duplicates * so I can examine the problem more closely * (duplicates examples is another option) duplicates tag latitude longitude, g(duplicated_data) * If I want to know not just whether there are duplicates but how many * of each there are for when I look more closely, I can instead do by latitude longitude, sort: g number_of_duplicates_in_this_group = _N . For especially large datasets the Gtools version of the various duplicates commands, gduplicates, is a great option . * Install gtools if necessary * ssc install gtools * Recreate the two duplicates tasks from above gduplicates report latitude longitude gduplicates tag latitude longitude, g(g_duplicated_data) . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#stata",
    
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#stata"
  },"364": {
    "doc": "Entropy Balancing",
    "title": "Entropy Balancing",
    "content": "Entropy balancing is a method for matching treatment and control observations that comes from Hainmueller (2012). It constructs a set of matching weights that, by design, forces certain balance metrics to hold. This means that, like with Coarsened Exact Matching there is no need to iterate on a matching model by performing the match, checking the balance, and trying different parameters to improve balance. However, unlike coarsened exact matching, entropy balancing does not require enormous data sets or drop large portions of the sample. Entropy balancing requires a set of balance conditions to be provided. These are often of the form “the mean of matching variable \\(A\\) must be the same between treated and control observations,” i.e. \\[\\sum_{i|D_i=0}w_iA_i = \\sum_{i|D_i=1}A_i\\] where \\(D_i\\) indicates treatment status and \\(w_i\\) are the matching weights, and similarly for other variables for which the mean should match. However, other conditions can also be included, such as matching to equalize the variance of a matching variable, or the skewness, and so on. This is sort of like the Generalized Method of Moments . Then, the entropy balancing algorithm searches for a set of matching weights \\(w_i\\) that best satisfies the set of balance conditions. These matching weights can then be used to weight an analysis comparing the treated and control groups to remove measured confounding between them. ",
    "url": "/Model_Estimation/Matching/entropy_balancing.html",
    
    "relUrl": "/Model_Estimation/Matching/entropy_balancing.html"
  },"365": {
    "doc": "Entropy Balancing",
    "title": "Keep in Mind",
    "content": ". | You must specify all the conditions you would like to match on. Adding more and more conditions may improve the amount of balance, but also make it more likely that the match process will fail. | . ",
    "url": "/Model_Estimation/Matching/entropy_balancing.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Matching/entropy_balancing.html#keep-in-mind"
  },"366": {
    "doc": "Entropy Balancing",
    "title": "Also Consider",
    "content": ". | Coarsened Exact Matching | Propensity Score Matching | Nearest-Neighbor Distance Matching | . ",
    "url": "/Model_Estimation/Matching/entropy_balancing.html#also-consider",
    
    "relUrl": "/Model_Estimation/Matching/entropy_balancing.html#also-consider"
  },"367": {
    "doc": "Entropy Balancing",
    "title": "Implementations",
    "content": "These examples use data from Broockman (2013). ",
    "url": "/Model_Estimation/Matching/entropy_balancing.html#implementations",
    
    "relUrl": "/Model_Estimation/Matching/entropy_balancing.html#implementations"
  },"368": {
    "doc": "Entropy Balancing",
    "title": "R",
    "content": "Entropy balancing can be implemented in R using the ebal package. # R CODE library(ebal); library(tidyverse) br &lt;- read_csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Matching/Data/broockman2013.csv') # Outcome Y &lt;- br %&gt;% pull(responded) # Treatment D &lt;- br %&gt;% pull(leg_black) # Matching variables X &lt;- br %&gt;% select(medianhhincom, blackpercent, leg_democrat) %&gt;% # Add square terms to match variances if we like mutate(incsq = medianhhincom^2, bpsq = blackpercent^2) %&gt;% as.matrix() eb &lt;- ebalance(D, X) # Get weights for usage elsewhere # Noting that this contains only control weights br_treat &lt;- br %&gt;% filter(leg_black == 1) %&gt;% mutate(weights = 1) br_con &lt;- br %&gt;% filter(leg_black == 0) %&gt;% mutate(weights = eb$w) br &lt;- bind_rows(br_treat, br_con) # Compare outcome (responded) between groups after matching m &lt;- lm(responded ~ leg_black, data = br, weights = weights) summary(m) . ",
    "url": "/Model_Estimation/Matching/entropy_balancing.html#r",
    
    "relUrl": "/Model_Estimation/Matching/entropy_balancing.html#r"
  },"369": {
    "doc": "Entropy Balancing",
    "title": "Stata",
    "content": "Entropy balancing can be implemented in Stata using the ebalance package. * STATA CODE * If necessary: * ssc install ebalance import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Matching/Data/broockman2013.csv\", clear * Specify the treatment and matching variables * And then in targets() specify which moments to match * 1 for means, 2 for variances, 3 for skew * Let's do means and variances for our continuous variables * and just means for our binary matching variable (leg_democrat) * and store the resulting weights in wt ebalance leg_black medianhhincom blackpercent leg_democrat, targets(2 2 1) g(wt) * Use pweight = wt to adjust estimates reg responded leg_black [pw = wt] . ",
    "url": "/Model_Estimation/Matching/entropy_balancing.html#stata",
    
    "relUrl": "/Model_Estimation/Matching/entropy_balancing.html#stata"
  },"370": {
    "doc": "Difference in Differences Event Study",
    "title": "Difference-in-Differences Event Study / Dynamic Difference-in-Differences",
    "content": "A Difference-in-Difference (DID) event study, or a Dynamic DID model, is a useful tool in evaluating treatment effects of the pre- and post- treatment periods in your respective study. However, since treatment can be staggered — where the treatment group are treated at different time periods — it might be challenging to create a clean event study. We will try to address these issues explicitly in the implementation examples that follow. Important: The dataset used for the implementation examples on this page derives from a staggered treatment (“rollout”) design. Various studies have shone a light on the potential biases that can result from applying a standard, twoway fixed-effect (TWFE) regression estimator on such a staggered setup, including Sun and Abraham (2020), Callaway and Sant’Anna (2020), and Goodman-Bacon (2021). Where possible, we will highlight these issues and provide code examples that addresses the potential baises. Note that in cases where the rollout is not staggered (i.e. there is only one treatment period), the same approaches can be applied without loss. The regression that DID event studies are based aroud is: . \\[Y_{gt} = \\alpha + \\Sigma_{k=T_0}^{-2}\\beta_k\\times treat_{gk}+\\Sigma_{k=0}^{T_1}\\beta_k\\times treat_{gk}+ X_{st}\\Gamma+\\phi_s+\\gamma_t+\\epsilon_{gt}\\] Where: . | \\(treat_{sk}\\) is a dummy variable, equaling 1 if the observation’s periods relative to the group \\(g\\)’s first treated period is the same value as k; 0 otherwise (and 0 for all never-treated groups). | \\(T_0\\) and \\(T_1\\) are the lowest and highest number of leads and lags to consider surrouning the treatment period, respectively. | \\(X`\\) are controls . | \\(\\phi\\) and \\(\\gamma\\) are state and time fixed effects . | Estimation is generally performed with standard errors clustered at the group level . | . Important notes on the regression: . | The point of the regression is to show for each period before and after treatment that the coefficients on the pre-treated periods are statistically insignificant . | Showing the control and treated groups are statistically the same before treatment (\\(\\beta_k=0\\) for \\(k &lt; 0\\)) supports, though does not prove, the parallel trends assumption in DID estimation. | One of the time periods must be dropped to avoid perfect multicollinearity (as in most fixed-effects setups). In most event studies, the -1 time lag is used as the dropped reference. | . ",
    "url": "/Model_Estimation/Research_Design/event_study.html#difference-in-differences-event-study--dynamic-difference-in-differences",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#difference-in-differences-event-study--dynamic-difference-in-differences"
  },"371": {
    "doc": "Difference in Differences Event Study",
    "title": "Keep in Mind",
    "content": "Mechanically, an event study is a graphical illustration of the point estimates and confidence intervals of the regression for each time period before and after the treatment period. It’s especially relevant in the DID environment as the point estimates are the average mean differences between the treated and control groups, which provides further evidence of the credibility in assuming parallel trends. ",
    "url": "/Model_Estimation/Research_Design/event_study.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#keep-in-mind"
  },"372": {
    "doc": "Difference in Differences Event Study",
    "title": "Also Consider",
    "content": ". | 2x2 Difference-in-Differences | A great resource for learning more about DID and event study theory is at Causal Inference: The Mixtape. | . ",
    "url": "/Model_Estimation/Research_Design/event_study.html#also-consider",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#also-consider"
  },"373": {
    "doc": "Difference in Differences Event Study",
    "title": "Implementations",
    "content": "All implementations use the same data, which comes from Stevenson and Wolfers (2006) by way of Clarke &amp; Schythe (2020), who use it as an example to demonstrate Goodman-Bacon effects. This data is a balanced panel from 1964 through 1996 of the United States no-fault divorce reforms and female suicide rates. You can directly download the data here. Column _nfd in the data specifies the year in which the law went into effect for the respective state. We use this column to identify the lead and lags with respect to year of treatment. pcinc, asmrh, and cases are controls. Again, it is important to emphasise that treatment (i.e. when the law went into effect for each state) is staggered over time. Note that there are some states in which _nfd is empty. These states never received treatment, and thus exist as a control. ",
    "url": "/Model_Estimation/Research_Design/event_study.html#implementations",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#implementations"
  },"374": {
    "doc": "Difference in Differences Event Study",
    "title": "Python",
    "content": "Python makes dealing with lots of interaction terms like we have here a little painful, but we can iterate to do a lot of the work for us. import pandas as pd import linearmodels as lm # Read in data df = pd.read_csv(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Event_Study_DiD/bacon_example.csv\") # create the lag/lead for treated states # fill in control obs with 0 # This allows for the interaction between `treat` and `time_to_treat` to occur for each state. # Otherwise, there may be some missingss and the estimations will be off. df['time_to_treat'] = ( df['year'].sub(df['_nfd']) # missing values for _nfd implies no treatment .fillna(0) # so we don't have decimals in our factor names .astype('int') ) # Create our interactions by hand, # skipping -1, the last one before treatment df = ( # returns dataframe with dummy columns in place of the columns # in the named argument, all other columns untouched pd.get_dummies(df, columns=['time_to_treat'], prefix='INX') # Be sure not to include the minuses in the name .rename(columns=lambda x: x.replace('-', 'm')) # get_dummies has a `drop_first` argument, but if we want to # refer to a specific level, we should return all levels and # drop out reference column manually .drop(columns='INX_m1') # Set our individual and time (index) for our data .set_index(['stfips', 'year']) ) # Estimate the regression scalars = ['pcinc', 'asmrh', 'cases'] factors = df.columns[df.columns.str.contains('INX')] exog = factors.union(scalars) endog = 'asmrs' # with the standard api: mod = lm.PanelOLS(df[endog], df[exog], entity_effects=True, time_effects=True) fit = mod.fit(cov_type='clustered', cluster_entity=True) fit.summary # with the formula api: # We can save ourselves some time by creating the regression formula automatically inxnames = df.columns[range(13,df.shape[1])] formula = '{} ~ {} + EntityEffects + TimeEffects'.format(endog, '+'.join(exog)) mod = lm.PanelOLS.from_formula(formula,df) # Specify clustering when we fit the model clfe = mod.fit(cov_type = 'clustered', cluster_entity = True) # Look at regression results clfe.summary . Now we can plot the results with matplotlib. Two common approaches are to include vertical-line confidence intervals with errorbar() or to include a confidence interval ribbon with fill_between(). I’ll show the errorbar() version. # Get coefficients and CIs res = pd.concat([clfe.params, clfe.std_errors], axis = 1) # Scale standard error to 95% CI res['ci'] = res['std_error']*1.96 # We only want time interactions res = res.filter(like='INX', axis=0) # Turn the coefficient names back to numbers res.index = ( res.index .str.replace('INX_', '') .str.replace('m', '-') .astype('int') .rename('time_to_treat') ) # And add our reference period back in, and sort automatically res.reindex(range(res.index.min(), res.index.max()+1)).fillna(0) # Plot the estimates as connected lines with error bars ax = res.plot( y='parameter', yerr='ci', xlabel='Time to Treatment', ylabel='Estimated Effect', legend=False ) # Add a horizontal line at 0 ax.axhline(0, linestyle='dashed') # And a vertical line at the treatment time # some versions of pandas have bug return x-axis object with data_interval # starting at 0. In that case change 0 to 21 ax.axvline(0, linestyle='dashed') . Which produces: . Of course, as earlier mentioned, this analysis is subject to the critique by Sun and Abraham (2020). You want to calculate effects separately by time-when-treated, and then aggregate to the time-to-treatment level properly, avoiding the way these estimates can “contaminate” each other in the regular model. You are on your own for this process in Python, though. Read the paper (and the back-end code from the R or Stata implementations listed below). ",
    "url": "/Model_Estimation/Research_Design/event_study.html#python",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#python"
  },"375": {
    "doc": "Difference in Differences Event Study",
    "title": "R",
    "content": "A variety of R packages can be used to conduct event-study DiD analysis. Here we will use fixest, which is both extremely fast and provides several convenience features (including ES graphics and implementation of the Sun-Abraham method). Note that we are using version 0.9.0 of fixest. library(data.table) ## For some minor data wrangling library(fixest) ## NB: Requires version &gt;=0.9.0 # Load and prepare data dat = fread(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Event_Study_DiD/bacon_example.csv\") # Let's create a more user-friendly indicator of which states received treatment dat[, treat := ifelse(is.na(`_nfd`), 0, 1)] # Create a \"time_to_treatment\" variable for each state, so that treatment is # relative for all treated units. For the never-treated (i.e. control) units, # we'll arbitrarily set the \"time_to_treatment\" value at 0. This value # doesn't really matter, since it will be canceled by the treat==0 interaction # anyway. But we do want to make sure they aren't NA, otherwise feols would drop # these never-treated observations at estimation time and our results will be # off. dat[, time_to_treat := ifelse(treat==1, year - `_nfd`, 0)] . Let’s run our event study model. Again, we’ll be using fixest::feols() to do so. Note that the key time × treatment interaction takes place via the special i() operator. In the below case, we are essentially implementing a standard two-way fixed-effects (TWFE) procedure. We’ll also specify state+year fixed effects and explicitly cluster the standard errors at the state level (although this would be done by default). mod_twfe = feols(asmrs ~ i(time_to_treat, treat, ref = -1) + ## Our key interaction: time × treatment status pcinc + asmrh + cases | ## Other controls stfips + year, ## FEs cluster = ~stfips, ## Clustered SEs data = dat) . While it’s relatively straightforward to convert this model object into a data frame and then plot manually using ggplot2, this is more work than we need to do here. Specifically, fixest provides a convenient iplot() function for plotting the interaction term coefficients from a model. The resulting plot can be heavily customized, but for event-study designs it generally does exactly what you’d want out of the box. iplot(mod_twfe, xlab = 'Time to treatment', main = 'Event study: Staggered treatment (TWFE)') . This results in: . As earlier mentioned, the standard TWFE approach to event studies is subject to various problems in the presence of staggered treatment rollout. Fortunately, fixest provides the sunab() convenience function for estimating the aggregated “cohort” method proposed by Sun and Abraham (2020). In the below example, the only material change is to swap out the i() interaction with the sunab() equivalent. # Following Sun and Abraham, we give our never-treated units a fake \"treatment\" # date far outside the relevant study period. dat[, year_treated := ifelse(treat==0, 10000, `_nfd`)] # Now we re-run our model from earlier, but this time swapping out # `i(time_to_treat, treat, ref = -1)` for `sunab(year_treated, year)`. # See `?sunab`. mod_sa = feols(asmrs ~ sunab(year_treated, year) + ## The only thing that's changed pcinc + asmrh + cases | stfips + year, cluster = ~stfips, data = dat) . Again, we can use iplot() to depict the resulting event study in graphical form. In this case, we’ll plot both the new Sun-Abraham mode results and the previous (naive) TWFE model results . iplot(list(mod_twfe, mod_sa), sep = 0.5, ref.line = -1, xlab = 'Time to treatment', main = 'Event study: Staggered treatment') legend(\"bottomleft\", col = c(1, 2), pch = c(20, 17), legend = c(\"TWFE\", \"Sun &amp; Abraham (2020)\")) . ",
    "url": "/Model_Estimation/Research_Design/event_study.html#r",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#r"
  },"376": {
    "doc": "Difference in Differences Event Study",
    "title": "Stata",
    "content": "We can use the reghdfe package to help with our two-way fixed effects and high-dimensional data. Install with ssc install reghdfe first if you don’t have it. use \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Event_Study_DiD/bacon_example.dta\", clear * create the lag/lead for treated states * fill in control obs with 0 * This allows for the interaction between `treat` and `time_to_treat` to occur for each state. * Otherwise, there may be some NAs and the estimations will be off. g time_to_treat = year - _nfd replace time_to_treat = 0 if missing(_nfd) * this will determine the difference * btw controls and treated states g treat = !missing(_nfd) * Stata won't allow factors with negative values, so let's shift * time-to-treat to start at 0, keeping track of where the true -1 is summ time_to_treat g shifted_ttt = time_to_treat - r(min) summ shifted_ttt if time_to_treat == -1 local true_neg1 = r(mean) * Regress on our interaction terms with FEs for group and year, * clustering at the group (state) level * use ib# to specify our reference group reghdfe asmrs ib`true_neg1'.shifted_ttt pcinc asmrh cases, a(stfips year) vce(cluster stfips) . Now we can plot. * Pull out the coefficients and SEs g coef = . g se = . levelsof shifted_ttt, l(times) foreach t in `times' { replace coef = _b[`t'.shifted_ttt] if shifted_ttt == `t' replace se = _se[`t'.shifted_ttt] if shifted_ttt == `t' } * Make confidence intervals g ci_top = coef+1.96*se g ci_bottom = coef - 1.96*se * Limit ourselves to one observation per quarter * now switch back to time_to_treat to get original timing keep time_to_treat coef se ci_* duplicates drop sort time_to_treat * Create connected scatterplot of coefficients * with CIs included with rcap * and a line at 0 both horizontally and vertically summ ci_top local top_range = r(max) summ ci_bottom local bottom_range = r(min) twoway (sc coef time_to_treat, connect(line)) /// (rcap ci_top ci_bottom time_to_treat) /// (function y = 0, range(time_to_treat)) /// (function y = 0, range(`bottom_range' `top_range') horiz), /// xtitle(\"Time to Treatment\") caption(\"95% Confidence Intervals Shown\") . Which produces: . Any further decoration or theming at that point is up to you. Of course, as earlier mentioned, this analysis is subject to the critique by Sun and Abraham (2020). We can estimate the Sun and Abraham method using the eventstudyinteract command in Stata. Install by installing the github package with net install github, from(\"https://haghish.github.io/github/\") and then installing eventstudyinteract with github install lsun20/eventstudyinteract. See help eventstudyinteract for more information. * Reload our data since we squashed it to graph use \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Event_Study_DiD/bacon_example.dta\", clear * create the lag/lead for treated states * fill in control obs with 0 * This allows for the interaction between `treat` and `time_to_treat` to occur for each state. * Otherwise, there may be some NAs and the estimations will be off. g time_to_treat = year - _nfd replace time_to_treat = 0 if missing(_nfd) * this will determine the difference * btw controls and treated states g treat = !missing(_nfd) g never_treat = missing(_nfd) * Create relative-time indicators for treated groups by hand * ignore distant leads and lags due to lack of observations * (note this assumes any effects outside these leads/lags is 0) tab time_to_treat forvalues t = -9(1)16 { if `t' &lt; -1 { local tname = abs(`t') g g_m`tname' = time_to_treat == `t' } else if `t' &gt;= 0 { g g_`t' = time_to_treat == `t' } } eventstudyinteract asmrs g_*, cohort(_nfd) control_cohort(never_treat) covariates(pcinc asmrh cases) absorb(i.stfips i.year) vce(cluster stfips) * Get effects and plot * as of this writing, the coefficient matrix is unlabeled and so we can't do _b[] and _se[] * instead we'll work with the results table matrix T = r(table) g coef = 0 if time_to_treat == -1 g se = 0 if time_to_treat == -1 forvalues t = -9(1)16 { if `t' &lt; -1 { local tname = abs(`t') replace coef = T[1,colnumb(T,\"g_m`tname'\")] if time_to_treat == `t' replace se = T[2,colnumb(T,\"g_m`tname'\")] if time_to_treat == `t' } else if `t' &gt;= 0 { replace coef = T[1,colnumb(T,\"g_`t'\")] if time_to_treat == `t' replace se = T[2,colnumb(T,\"g_`t'\")] if time_to_treat == `t' } } * Make confidence intervals g ci_top = coef+1.96*se g ci_bottom = coef - 1.96*se keep time_to_treat coef se ci_* duplicates drop sort time_to_treat keep if inrange(time_to_treat, -9, 16) * Create connected scatterplot of coefficients * with CIs included with rcap * and a line at 0 both horizontally and vertically summ ci_top local top_range = r(max) summ ci_bottom local bottom_range = r(min) twoway (sc coef time_to_treat, connect(line)) /// (rcap ci_top ci_bottom time_to_treat) /// (function y = 0, range(time_to_treat)) /// (function y = 0, range(`bottom_range' `top_range') horiz), /// xtitle(\"Time to Treatment with Sun and Abraham (2020) Estimation\") caption(\"95% Confidence Intervals Shown\") . ",
    "url": "/Model_Estimation/Research_Design/event_study.html#stata",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html#stata"
  },"377": {
    "doc": "Difference in Differences Event Study",
    "title": "Difference in Differences Event Study",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/event_study.html",
    
    "relUrl": "/Model_Estimation/Research_Design/event_study.html"
  },"378": {
    "doc": "Faceted Graphs",
    "title": "Faceted Graphs",
    "content": "When plotting relationship among variables of interest, one of the useful ways to create visual impact is by way of using facet, which subsets data with faceted variable and creates plots for each of the subset seperately. The result is a panel of subplots, with each subplot depicting the plot for same set of variables. This approach can be especially useful for panel datasets, with the panel variable acting as facet variable and each subplot depicting time series trend of variable of interest. ",
    "url": "/Presentation/Figures/faceted_graphs.html",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html"
  },"379": {
    "doc": "Faceted Graphs",
    "title": "Keep in Mind",
    "content": ". | It is important to use a categorical (discrete) variable as a facet variable for creating faceted graphs. | Plotting libraries generally fall into two broad camps: imperative (specify all of the steps to get the desired outcome) or declarative (specify the desired outcome without the steps). Imperative plotting gives more control and some people may find each step clearer to read, but it can also be fiddly and cumbersome, especially with simple plots. Declarative plotting trades away control in favour of tried and tested processes that can quickly produce standardised charts, but the specialised syntax can be a barrier for newcomers. Facets are available in both types, but the code to produce them will look quite different. | . ",
    "url": "/Presentation/Figures/faceted_graphs.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html#keep-in-mind"
  },"380": {
    "doc": "Faceted Graphs",
    "title": "Also Consider",
    "content": ". | It is important to know the basic plotting techniques such as Bar Graphs, Line Graphs and Scatterplot before learning about faceted graphs as the facets are an addition to the underlying plot such as bar graph, line graph, scatterplot etc. | . ",
    "url": "/Presentation/Figures/faceted_graphs.html#also-consider",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html#also-consider"
  },"381": {
    "doc": "Faceted Graphs",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/faceted_graphs.html#implementations",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html#implementations"
  },"382": {
    "doc": "Faceted Graphs",
    "title": "Python",
    "content": "Python has both imperative and declarative plotting libraries, and many of them have support for faceted graphs. In the example below, we’ll look at three libraries: two declarative, seaborn and plotnine, and one imperative, matplotlib. The imperative library matplotlib is by far the most popular plotting tool in Python, having been used as part of efforts to detect gravitational waves and produce the first image of a black hole. Often, other libraries build upon it as a foundation. For plots that will be shown on the web, the declarative library altair has very good facet support. Because matplotlib is imperative, it takes more effort by the user to produce a simple facet chart. So the first example below will use seaborn, a declarative library that builds on matplotlib. We’ll use it to produce a plot from the Penguins dataset, with a facet for each island. As ever, you may need to conda or pip install the libraries used in the examples. import seaborn as sns # Load the example Penguins dataset df = sns.load_dataset(\"penguins\") # Plot a scatter of bill properties with # columns (facets) given by island and colour # given by the species of Penguin sns.relplot(x=\"bill_depth_mm\", y=\"bill_length_mm\", hue=\"species\", col=\"island\", alpha=.5, palette=\"muted\", data=df) . Results in: . If you have used R for plotting, you might be familiar with the ggplot package. plotnine is another declarative plotting library in Python that is inspired by the API for ggplot in R. from plotnine import * from plotnine.data import mtcars (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')) + geom_point() + stat_smooth(method='lm') + facet_wrap('~gear')) . Results in: . For more complex charts, where you want full control over facet placement, the imperative library matplotlib has a wealth of options. The code for a simple facet plot using synthetic data is: . import matplotlib.pyplot as plt import numpy as np # Some example data to display x = np.linspace(0, 2 * np.pi, 400) y = np.sin(x ** 2) fig, (ax1, ax2) = plt.subplots(2, sharex=True) fig.suptitle('Two sine waves') ax1.plot(x, y) ax2.scatter(x + 1, -y, color='red') . (NB: no figure shown in this case.) Note how everything is specified. While plt.subplots(nrows, ncols, ...) allows for a rectangular facet grid, even more complex facets can be constructed using the mosaic option in matplotlib version 3.3.0+. The arrangment of facets can be specified either through text, as in the example below, or with lists of lists: . import matplotlib.pyplot as plt axd = plt.figure(constrained_layout=True).subplot_mosaic( \"\"\" TTE L.E \"\"\") for k, ax in axd.items(): ax.text(0.5, 0.5, k, ha='center', va='center', fontsize=36, color='darkgrey') . Results in: . ",
    "url": "/Presentation/Figures/faceted_graphs.html#python",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html#python"
  },"383": {
    "doc": "Faceted Graphs",
    "title": "R",
    "content": "Implementation of faceted graph in R explained below is taken from online book R for Data Science by Hadley Wickham and Garett Grolemund. The book is also an excellent source for various data visualization techniques in R and learning R in general. We will use tidyverse package available in R for faceted graphs. Tidyverse is actually a meta-package which has various packages, and we will use ggplot2 package for our purpose. This package has a data frame (it is like a table in R), called ‘mpg’ which contains observations collected by the US Environmental Protection Agency on 38 models of car. To create faceted graph, use facet_wrap() option in ggplot. The argument inside the bracket is ~ sign follwed by the categorical variable to be used to create subsets of data. Its use is illustrated in the code given below. library(tidyverse) # Now, we will create faceted graph, with variable 'displ' (a car's engine size) on # x-axis and variable 'hwy (car's fuel efficiency on highway) on y-axis. We will use # `facet_wrap(~class)` option to created faceted graph. The variable 'class' denotes # type of car. We use 'geom_point()` to create a scatterplot. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~class) . The above set of code results in the following panel of subplots: . Additionally, one can create faceted graph using two variables with facet_grid(). Inside the bracket, use two variables seperated by ~. The example of the same using ‘mpg’ dataframe and two variables ‘drv’ (whether it’s front wheel, rear wheel or 4wd) and ‘cyl’ (number of cylinders) is given below. library(tidyverse) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy))+ facet_grid(drv ~ cyl) . The code reults in the follwing panel of subplots: . ",
    "url": "/Presentation/Figures/faceted_graphs.html#r",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html#r"
  },"384": {
    "doc": "Faceted Graphs",
    "title": "Stata",
    "content": "In stata, faceted graph can be created by using option by() and mentioning the faceted variable in the bracket. Let’s see an example of the same . Let’s access pre-installed dataset in Stata, called ‘auto.dta’ which has 1978 automobile data. The following code generates scatterplot with ‘length of car’ on x-axis, ‘mileage of car’ on y-axis and variable ‘foreign’ (whether the car is manufactured domestically or imported) used to create subsets of data. sysuse auto is used to load the table ‘auto.dta’. sysuse auto twoway (scatter mpg length), by(foreign) . The code generates the following graph: . ",
    "url": "/Presentation/Figures/faceted_graphs.html#stata",
    
    "relUrl": "/Presentation/Figures/faceted_graphs.html#stata"
  },"385": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Fixed Effects in Linear Regression",
    "content": "Fixed effects is a statistical regression model in which the intercept of the regression model is allowed to vary freely across individuals or groups. It is often applied to panel data in order to control for any individual-specific attributes that do not vary across time. For more information, see Wikipedia: Fixed Effects Model. ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html"
  },"386": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Keep in Mind",
    "content": ". | To use individual-level fixed effects, you must observe the same person multiple times (panel data). | In a linear regression context, fixed effects regression is relatively straightforward, and can be thought of as effectively adding a binary control variable for each individual, or subtracting the within-individual mean of each variable (the “within” estimator). However, you may want to apply fixed effects to other models like logit or probit. This is usually possible (depending on the model), but if you just add a set of binary controls or subtract within-individual means, it won’t work very well. Instead, look for a command specifically designed to implement fixed effects for that model. | If you are using fixed effects to estimate the causal effect of a variable \\(X\\), individuals with more variance in \\(X\\) will be weighted more heavily (Gibbons, Serrano, &amp; Urbancic 2019, ungated copy here). You may want to consider weighting your regression by the inverse within-individual variance of \\(X\\). | . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#keep-in-mind"
  },"387": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Also Consider",
    "content": ". | Instead of fixed effects you may want to use random effects, which requires additional assumptions but is statistically more efficient and also allows the individual effect to be modeled using covariates. See Linear Mixed-Effects Regression | You may want to consider clustering your standard errors at the same level as (some or more of) your fixed effects. | . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#also-consider",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#also-consider"
  },"388": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#implementations",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#implementations"
  },"389": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Julia",
    "content": "Julia provides support for estimating high-dimensional fixed effect models through the FixedEffectModels.jl package (link). Similarly to felm (R) and reghdfe (Stata), the package uses the method of alternating projections to sweep out fixed effects. However, the Julia implementation is typically quite a bit faster than these other two methods. It also offers further performance gains via GPU computation for users with a working CUDA installation (up to an order of magnitude faster for complicated problems). # If necessary, install JuliaFixedEffects.jl and some ancilliary packages for reading in the data # ] add JuliaFixedEffects, CSVFiles, DataFrames # Read in the example CSV and convert to a data frame using CSVFiles, DataFrames df = DataFrame(load(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\")) # Calculate proportion of graduates working df[!, :prop_working] = df[!, :count_working] ./ (df[!, :count_working ] .+ df[!, :count_not_working]) using JuliaFixedEffects # Regress median earnings on the proportion of working graduates. # We'll control for institution name and year as our fixed effects. # We'll also cluster our standard errors by institution name. reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name)) # Multithread example Threads.nthreads() ## See: https://docs.julialang.org/en/v1.2/manual/parallel-computing/#man-multithreading-1 reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_threads) # GPU example (requires working CUDA installation) reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_gpu) . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#julia",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#julia"
  },"390": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Python",
    "content": "There are a few packages for doing the same task in Python, however, there is a well-known issue with these packages.That is, the calculation of standard deviation might be a little different. We are going to use linearmodels in python. Installation can be done through pip install linearmodels and the documentation is here . # Import the packages import pandas as pd from linearmodels import PanelOLS import numpy as np # Load the data data = pd.read_csv(r\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\") # Set the index for fixed effects data = data.set_index(['inst_name', 'year']) # Calculate and drop the NA Values data['prop_working'] = data['count_working']/(data['count_working'] + data['count_not_working']) #data = data.dropna(subset=['earnings_med', 'prop_working']) # Regression FE = PanelOLS(data.earnings_med, data['prop_working'], entity_effects = True, time_effects=True ) # Result result = FE.fit(cov_type = 'clustered', cluster_entity=True, # cluster_time=True ) . There are also other packages for fixed effect models, such as econtools (link), FixedEffectModelPyHDFE (link), regpyhdfe(link) and econtools (link). ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#python",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#python"
  },"391": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "R",
    "content": "There are numerous packages for estimating fixed effect models in R. We will limit our examples here to the two fastest implementations — lfe::felm and fixest::feols — both of which support high-dimensional fixed effects and standard error correction (multiway clustering, etc.). We first demonstrate fixed effects in R using felm from the lfe package (link). lfe::felm uses the Method of Alternating Projections to “sweep out” the fixed effects and avoid estimating them directly. By default, this is automatically done in parallel, using all available cores on a user’s machine to maximize performance. (It is also possible to change this behaviour.) . library(lfe) # Read in data from the College Scorecard df &lt;- read.csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # A felm formula is constructed as: # outcome ~ # covariates | # fixed effects | # instrumental variables specification | # cluster variables for standard errors # Here let's regress earnings_med on prop_working # with institution name and year as our fixed effects # And clusters for institution name felm_model &lt;- felm(earnings_med ~ prop_working | inst_name + year | 0 | inst_name, data = df) # Look at our results summary(felm_model) . Next, we consider feols from the fixest package (link). The syntax is very similar to lfe::felm and again the estimation will be done in parallel by default. However, rather than the method of alternating projections, fixest::feols uses a concentrated maximum likelihood method to efficiently estimate models with an arbitrary number of fixed effects. Current benchmarks suggest that this can yield significant speed gains, especially for large problems. For the below example, we’ll continue with the same College Scorecard dataset already loaded into memory. # If necessary, install fixest # install.packages('fixest') library(fixest) # Run the same regression as before feols_model &lt;- feols(earnings_med ~ prop_working | inst_name + year, data = df) # Look at our results # Standard errors are automatically clustered at the inst_name level summary(feols_model) # It is also possible to specify additional or different clustering of errors summary(feols_model, se = \"twoway\") summary(feols_model, cluster = c(\"inst_name\", \"year\")) ## same as the above . As noted above, there are numerous other ways to implement fixed effect models in R. Users may also wish to look at the plm, lme4, and estimatr packages among others. For example, the latter’s estimatr::lm_robust function provides syntax that may be more familar syntax to new R users who are coming over from Stata. Note, however, that it will be less efficient for complicated models. ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#r",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#r"
  },"392": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Stata",
    "content": "We will estimate fixed effects using Stata in two ways. First, using the built in xtreg command. Second, using the reghdfe package (link), which is more efficient and better handles multiple levels of fixed effects (as well as multiway clustering), but must be downloaded from SSC first. * Load in College Scorecard data import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\", clear * The missings are written as \"NA\", let's turn this numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the \"fe\" option to run fixed effects * Regressing earnings_med on prop_working * with fixed effects for name_number (implied by fe) * and also year (which we'll add manually with i.year) * and standard errors clustered by name_number xtreg earnings_med prop_working i.year, fe vce(cluster name_number) * Now, let's demonstrate the same regression with reghdfe. * Install the package first if necessary. * ssc install reghdfe * For reghdfe we don't need to xtset the data. Let's undo that xtset, clear * We specify both sets of fixed effects in absorb() reghdfe earnings_med prop_working, absorb(name_number year) vce(cluster inst_name) . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#stata",
    
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#stata"
  },"393": {
    "doc": "Formatting Graph Axes",
    "title": "Formatting Graph Axes",
    "content": "Graph axes have the power to manipulate the story of the data, and that is why formatting graph axis is an important skill in big data. When plotting relationship among variables of interest, one of the useful ways to create visual impact and clarity is by way of formatting the x and y axis. Different axis features that can be manipulated include: . | Axis titles (always set them to be readable! Rarely do you want whatever the variable name is in your statistics package) | Axis scale (linear scale, logarithmic scale, reversed scale, etc.) | Axis marks and labels (do you get a tick at 1, 2, 3, 4, and 5, or just 1, 3, and 5? Are the numeric values percentages? Numbers? In thousands or millions?) | Axis limits (the range of x and y to show) | . ",
    "url": "/Presentation/Figures/formatting_graph_axes.html",
    
    "relUrl": "/Presentation/Figures/formatting_graph_axes.html"
  },"394": {
    "doc": "Formatting Graph Axes",
    "title": "Keep in Mind",
    "content": ". | Visual appeal of your newly formatted graph. Who is the audience? What do you want them to notice? For example, for most common audiences you would want to avoid labeling your axes with scientific notation, as most people don’t understand it. | Caution: Use an appropriate scale when formatting your x and $y$ axis: You can manipulate data to say what you want it to, but using a more transparent approach is better practice. | Changing the limits of the graph axes might make changes seem more or less dramatic. This comes up often in the debate of whether the y-axis on a line graph should always start from 0. If, for example, test scores dropped from 100 to 95, that change looks enormous if the y-axis goes from 94 to 100, with a change the size of the whole graph but looks pretty modest if the y-axis goes from 0 to 100 with a change the size of 5% of the graph. | . ",
    "url": "/Presentation/Figures/formatting_graph_axes.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/formatting_graph_axes.html#keep-in-mind"
  },"395": {
    "doc": "Formatting Graph Axes",
    "title": "Also Consider",
    "content": ". | It is important to know the basic plotting techniques such as Line Graphs and Scatterplots | . ",
    "url": "/Presentation/Figures/formatting_graph_axes.html#also-consider",
    
    "relUrl": "/Presentation/Figures/formatting_graph_axes.html#also-consider"
  },"396": {
    "doc": "Formatting Graph Axes",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/formatting_graph_axes.html#implementations",
    
    "relUrl": "/Presentation/Figures/formatting_graph_axes.html#implementations"
  },"397": {
    "doc": "Formatting Graph Axes",
    "title": "R",
    "content": "Let’s walk through an example in R to learn how to manipulate intervals on graph axes. The dataset used in this article example will be the diamonds dataset since it comes with the package ggplot2: you can follow along with this brief tutorial at home. This data reports prices of over 50,000 round cut diamonds. library(ggplot2) . Let’s begin with a graph plotting price and clarity of a diamond, and seeing what it gives us as a default axis. We plot histogram in ggplot2 by using the ggplot() function with the diamonds dataset, and adding geom_density() to plot a histogram. Aesthetic mappings describe how variables in the data are mapped to visual properties(aesthetics) of geoms. diamonds %&gt;% ggplot() + geom_histogram(binwidth=500, aes(x=price)) . Now let’s manipulate the four main elements of the axis: titles, scale, marks and ticks, value labels, and limits. Axis titles can be changed using the labs() command. This works not only for the x and y axes, but also any other aesthetic that has a legend. (You can also set the title, subtitle, and caption in labs() - those aren’t axes but a lot of people don’t know you can use labs() for this! Who needs ggtitle?). We’ll switch to geom_density() here so we can add, and then label, a color aesthetic. diamonds %&gt;% ggplot() + geom_density(aes(x=price, color = cut)) + labs(x = 'Price', y = 'Density', color = 'Diamond Cut') . The scale and limits of the axes can be set using one of the scale_ functions. scale_x_log10(), for example, will use a logarithmic scale for the x-axis. There are a lot of different scale_ functions for different combinations of axis (including axes like color and shape, not just x and y) and scale type. You can use the limits argument inside of a scale_ function to determine the limits of the axis. The limits can be outside the range of the data, “zooming out” on the graph, or inside the range of the data, cutting some of the data out of the graph. Note that using limits inside scale_ will remove all data outside the limits. If you want to retain that data but just zoom the window in, instead look at the xlim and ylim arguments of coord_cartesian(). Note you can also use the position argument of scale_x_ and scale_y_ functions to move the axes themselves, perhaps putting the x-axis line at the \"top\" for example. diamonds %&gt;% ggplot() + geom_density(aes(x=price, color = cut)) + labs(x = 'Price (Log Scale)', y = 'Density', color = 'Diamond Cut') + scale_x_log10(limits = c(1, 5000)) # I use 1 instead of 0 here because log(0) is undefined . Note how (1) the x-axis is now on a logarithmic scale, (2) the density numbers are different - density has been recalculated after dropping data outside the range, we haven’t just “zoomed in”, and (3) the graph begins and ends at our specified limits. The scale_ functions can also be used to label the values on the axes and specify how many tick marks there are. The breaks option lets you specify how many (or which) values get markers on the axis. The labels option lets you give it a function (or vector of values) that it uses to turn value labels in the data to printed labels on the graph. The scales package has a lot of good functions for use here in formatting numbers for print, like number(), dollar(), and percent(). These arguments canbe very useful in avoiding scientific notation, which ggplot2 tends to default to in a lot of cases. labels is also handy for labeling discrete values. In this example scale_color_discrete is being used to relabel the categories in the legend, but this would also work if the cut variable were on the x-axis with scale_x_discrete. diamonds %&gt;% ggplot() + geom_density(aes(x=price, color = cut)) + labs(x = 'Price (Log Scale)', y = 'Density', color = 'Diamond Cut') + scale_x_log10(limits = c(1, 5000), breaks = c(100, 1000, 3000), labels = scales::dollar) + scale_color_discrete(labels = c('Fair' = 'Worst', 'Good' = 'Okay', 'Very Good' = 'Nice', 'Premium' = 'Better', 'Ideal' = 'Oh Dang')) . Note that if you don’t like the scales function defaults, you can use an unnamed function. For example, labels = function(x) scales::dollar(x, accuracy = 1) (or labels = \\(x) scales::dollar(x, accuracy = 1) in R 4.1) would get rid of those cents after the decimal place. Finally, we can manipulate the look of the axes and ticks using the theme() function. The axis.ticks (or axis.ticks.x and axis.ticks.y to just change one of them) option takes an element_line() argument, in which you can set options like size, color, and so on. You can also change the axis.line (or axis.line.x and axis.line.y) options to change the look of the actual axis lines. There are also axis.text and axis.title options for changing how the text looks with element_text(). Now, let’s manipulate the size of the ticks on the x axis. Let’s make them really wide for effect, and also really long. Let’s choose “12 point unit” long length ticks for dramatic effect. diamonds %&gt;% ggplot()+ geom_histogram(binwidth=500, aes(x=price)) + theme(axis.ticks.x = element_line(size = 5), axis.ticks.length = unit(12, \"pt\")) . ",
    "url": "/Presentation/Figures/formatting_graph_axes.html#r",
    
    "relUrl": "/Presentation/Figures/formatting_graph_axes.html#r"
  },"398": {
    "doc": "Formatting Graph Legends",
    "title": "Formatting Graph Legends",
    "content": "Graph legends are an important part of data representation, as they enable the reader to better under stand the figure infront of them while not cluttering the figure itself. Legends can often be seen next to graphs describing some of aspect of a graph. For more information on graphs see other articles such as: bar graphs, scatter plots and histograms. ",
    "url": "/Presentation/Figures/formatting_graph_legends.html",
    
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html"
  },"399": {
    "doc": "Formatting Graph Legends",
    "title": "Keep in Mind",
    "content": ". | Title: A legend should be well-titled so you can tell what the different component elements, like any key symbols, colors, scale changes, lines or other compeents mean. | Some more extended legends cal also include very brief information about methods or results, as simplified as possible. | Avoid clutter: It is important to keep these legends simple, an effective legend is there to help the graph stand alone. | Ask if a legend is necessary. You may be able to move labels for things like colors or shapes onto the graph itself where it is more closely tied to the data. Or you may be able to make things self-explanatory with a good graph title. | . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#keep-in-mind"
  },"400": {
    "doc": "Formatting Graph Legends",
    "title": "Also Consider",
    "content": ". | Before finalizing your legend the graph itself should be completed. Deciding which type of graph is best suited for your data is a whole topic in itself. | Some of the graphical implementations this page contains include are; Bar Graphs, Histograms, and Scatterplot by Group on Shared Axes. | Check out this article for a brief description of the relationships shown for different graph types . | . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#also-consider",
    
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#also-consider"
  },"401": {
    "doc": "Formatting Graph Legends",
    "title": "Graphs legends in R",
    "content": "Here are some of the packages that will be used in this section: . library(ggplot2) . The dataset used in this article will be the mtcars dataset as it comes with base R. The functions here are ggplot and geom_point from the ggplot2 package. geom_point creates a scatterplot while in ggplot we will assign the axis and the legend. More specifically, in aes() we color the point depending on the number of cylinders. fig1 &lt;- ggplot(mtcars, aes(wt, mpg, colour = factor(cyl))) + geom_point() fig1 . Now we can add another layor by giving each type of transition a different shape through shape in aes(). Additionally, you can augment the labels for both colour and shape with labs(). fig2 &lt;- ggplot(mtcars, aes(wt, mpg, colour = factor(cyl), shape = factor(am))) + geom_point() fig2 + labs(colour = \"Number of Cylinders\", shape = \"Transmission Type\") . To change the legend position use the theme() modifier in ggplot. From there you can choose top, right, bottom, left, or none (removes the legend). To put the legends inside the plot create column vector of size 2 (the first value refers to the x coordinate. while the second refers to the y) where both elements are between 0 and 1. To ensure that the whole legends is within the graph use the legend.justification to set the corner where you want the legend. fig2 + theme( legend.justification = c(1, 1), legend.position = c(.95, .95) ) . You can also use this function with pre-set locations like \"bottom\", \"left\" \"right\" and \"top\" where ggplot centers the legend on the side you choose. fig2 + theme( legend.position = \"bottom\" ) . There are other cool things you can do to the legend to better customize the visual experience by adding more to the theme modifier such as: . | Changing the margin size with legend.box.margin | The color of the box around the legend with legend.box.background | Changing the font size and color with legend.text | Changing the boxes of the legend key with legend.key | . fig3 &lt;- fig2 + theme( legend.box.background = element_rect(color=\"red\", size=2), legend.box.margin = margin(116, 6, 6, 6), legend.key = element_rect(fill = \"white\", colour = \"black\"), legend.text = element_text(size = 8, colour = \"red\") ) fig3 . Sometimes you may want to remove a legend for the sake of graph readability or to reduce clutter. You can remove a legend by changing its position to “none” within the theme modifier. fig4 &lt;- fig2 + theme(legend.position = \"none\") fig4 . You can alternately remove legends (or components of legends) with guides . # Here we've removed the color legend, but the shape legend is still there. fig5 &lt;- fig2 + guides(color = FALSE) fig5 # This removes both fig6 &lt;- fig2 + guides(color = FALSE, shape = FALSE) fig6 . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#graphs-legends-in-r",
    
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#graphs-legends-in-r"
  },"402": {
    "doc": "Formatting Graph Legends",
    "title": "Legend Formatting in Stata",
    "content": "In Stata the legend is automatically added when you create a graph with multiple lines. However there may be instances where you would still prefer to have a legend on a single line graph or possibly no legend on your graph in general. For those times just add legend(on) or legend(off). * Let's use the US Life Expectancy data that comes with Stata sysuse uslifeexp.dta, clear line le year, legend(on) . Labelling legends is also straight forward, within the legend function add label(n \"line title\") to augment the titles, where ‘n’ is the nth group. line le_m le_f year, legend(label(1 \"Males\") label(2 \"Females\")) . Legend position can be changed with the position(), col(), and ring() modifiers in the legend() function. Here position refers to where the legend is placed with respect to the center of the graph using clock directions (i.e. 6 is 6 o’clock or below and 3 is 3 o’clock or right). col() is the number of columns the legend will take up, usually you just want to set the to be 1 to prevent the graph and legend fighting over space. ring() refers to the distance away from the center of the graph. line le_m le_f year, legend(pos(3) col(1) lab(1 \"Males\") lab(2 \"Females\") stack) . Then you can add some color to the legend with region() and a title with subtitle(): . line le_m le_f year, legend(pos(5) ring(0) col(1) lab(1 \"Males\") lab(2 \"Females\") region(fcolor(gs15))) legend(subtitle(\"Legend\")) . In the case where you are dealing with a two-way or a combination of graphs, augmenting the legend is exactly like above only now you must specify the line which you want to augment. Take this example from the stata manual on two-way graphs: . line le_wm year, yaxis(1 2) xaxis(1 2) /// || line le_bm year /// || line diff year /// || lfit diff year /// ||, /// ylabel(0(5)20, axis(2) gmin angle(horizontal)) /// ylabel(0 20(10)80, gmax angle(horizontal)) /// ytitle(\"\", axis(2)) /// xlabel(1918, axis(2)) xtitle(\"\", axis(2)) /// ylabel(, axis(2) grid) /// ytitle(\"Life expectancy at birth (years)\") /// title(\"White and black life expectancy\") /// subtitle(\"USA, 1900-1999\") /// note(\"Source: National Vital Statistics, Vol 50, No. 6\" /// \"(1918 dip caused by 1918 Influenza Pandemic)\") /// legend(label(1 \"White males\") label(2 \"Black males\")) /// legend(col(1) pos(3)) . Notice within the label() command nested within legend() you must specify which part of the graph you’re labeling first (the number denotes its order i.e. the first graph is 1) then the new label with a space between both items. Here they change the legend label for the first 2 lines. In regards to legend positioning, the same rules discussed above apply. Sources . Stata’s manual on two-way graphs: https://www.stata.com/manuals13/g-2graphtwowayline.pdf Stata’s manual on legends: https://www.stata.com/manuals13/g-3legend_options.pdf . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#legend-formatting-in-stata",
    
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#legend-formatting-in-stata"
  },"403": {
    "doc": "Geocoding",
    "title": "Geocoding",
    "content": "Geocoding is taking an address (e.g. 1600 Pennsylvania Ave NW, Washington DC 20500) or a name of a place (e.g. The White House) and turning it into a geographic position on the earth’s surface. Commonly, the cooridinate system is longitude and latitude but there are other potential coordinate systems that can be used. There are many different types of locations one can geocode including: . | Cities | Landmarks | Geographic Locations * Mountains * Rivers | Addresses * Street Intersections * House Numbers with street names * Postal Codes | . There are multiple ways to geocode. For instance, you could find the corrdinates of the Empire State building by flying to New York, riding an elevator to the top of the building, and then using your GPS to get the latitiude and longitude of where you were standing. A much more efficient way of geocoding is through interpolation. Interpolation uses other known geocoded locations to estimate the coordinates of the data that you wish to geocode. A computer uses an algorithm and the closest known geocodes to conduct this interpolation. However, the farther the “closest” known geocodes are to the data you are trying to geocode the less accurate the geocoding process is. smartystreets, a geocode platform, has a good explanation of this. Additionally, Reverse Geocoding takes a latitude-longitude pair (or other global coordinates) and converts it into an address or a place. Depending on the data that is available reverse geocoding can be very useful. Similar to regular geocoding, reverse geocoding uses other known reverse geocoded locations to estimate the address of the inputted coordinates. ",
    "url": "/Geo-Spatial/geocoding.html",
    
    "relUrl": "/Geo-Spatial/geocoding.html"
  },"404": {
    "doc": "Geocoding",
    "title": "The Geocoding Process",
    "content": "Whenever you geocode data there is a 3 step process that is undergone: . | Step 1: Input Data Descriptive or textual data is inputted to yield a desired corresponding spatial data | Step 2: Classification of Input Data Input data is sorted into two groups relative input data and absolute data | Step 3A: Relative Input Data Relative input data is the non-preferred type of data (most geocoding reject relative input data). Relative data are textual descriptions of locations that cannot be converted into precise spatial data on their own. Instead they are dependent on a other reference locations. For example, “across the street from the White House” has to use “the White House” as a reference point and then deduce what “across the street” means. | Step 3B: Absolute Data This is the sweet sweet data that geocoding platforms love. A spatial coordinate (lon, lat) can be defined for this data independently of other reference points. Examples: USPS ZIP codes; complete and partial postal addresses; PO boxes; cities; counties; intersections; and named places | . ",
    "url": "/Geo-Spatial/geocoding.html#the-geocoding-process",
    
    "relUrl": "/Geo-Spatial/geocoding.html#the-geocoding-process"
  },"405": {
    "doc": "Geocoding",
    "title": "Why is geocoding helpful?",
    "content": "Odds are if you are on this page then you already have a reason to use geocoding, but here is a brief motivation for how geocoding can help with a project. Geocoding is helpful when you want to do spatial work. For example, maybe you have data on voter addresses and want to visualize party allegiance. Perhaps, you are wondering who is affected by a certain watershed. If you are limited to postal addresses without being able to visualize the actual location of those addresses the inference is limited. Commuter habits, crime trends, pandemic evolution, and (fill in your example here) analyses are all improved with geocoding. Thanks, geocoding! . ",
    "url": "/Geo-Spatial/geocoding.html#why-is-geocoding-helpful",
    
    "relUrl": "/Geo-Spatial/geocoding.html#why-is-geocoding-helpful"
  },"406": {
    "doc": "Geocoding",
    "title": "Geocoding Services",
    "content": "It is important to recognize that there are many different geocoding platforms. There are others but here is a short list of platforms to consider: . | Geocodio | Google’s geocode API service | IPUMS Geomarker | ArcGIS | . When you are deciding which geocode platform to use some important things to keep in mind are pricing structures and other specific features of the platfrom like bulk geocoding and coverage. For example, Geocodio is much more suited to geocode big data sets than Google’s platform. However, Geocodio is only able to geocode within the United States and Cananda whereas Google has international capabilities. Google is better at guessing what location you are trying to geocode (“the White House”) than Geocodio, but Geocodio offers census appends. The pricing sturcture is also nuanced across platforms. Here is a comparison chart provided by Geocodio that gives a flavor of what to consider when deciding which service to use (although you should bear in mind that vendor evaluations may be biased…) Lots to consider! In the end, which platform works best will depend on your preferences and the nature of your project. ",
    "url": "/Geo-Spatial/geocoding.html#geocoding-services",
    
    "relUrl": "/Geo-Spatial/geocoding.html#geocoding-services"
  },"407": {
    "doc": "Geocoding",
    "title": "Keep in Mind",
    "content": ". | Be attentive to accuracy and accuracy type. Just because something is spit out doesn’t mean you should use/trust it | When you use a service like Geocodio, you need to consider pricing (2,500 free lookups per day) | One size doesn’t necesarily fit all–tailor the geocode platform you choose to your project | . ",
    "url": "/Geo-Spatial/geocoding.html#keep-in-mind",
    
    "relUrl": "/Geo-Spatial/geocoding.html#keep-in-mind"
  },"408": {
    "doc": "Geocoding",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/geocoding.html#implementations",
    
    "relUrl": "/Geo-Spatial/geocoding.html#implementations"
  },"409": {
    "doc": "Geocoding",
    "title": "Python",
    "content": "Geopy is a Python package thay provides a front end for a large number of geocoding APIs, including OpenStreetMap, Bing, Google, ArcGIS, and more. Below is an example of using geopy. Users may also want to explore the geocoder Python package. We’ll use the OpenStreetMap API to do the geocoding. It’s important to note that this API has some fair usage conditions including a maximum of 1 request per second, that you provide an informative ‘user agent’ parameter, and that you clearly display attribution (thank you OpenStreetMap!). For bulk geocoding, you may need to pay a fee to a provider. # If you don't have it, install geopy using 'pip install geopy' from geopy.geocoders import Nominatim # Create a geolocator using Open Street Map (aka Nominatim) # Use your own user agent identifier here geolocator = Nominatim(user_agent='LOST_geocoding_page') # Pass an address to retrieve full location information: location = geolocator.geocode('Bank of England') print(location.address) # &gt;&gt; Bank of England, 8AH, Threadneedle Street, Bishopsgate, City of London, # England, EC2R 8AH, United Kingdom print(location.latitude, location.longitude) # &gt;&gt; 51.51413225 -0.08892476721255456 # We can also reverse geocode from a lat and lon: scnd_location = geolocator.reverse(\"51.529969, -0.127688\") print(scnd_location.address) # &gt;&gt; British Library, 96, Euston Road, Bloomsbury, London Borough of Camden, # England, NW1 2DB, United Kingdom . ",
    "url": "/Geo-Spatial/geocoding.html#python",
    
    "relUrl": "/Geo-Spatial/geocoding.html#python"
  },"410": {
    "doc": "Geocoding",
    "title": "rgeocodio (R + Geocodio)",
    "content": "This example will talk specifically about Geocodio and how to use the Goecodio platform in R studio. Geocodio . Geocodio’s website is very straight forward, but I will briefly walk through the process: . | Start by making an account. This account will allow you to do your geocoding with Geocodio as well as get a Geocodio API which we can use in R studio. | To geocode on the website you can either upload a spreadsheet or copy and paste addresses into the input window. I highly recommend a spreadsheet which takes a specific format . | Geocodio will ask you to make edits if the data you have provided isn’t accurate enough . | Once your data is in satisfactory form Geodio allows you to make appends which allows you to include information pertaining to the addresses you wish to geocode (e.g. what State Legislative District the address is in or Census ACS Demographic information for the addresses you are geocoding) . | Finally Geocodio will geocode your addresses and return a downloadable csv file. The cost and the time of this process depends on the size of your data. For example, 250,000 addresses can be geocoded for $123.75 and will take about an hour to process. For estimates of both cost and time click here . | . ",
    "url": "/Geo-Spatial/geocoding.html#rgeocodio-r--geocodio",
    
    "relUrl": "/Geo-Spatial/geocoding.html#rgeocodio-r--geocodio"
  },"411": {
    "doc": "Geocoding",
    "title": "Example",
    "content": "rgeocodio allows you to access the Geocodio platform in R studio. Instead of the steps mentioned above you can use the rgeocodio to perform the same functions. In order to install rgeocodio you will need to load the devtools package. Install it if you haven’t already install.packages(\"devtools\"). Once devtools is loaded run:devtools::install_github('hrbrmstr/rgeocodio'). rgeocodio uses an API that you can get from the geocodio website. To get an API visit geocodio’s website. Then save it in your Renviron. To save the API in your Renvrion: . | Open the Renviron by running usethis::edit_r_environ() | Once you are in the Renviron name and save the API you got from Geocodio. Maybe something like: | . #geocodio_API = 'your api` . | Save your Renviron and then restart your R session just to be sure that the API is saved. | . Now that you have your API saved in R you still need to authorize the API in your R session. Do so by running gio_auth(). # If necessary # install.packages(c('rgeocodio','readxl','tidyverse')) library(rgeocodio) gio_auth(force = F) . A quick note, force makes you set a new geocodio API key for the current environment. In general you will want to run force=F. Lets try a regeocodio example. Say you want to get the coordinates of the White House. You could run: . rgeocodio::gio_geocode('1600 Pennsylvania Ave NW, Washington DC 20500') . Most of these variables are intuitive but I want to spend a few seconds on accuracy and accuracy type which we can learn more about here. | Accuracy: because geocodio is interpolating the output will tell you how confident geocodio is in its estimation. Anything below 0.8 should be considered not accurate enough, but that is up to the user. | Accuracy Type: interpolation uses the closest know geocodes. So if the closest geocodes are, for instance two ends of a street and you are trying to geocode a location somewhere on that street then the accuracy type will be “street.” In this case the accuracy type is “rooftop” which means the buildings on either side of the location were used to interpolate your query. Again, smartystreets has a good explanation of this. | . What if we want to geocode a bunch of addresses at once? To geocode multiple addresses at once we will use gio_batch_geocode. The data that we enter will need to be a character vector of addresses. library(readxl) library(tidyverse) addresses&lt;- c('Yosemite National Park, California', '1600 Pennsylvania Ave NW, Washington DC 20500', '2975 Kincaide St Eugene, Oregon, 97405') gio_batch_geocode(addresses) . You will notice that the output is a list with dataframes of the results embedded. There are a number of ways to extract the relevant data but one approach would be: . addresses&lt;- c('Yosemite National Park, California', '1600 Pennsylvania Ave NW, Washington DC 20500', '2975 Kincaide St Eugene, Oregon, 97405') extract_function&lt;- function(addresses){ data&lt;-gio_batch_geocode(addresses) vector&lt;- (1: length(addresses)) df_function&lt;-function(vector){ df&lt;-data$response_results[vector] df&lt;-df%&gt;%as.data.frame() } geocode_data&lt;-do.call(bind_rows, lapply(vector, df_function)) return(geocode_data) } extract_function(addresses) . Reverse geocoding uses gio_reverse and gio_batch_reverse. For gio_reverse you submit a longitude-latitude pair: . gio_reverse(38.89767, -77.03655) . For gio_batch_reverse we will submit a vector of numeric entries ordered by c(longitude, latitude): . #make a dataset data&lt;-data.frame( lat = c(35.9746000, 32.8793700, 33.8337100, 35.4171240), lon = c(-77.9658000, -96.6303900, -117.8362320, -80.6784760) ) gio_batch_reverse(data) . Notice that the output gives us multiple accuracy types. What about geocoding the rest of the world, chico? . rgeocodio::gio_batch_geocode('523-303, 350 Mokdongdong-ro, Yangcheon-Gu, Seoul, South Korea 07987') . gasp Geocodio only works, from my understanding, in the United States and Canada. We would need to use a different service like Google’s geocoder to do the rest of the world. ",
    "url": "/Geo-Spatial/geocoding.html#example",
    
    "relUrl": "/Geo-Spatial/geocoding.html#example"
  },"412": {
    "doc": "Get a List of Files",
    "title": "Get a List of Files",
    "content": "When cleaning and compiling data, it is common to need to get a list of files in a directory so that you can loop over them. Often, the goal is to open the files one at a time so you can combine them together in some way. For example, perhaps you have one data file per month, and want to compile them all together into a single data set. ",
    "url": "/Other/get_a_list_of_files.html",
    
    "relUrl": "/Other/get_a_list_of_files.html"
  },"413": {
    "doc": "Get a List of Files",
    "title": "Keep in Mind",
    "content": ". | Giving your files a consistent naming scheme will often make it easier to get this to work, as many approaches require you to give a template for the file names you want. | Before doing this you will probably find it useful to Set a Working Directory | . ",
    "url": "/Other/get_a_list_of_files.html#keep-in-mind",
    
    "relUrl": "/Other/get_a_list_of_files.html#keep-in-mind"
  },"414": {
    "doc": "Get a List of Files",
    "title": "Implementations",
    "content": "Note that, because these code examples necessarily refer to files on disk, they might not run properly if copied and pasted. But they can be used as templates. ",
    "url": "/Other/get_a_list_of_files.html#implementations",
    
    "relUrl": "/Other/get_a_list_of_files.html#implementations"
  },"415": {
    "doc": "Get a List of Files",
    "title": "Julia",
    "content": "The readdir() function returns the list of all files and subdirectories in the working directory as a one-dimensional array of strings. readdir() . It is also possible to return the list of all files and subdirectories in any given directory using the dir argument, which may be used to specify both absolute and relative paths. path_abs = \"C:/this/is/an/absolute/path\" readdir(dir = path_abs) path_rel = \"this/is/a/relative/path\" readdir(dir = path_rel) . The returned object contains relative paths by default, but it is possible to return absolute paths by setting the join argument equal to true. readdir(join = true) . Lastly, we may filter file and subdirectory names using the filter() and contains() functions. Suppose we would like to obtain all file and subdirectory names in the working directory that contain the string “lost”. y = readdir() filter(contains(\"lost\"), y) . Note: The readdir() function returns hidden subdirectories along with regular ones. ",
    "url": "/Other/get_a_list_of_files.html#julia",
    
    "relUrl": "/Other/get_a_list_of_files.html#julia"
  },"416": {
    "doc": "Get a List of Files",
    "title": "Python",
    "content": "The glob module finds all pathnames matching a specified pattern and stores them in a list. import glob # Retrieve all csvs in the working directory list_of_files = glob.glob('*.csv') # Retrieve all csvs in the working directory and all sub-directories list_of_files = glob.glob('**/*.csv', recursive=True) . ",
    "url": "/Other/get_a_list_of_files.html#python",
    
    "relUrl": "/Other/get_a_list_of_files.html#python"
  },"417": {
    "doc": "Get a List of Files",
    "title": "R",
    "content": "The list.files() function can produce a list of files that can be looped over. # Get a list of all .csv files in the Data folder # (which sits inside our working directory) filelist &lt;- list.files('Data','*.csv') # filelist just contains file names now. If we want it to # open them up from the Data folder we must say so filelist &lt;- paste0('Data/',filelist) # Read them all in and then row-bind them together # (assuming they're all the same format and can be rbind-ed) datasets &lt;- lapply(filelist,read.csv) data &lt;- do.call(rbind,datasets) # Or, use the tidyverse with purrr # (assuming they're all the same format and can be rbind-ed) library(tidyverse) data &lt;- filelist %&gt;% map(read_csv) %&gt;% bind_rows() . ",
    "url": "/Other/get_a_list_of_files.html#r",
    
    "relUrl": "/Other/get_a_list_of_files.html#r"
  },"418": {
    "doc": "Get a List of Files",
    "title": "Stata",
    "content": "The dir function in Stata can be used to produce a list of files, which can be stored in a local (or global), and then looped over. * Get a list of all .xlsx files in the Data folder * (which sits inside our working directory) local filelist: dir \"Data/\" files \"*.xlsx\" * Append them all together * (assuming this is what you want to do) * filelist only contains file names - if we want it to look in * the Data folder, we must say so explicitly local firsttime = 1 foreach f in `filelist' { * import the data import excel using \"Data/`f'\", clear firstrow * Append it to the data we've already imported * Unless this is the first one we opened, in which * case just start a new file if `firsttime' == 0 { append using compiled_data.dta } save compiled_data.dta, replace local firsttime = 0 } . ",
    "url": "/Other/get_a_list_of_files.html#stata",
    
    "relUrl": "/Other/get_a_list_of_files.html#stata"
  },"419": {
    "doc": "Generalized Method of Moments",
    "title": "Generalized Method of Moments",
    "content": "GMM is an estimation technique that does not require strong assumptions about the distributions of the underlying parameters. The key intuition is that if we know the expected value of population moments (such as mean or variance), then the sample equivalents will converge to that expected value using the law of large numbers. If the moments are functions of the parameters that we wish to estimate, then we can use these moment restrictions to estimate our parameters. Suppose we have a vector of \\(K\\) parameters we want to estimate, where the true values of those parameters are \\(\\theta_0\\) and a set of \\(L \\geq K\\) moment conditions provided by theory, \\(E\\left[g(Y_i,\\theta_0)\\right] = 0\\), where \\(Y_i\\) is a vector of variables corresponding to one observation in our data. The sample version of the population moments are \\(\\hat{g}(\\theta) \\equiv \\frac{1}{n}\\sum_{i=1}^ng(Y_i,\\theta)\\). Thus, we are trying find \\(\\theta\\) that makes \\(\\hat{g}(\\theta)\\) as close to \\(0\\) as possible. The GMM estimator is . \\[\\hat{\\theta}_{GMM} = \\underset{\\theta}{\\operatorname{argmin}}\\hat{g}(\\theta)^\\prime \\hat{W}\\hat{g}(\\theta)\\] Where \\(\\hat{W}\\) is some positive semi-definite matrix, which gives us a consistent estimate of true parameters \\(\\theta\\) under relatively benign assumptions. For more details, visit the Wikipedia Page. ",
    "url": "/Model_Estimation/GLS/gmm.html",
    
    "relUrl": "/Model_Estimation/GLS/gmm.html"
  },"420": {
    "doc": "Generalized Method of Moments",
    "title": "Keep in Mind",
    "content": ". | There are two important assumptions necessary for identification, meaning that \\(\\hat{\\theta}_{GMM}\\) is uniquely minimized at the true value \\(\\theta_0\\) . | Order Condition: There are at least as many moment conditions as parameters to be estimated, \\(L \\geq K\\). | Rank Condition: The \\(K \\times L\\) matrix of derivatives \\(\\bar{G}_n(\\theta_0)\\) will have full column rank, \\(L\\). | . | Any positive semi-definite weight matrix \\(\\hat{W}\\) will produce an asymptotically consistent estimator for \\(\\theta\\), but we want to choose the weight matrix that gives estimates the smallest asymptotic variance. There are various methods for choosing \\(\\hat{W}\\) outlined here, which are various iterative processes | Sargan-Hansen J-Test can be used to test the specification of the model, by determining whether the sample moments are sufficiently close to zero | The small sample properties of GMM are not great, consider bootstrapping or set \\(\\hat{W} = I\\) (See Hayashi, Econometrics pg 215) | . ",
    "url": "/Model_Estimation/GLS/gmm.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/gmm.html#keep-in-mind"
  },"421": {
    "doc": "Generalized Method of Moments",
    "title": "Also Consider",
    "content": ". | Under certain moment conditions, GMM is equivalent to many other estimators that are used more commonly. These include… . | OLS if \\(E[x_i(y_i - x_i^\\prime\\beta)]=0\\) | Instrumental Variables if \\(E[z_i(y_i - x_i^\\prime\\beta)]=0\\) | . | Maximum likelihood estimation is also a specific case of GMM that makes assumptions about the distributions of the parameters. This gives maximum likelihood better small sample properties, at the cost of the stronger assumptions | . ",
    "url": "/Model_Estimation/GLS/gmm.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/gmm.html#also-consider"
  },"422": {
    "doc": "Generalized Method of Moments",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/gmm.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/gmm.html#implementations"
  },"423": {
    "doc": "Generalized Method of Moments",
    "title": "R",
    "content": "The gmm package (link) can be used to implement GMM in R, with the key function being gmm(). The first example recovers the parameters of a normal distribution, where the moment conditions are derived from the normal distribution. library(gmm) # Parameters we are going to estimate mu = 3 sigma = 2 # Generating random numbers set.seed(0219) n = 500 x = rnorm(n = n, mean = mu, sd = sigma) # Moment restrictions g1 &lt;- function(theta, x) { m1 = (theta[1]-x) m2 = (theta[2]^2 - (x - theta[1])^2) m3 = x^3-theta[1]*(theta[1]^2+3*theta[2]^2) f = cbind(m1,m2,m3) return(f) } # Running GMM gmm_mod = gmm( # Moment restriction equations g = g1, # Matrix of data x = x, # Starting location for minimization algorithm t0 = c(0,0) # Required when g argument is a function ) # Reporting results summary(gmm_mod) . Another common application of GMM is with linear moment restrictions. These can be specified by writing the regression formula as the g argument of the gmm() function and the matrix of instruments as the x argument. Suppose we have a model \\(y_i = \\alpha + \\beta x_i + \\epsilon_i\\), but \\(E[x_i(y_i - x_i^\\prime\\beta)]\\neq 0\\), so OLS would produce a biased estimate of \\(\\beta\\). If we have a vector of instruments \\(z_i\\) that are correlated with \\(x_i\\) and have moment conditions \\(E[z_i(y_i - x_i^\\prime\\beta)]=0\\), then we can use GMM to estimate \\(\\beta\\). # Setting parameter values alpha = 1 beta = 2 # Taking random draws set.seed(0219) z1 = rnorm(n = 500, 1,2) z2 = rnorm(n = 500,-1,1) e = rnorm(n = 500, 0, 1) # Collecting instruments Z = cbind(z1, z2) # Specifying model, where x is endogenous x = z1 + z2 + e y = alpha + beta * x + e # Running GMM lin_gmm_mod = gmm( g = y ~ x, x = Z ) # Reporting results summary(lin_gmm_mod) . ",
    "url": "/Model_Estimation/GLS/gmm.html#r",
    
    "relUrl": "/Model_Estimation/GLS/gmm.html#r"
  },"424": {
    "doc": "Generalized Method of Moments",
    "title": "Stata",
    "content": "Stata provides an official command gmm, which can be used for the estimation of models via this method if you provide moments of interest. The first example will be in recovering the coefficients that determine the distribution of a variable, assuming that variable follows a normal distribution. First lets simulate some data, and set the parameters . **# Normal distribution *** Parameters to estimate local mu = 3 local sigma = 2 *** Generate sample set seed 219 clear set obs 500 gen x = rnormal(`mu',`sigma') . In this code, I use local to identify the true parameters of interest. the mean mu and standard deviation sigma. Im also using set seed, to simulare data that can be replicated. The variable x is obtained based on a random draw from a normal distribution. The next step is to declare all the moment conditions. Here Im declaring them with local but could be written just as well with global or written directly instead of the ``m1’ expressions in the gmm` command. Keep in mind that locals only stay in memory locally. Once you run the program, they dissapear. Globals instead, can be used at any point after they are declared. For this reason, when using the code for this example, either copy it by hand into the console, or put it all in the same do file and run it together, so the locals are all in the same environment together. In the gmm syntax below, the coefficients of interest to be estimated, are written within curly brakets “{}”. I can also declare within the brakets initial values for the coefficients. Here mu=1 and sigma=1. Following the example above, we can declare 3 equations to define the moments for a normal distribution: the mean, the variance, and the kurtosis. However, because there are only 2 unknowns (mean and standard deviation), the model will be overidentified. This means that I can use either all 3 moments, or just any 2 of them. In this case, because the true distribution is normal, you only need two parameters to describe the distribution. Thus, you should get the same results, (or quite close), regardless of which pair of moments you use. *** Declare moments restrictions local m1 {mu=1}-x local m2 {sigma=1}^2 - (x-{mu=1})^2 local m3 x^3 - {mu=1}*({mu}^2+3*{sigma=1}^2) gmm (`m1') (`m2') (`m3'), winitial(identity) est sto m1 gmm (`m1') (`m2') , winitial(identity) est sto m2 gmm (`m1') (`m3') , winitial(identity) est sto m3 gmm (`m2') (`m3') , winitial(identity) est sto m4 est tab m1 m2 m3 m4, se . A second example for the use of gmm is for the estimation of standard linear regression models. For this, lets create some data, where the variable of interest is \\(X\\) . **# LR estimation *** Parameters to estimate local a0 1 local a1 2 clear set obs 500 *** Exogenous variables gen z1 = rnormal(1,2) gen z2 = rnormal(-1,1) *** unobserved error gen e = rnormal() *** Data Generating process gen x = z1+z2+e // X is endogenous to e gen y=`a0'+`a1'*x+e . Next, we can use gmm to estimate the model, under different assumptions . *** gmm ignoring endogeneity ** this is your error local m1 y-{a0}-{a1}*x ** which implies First order condition E(m1)=0 ** and E(x*m1)=0 gmm (`m1'), winitial(identity) instrument(x) *** gmm with endogeneity ** here, it implies E(z*m1)=0 local m1 y-{a0}-{a1}*x gmm (`m1'), winitial(identity) instrument(z1) est sto m1 gmm (`m1'), winitial(identity) instrument(z2) est sto m2 gmm (`m1'), winitial(identity) instrument(z1 z2) est sto m3 est tab m1 m2 m3, se ** I could also write the moment conditions more explicitly local m1 y-{a0}-{a1}*x gmm (`m1') (z1*(`m1')), winitial(identity) est sto m1 gmm (`m1') (z2*(`m1')), winitial(identity) est sto m2 gmm (`m1') (z1*(`m1')) (z2*(`m1')), winitial(identity) est sto m3 est tab m1 m2 m3, se ** producing the same results as before . ",
    "url": "/Model_Estimation/GLS/gmm.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/gmm.html#stata"
  },"425": {
    "doc": "Graph Themes",
    "title": "Introduction",
    "content": "Graph themes help enhance an already informative line, scatter, bar, or boxplot. With the implementation of useful themes, graphs become aesthetically pleasing; with improper implementation of themes, graphs become muddled and confusing. ",
    "url": "/Presentation/Figures/graph_themes.html#introduction",
    
    "relUrl": "/Presentation/Figures/graph_themes.html#introduction"
  },"426": {
    "doc": "Graph Themes",
    "title": "Keep in Mind",
    "content": ". | When adding themes to a graph, ensure that the theme being used is appropriate for the data that is represented. For example, a scatterplot works best with graph themes that include gridlines while geometric data graphs look better without any lines. | While there are a whole host of existing themes that can improve the appearance of graphs, modifying or creating your own themes can be the most effective way to visualize data. Though seemingly complicated, customizing your own graph theme can be easy depending on the graphing package used, and can be rewarding. | . ",
    "url": "/Presentation/Figures/graph_themes.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/graph_themes.html#keep-in-mind"
  },"427": {
    "doc": "Graph Themes",
    "title": "Also Consider",
    "content": ". | Not everyone can see the entire color spectrum. When using certain themes or arguments within themes, bear in mind that it can be hard to distinguish between similar colors or other color combinations for those with color-vision deficiency. | . ",
    "url": "/Presentation/Figures/graph_themes.html#also-consider",
    
    "relUrl": "/Presentation/Figures/graph_themes.html#also-consider"
  },"428": {
    "doc": "Graph Themes",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/graph_themes.html#implementations",
    
    "relUrl": "/Presentation/Figures/graph_themes.html#implementations"
  },"429": {
    "doc": "Graph Themes",
    "title": "R",
    "content": "In this R example, we will be using the ggplot2 package and the theme function to add themes to graphs. The dataset we are using comes built into ggplot2 and is known as msleep. The msleep dataset includes the sleep patterns of 83 different animals, as well as characteristics like the genus, brain weight, and whether the animal is a carnivore, herbivore, or other -vore. # Load in necessary packages library(ggplot2) library(ggthemes) library(hrbrthemes) # Load in desired data (msleep) data(msleep) . After viewing the data, we can create a scatterplot with the y-axis representing the total amount of hours the animal is asleep, the x axis representing the weight of the animal’s brain, and the dots of the scatterplot colored by the -vore type of the animal. #basic scatterplot of sleep against brain weight sleep_plot &lt;- ggplot(data=msleep, aes(x = sleep_total, y = brainwt, color = vore)) + geom_point() sleep_plot . Now that we have this basic plot, we can use the theme function to customize it. The ggplot2 package comes with some built-in theme options which can be explored at the Complete themes page of the ggplot2 documentation provided by tidyverse. Since we are using a scatterplot in this example, theme_bw is a simple and elegant way to display the data. Here is an example of what a theme_bw graph looks like: . #Here is the animal sleep plot we made earlier with all the ggplot2 theme_bw() added to it bw = sleep_plot + theme_bw() bw . This is a nice, clean look, with gridlines, borders, and clearly defined axes. While theme_bw does an excellent job displaying the data, there are some themes that do not lend themselves easily to scatterplots. One such theme is theme_void, shown below: . # Animal sleep plot with theme_void() from ggplot2 void = sleep_plot + theme_void() void . This theme removes all gridlines, borders, and axes, leading to a very confusing image of floating colored points and a legend. This theme is useful for geometric data, flowcharts, or other kinds of visualizations that are clearer without any axes or background. For scatterplots it is less than ideal. These are very basic themes to use, but they can clean up a graph in a pinch. However, there are other packages that contain their own prepackaged themes. A popular theming package is ggthemes. To check out multiple examples of ggthemes, visit the ALL YOUR FIGURE ARE BELONG TO US page of the official ggthemes website. And for more helpful insight into ggthemes and its arguments, visit the Introduction to ggthemes site by Jeffrey B. Arnold. While most of the ggthemes themes are great, some really stand out. One popular option is theme_tufte, which is a very minimal theme following the principles of prominent data visualization thinker Edward Tufte. Three other themes (theme_economist, theme_fivethirtyeight, and theme_wsj) all mimic the graph styles of major media/news outlets. If you really want to sell your graph and look like mainstream media, here are some examples: . # Theme following Edward Tufte tufte = sleep_plot + theme_tufte() tufte # theme_economist mimics graphs from The Economist magazine economist = sleep_plot + theme_economist() economist # theme_fivethirtyeight mimics graphs from FiveThirtyEight, a political/sports/statistics blogging site fivethirtyeight = sleep_plot + theme_fivethirtyeight() fivethirtyeight # theme_wsj mimcs graphs from The Wall Street Journal newspaper wsj = sleep_plot + theme_wsj() wsj . The data we plotted is rather simple and plain; these themes would look better on a more refined graph, one with proper names, clear positioning, and distinguished data points or lines, but these graphs still look better than the original. Another great repository of several ggplot2 themes is the hrbrthemes package. Themes in hrbrthemes change the formatting up a bit, so don’t be surprised if your axis titles move a bit. For this example, theme_modern_rc is used, as it makes the colored points of the graph pop off the screen. You can take your graph up another notch by adding one of the color options that hrbrthemes has to offer, in this case scale_color_ft: . # hrbrthemes graph with scale_color_ft and theme_modern_rc hrbr = sleep_plot + scale_color_ft() + theme_modern_rc() hrbr . There is much more that hrbrthemes has to offer beyond just theme_ functions, including other scaling options, font choices, and utilities. For more about hrbrthemes, check out the hrbrthemes page. If none of these existing themes are your cup of tea, you can try to create your own theme or modify one of those listed above to suit your tastes. To do this, you will use the theme function and include element_xx objects as arguments. Element objects that can be used are listed below: . | element_line() - can add color, linetype, and size arguments to line elements of a theme, like axis lines | element_text() - can add color, face, angle, justification, margins, and size arguments to text elements of a theme, like titles | element_rect() - can add color, fill, and size arguments to rectangular elements of a theme, like the panel window | element_blank() - can remove any element from a theme | . Please take note that customizing themes in this way will not do everything. It will not allow you to change aesthetic properties of your graph geometry, such as the different colors you assign with aes(color=). But it does allow you to make your graphs more aesthetically pleasing by changing font size or color, adding shapes around legends, and filling the backgrounds of graphs with colors and gridlines. For ways to customize your own theme in ggplot2, check out the R Graphics Cookbook by Winston Chang. Chang offers an excellent section on modifying graph themes in Chapter 9.4; at the bottom of the page is a helpful table that outlines each argument that can be used by theme, a description of what each does, and the element_xx to specify when using an argument. For a simple example of creating a theme, run the following code: . # This code modifies the legend of the graph legend = sleep_plot + theme( legend.background = element_rect(fill = \"white\", color = \"dodgerblue\", size = 1), legend.title = element_text(color = \"brown\", face = \"bold\", size = 18), legend.text = element_text(color = \"brown\", face = \"bold\", size = 10), legend.key = element_rect(color = \"dodgerblue\", size = 0.5) ) legend # We use blue and brown here, as most color-blind people can distinguish these two colors # element_rect changes the box around the legend and the boxes around the colors for -vore # element_text changes the text color, size, and face within the legend . You can also modify an already existing theme using the same method as above. Here is what it looks like to modify the axis titles of the hrbrthemes graph we created earlier: . modified = hrbr + theme(axis.title.x = element_text(colour = \"yellow\", size = 12, face = \"bold\"), axis.title.y = element_text(colour = \"yellow\", size = 12, face = \"bold\")) modified . This is just scratching the surface of graph themes in R. Even more theme packages exist, like ggpubr, with the excellent publication-ready theme_pubr(), and wesanderson; these won’t be explored on this page, but if you’re interested check out this ggpubr page and this wesanderson page. ",
    "url": "/Presentation/Figures/graph_themes.html#r",
    
    "relUrl": "/Presentation/Figures/graph_themes.html#r"
  },"430": {
    "doc": "Graph Themes",
    "title": "Graph Themes",
    "content": " ",
    "url": "/Presentation/Figures/graph_themes.html",
    
    "relUrl": "/Presentation/Figures/graph_themes.html"
  },"431": {
    "doc": "Handling Raster Data",
    "title": "Handling Raster Data",
    "content": "Raster Data is a form of storing data in a matrix organized by column and rows that contain values of information for an object. This matrix is formed of either cells or pixels. Information in this matrix can be discrete/thematic data like geospatial data, continuous data like temperature, and scanned data such as drawings. Since raster data is simple in design, the application of raster data is far reaching. Digital photographs and some LCD monitors have their data stored in raster format. In the context of geo-spatial, raster data is one pillar of geo-spatial analysis. Continuous data such as landscapes and population densities can be presented as a surface map. Discrete data from sources like satellites can be analyzed so that maps can be made categorizing pixels to determine land use. ",
    "url": "/Geo-Spatial/handling_raster_data.html",
    
    "relUrl": "/Geo-Spatial/handling_raster_data.html"
  },"432": {
    "doc": "Handling Raster Data",
    "title": "Keep in Mind",
    "content": ". | Raster data files can become very large very quickly. Each cell contains information and the more accurate a graph or picture gets, the more information and number of cells needed grow rapidly. | Zooming in and out of a raster image can lead to unappealing graphs or even loss of information. Inaccuracies can also occur depending on the dataset’s dimensions. | . ",
    "url": "/Geo-Spatial/handling_raster_data.html#keep-in-mind",
    
    "relUrl": "/Geo-Spatial/handling_raster_data.html#keep-in-mind"
  },"433": {
    "doc": "Handling Raster Data",
    "title": "Also Consider",
    "content": ". | Another way to store and analyze data is using a vector approach. Data Carpentry has a very useful online lecture about Raster and vector data. | . ",
    "url": "/Geo-Spatial/handling_raster_data.html#also-consider",
    
    "relUrl": "/Geo-Spatial/handling_raster_data.html#also-consider"
  },"434": {
    "doc": "Handling Raster Data",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/handling_raster_data.html#implementations",
    
    "relUrl": "/Geo-Spatial/handling_raster_data.html#implementations"
  },"435": {
    "doc": "Handling Raster Data",
    "title": "R",
    "content": "# if necessary # install.packages(c('sf', 'raster', 'stars', 'ggplot2', 'dplyr')) library(sf) library(raster) library(ggplot2) library(dplyr) library(stars) #The package **stars** contains data that can be used as a good starting point on how to work with raster data. ob_tif = system.file(\"tif/L7_ETMs.tif\", package = \"stars\") ob = read_stars(ob_tif) ob . As you can see above, ther is alot of information packed into raster objects. Near the bottom of the table you will see X and Y values. We can use a function from sf to quickly find the dimensions of the file. st_bbox(ob) #Now lets plot this information with the plot command with the raster package. plot(ob) . The next example will be using information from Harvard Forest and Quabbin Watershed NEON. This information is open to the public, but can be overwhelming. A single file can be found under the Data folder, then the Handling_Raster_Data folder. harv = raster(\"https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Geo-Spatial/Data/Handling_Raster_Data/HARV_dsmCrop.tif\") harv . There is a lot of meta data tied to every raster file. Note the line starting with CRS. This contains the bulk to the infromation you may be interested in. Notice at the end of the CRS line you can see “+units=m”. This means this data is measured in meters. summary(harv) #Here is another way to summarize the data. If you are more interested in learning about min, max, and NAs, then using the summary command is better. Notice the error statement. There is way too much data, so R is picking a random sample of the data. The summary with all the data can be pulled with the commented-out command below. # summary(harv, maxsamp = ncell(harv)) # In order to use this information in ggplot, we will need to turn this data into a dataframe. harv_m = as.data.frame(sanj, xy = TRUE) str(harv_m) #str command gives you a quick third way to summarize the data. The raster data was turned into a dataframe with the X, Y, and fill(HARV_dsmCrop) as their own variables. ggplot() + geom_raster(data = sanj_m, aes(x=x, y=y, fill=HARV_dsmCrop)) + scale_fill_viridis_c() #Look at this beautiful graph! #scale_fill_viridis_c() is a command in ggplot2 that is color blind friendly. ",
    "url": "/Geo-Spatial/handling_raster_data.html#r",
    
    "relUrl": "/Geo-Spatial/handling_raster_data.html#r"
  },"436": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Heteroskedasticity-consistent (HC) standard errors",
    "content": "Heteroskedasticity is when the variance of a model’s error term is related to the predictors in that model. For more information, see Wikipedia: Heteroscedasticity. Many regression models assume homoskedasticity (i.e. constant variance of the error term), especially when calculating standard errors. So in the presence of heteroskedasticity, standard errors will be incorrect. Heteroskedasticity-consistent (HC) standard errors — also called “heteroskedasticity-robust”, or sometimes just “robust” standard errors — are calculated without assuming such homoskedasticity. For more information, see Wikipedia: Heteroscedasticity-consistent standard errors. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#heteroskedasticity-consistent-hc-standard-errors",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#heteroskedasticity-consistent-hc-standard-errors"
  },"437": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Keep in Mind",
    "content": ". | Robust standard errors are a common way of dealing with heteroskedasticity. However, they make certain assumptions about the form of that heteroskedasticity which may not be true. You may instead want to use GMM instead. | For nonlinear models like Logit, heteroskedasticity can bias estimates in addition to messing up standard errors. Simply using a robust covariance matrix will not eliminate this bias. Check the documentation of your nonlinear regression command to see whether its robust-error options also adjust for this bias. If not, consider other ways of dealing with heteroskedasticity besides robust errors. | There are multiple kinds of robust standard errors, for example HC1, HC2, and HC3. Check in to the kind available to you in the commands you’re using. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#keep-in-mind"
  },"438": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Also Consider",
    "content": ". | Generalized Method of Moments | Cluster-Robust Standard Errors | Bootstrap Standard Errors | Jackknife Standard Errors | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#also-consider",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#also-consider"
  },"439": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#implementations",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#implementations"
  },"440": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "R",
    "content": "The easiest way to obtain robust standard errors in R is with the estimatr package (link) and its family of lm_robust functions. These will default to “HC2” errors, but users can specify a variety of other options. # If necessary, install estimatr # install.packages(c('estimatr')) library(estimatr) # Get mtcars data # data(mtcars) ## Optional: Will load automatically anyway # Default is \"HC2\". Here we'll specify \"HC3\" just to illustrate. m1 &lt;- lm_robust(mpg ~ cyl + disp + hp, data = mtcars, se_type = \"HC3\") summary(m1) . Alternately, users may consider the vcovHC function from the sandwich package (link), which is very flexible and supports a wide variety of generic regression objects. For inference (t-tests, etc.), use in conjunction with the coeftest function from the lmtest package (link). # If necessary, install lmtest and sandwich # install.packages(c('lmtest','sandwich')) library(sandwich) library(lmtest) # Create a normal regression model (i.e. without robust standard errors) m2 &lt;- lm(mpg ~ cyl + disp + hp, data = mtcars) # Get the robust VCOV matrix using sandwich::vcovHC(). We can pick the kind of robust errors # with the \"type\" argument. Note that, unlike estimatr::lm_robust(), the default this time # is \"HC3\". I'll specify it here anyway just to illustrate. vcovHC(m2, type = \"HC3\") sqrt(diag(vcovHC(m2))) ## HAC SEs # For statistical inference, use together with lmtest::coeftest(). coeftest(m2, vcov = vcovHC(m2)) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#r",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#r"
  },"441": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Stata",
    "content": "Stata has robust standard errors built into most regression commands, and they generally work the same way for all commands. * Load in auto data sysuse auto.dta, clear * Just add robust to the options of the regression * This will give you HC1 regress price mpg gear_ratio foreign, robust * For other kinds of robust standard errors use vce() regress price mpg gear_ratio foreign, vce(hc3) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#stata",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#stata"
  },"442": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Heteroskedasticity-consistent standard errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html"
  },"443": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Heatmap Colored Correlation Matrix",
    "content": "A correlation matrix shows the correlation between different variables in a matrix setting. However, because these matrices have so many numbers on them, they can be difficult to follow. Heatmap coloring of the matrix, where one color indicates a positive correlation, another indicates a negative correlation, and the shade indicates the strength of correlation, can make these matrices easier for the reader to understand. ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html"
  },"444": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Keep in Mind",
    "content": ". | Even with heatmap coloring, very large correlation matrices can still be difficult to read, as you must pinpoint which variable names go with which cell of the matrix. Consider breaking big correlation matrices up into smaller ones, or limiting the amount of data you’re trying to show in some other way. | . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#keep-in-mind"
  },"445": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Also Consider",
    "content": ". | You may just want to create a correlation matrix | . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#also-consider",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#also-consider"
  },"446": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#implementations",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#implementations"
  },"447": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Python",
    "content": "We present two ways you can create a heatmap. First, the seaborn package has a great collection of premade plots, one of which is a heatmap we’ll use. The second we’ll only point you to, which is a “by hand” approach that will allow you more customization. For the by hand approach, see this guide. For the seaborn approach, you will need to pip install seaborn or conda install seaborn before continuing. Once you’ve done that, the follow code will produce the below plot. # Ganked from https://seaborn.pydata.org/examples/many_pairwise_correlations.html # Assumes you have run `pip install numpy pandas matplotlib scikit-learn seaborn` # Standard imports import numpy as np import pandas as pd from matplotlib import pyplot as plt # For this example we'll use Seaborn, which has some nice built in plots import seaborn as sns # Grab a data set from scikit-learn from sklearn.datasets import fetch_california_housing data = fetch_california_housing() df = pd.DataFrame( np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'] ) # Create the correlation matrix corr = df.corr() # Generate a mask for the upper triangle; True = do NOT show mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio # More details at https://seaborn.pydata.org/generated/seaborn.heatmap.html sns.heatmap( corr, # The data to plot mask=mask, # Mask some cells cmap=cmap, # What colors to plot the heatmap as annot=True, # Should the values be plotted in the cells? vmax=.3, # The maximum value of the legend. All higher vals will be same color vmin=-.3, # The minimum value of the legend. All lower vals will be same color center=0, # The center value of the legend. With divergent cmap, where white is square=True, # Force cells to be square linewidths=.5, # Width of lines that divide cells cbar_kws={\"shrink\": .5} # Extra kwargs for the legend; in this case, shrink by 50% ) # You can save this as a png with # f.savefig('heatmap_colored_correlation_matrix_seaborn_python.png') . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#python",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#python"
  },"448": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "R",
    "content": "We will be creating our heatmap in two different ways. First, we will be using the corrplot package, which is tailor-made for the task and is very easy to use. Then, we will be using ggplot2 with geom_tile, which requires much more preprocessing to use, but then provides access to the entirety of the ggplot2 package for customization. First, we will use corrplot: . # Install the corrplot package if necessary # install.packages('corrplot') # Load in the corrplot package library(corrplot) # Load in mtcars data data(mtcars) # Don't use too many variables or it will get messy! mtcars &lt;- mtcars[,c('mpg','cyl','disp','hp','drat','wt','qsec')] # Create a corrgram corrplot(cor(mtcars), # Using the color method for a heatmap method = 'color', # And the lower half only for easier readability type = 'lower', # Omit the 1's along the diagonal to bring variable names closer diag = FALSE, # Add the number on top of the color addCoef.col = 'black' ) . This results in: . Now we will make the graph using ggplot2. We will also make a little use of dplyr and tidyr, and so we’ll load them all as a part of the tidyverse. This example makes use of this guide. # Install the tidyverse if necessary # install.packages('tidyverse') # Load in the tidyverse library(tidyverse) # Load in mtcars data data(mtcars) # Create a correlation matrix. C &lt;- mtcars %&gt;% # Don't use too many variables or it will get messy! # We use dplyr's select() here but there are other ways to limit variables, like [] select(cyl, disp, drat, hp, mpg, qsec, wt) %&gt;% # Correlation matrix cor() # At this point, we can limit the matrix to just its lower half # Note this will give weird results if you didn't select variables in alphabetical order earlier C[upper.tri(C)] &lt;- NA C &lt;- C %&gt;% # Turn it into a data frame as.data.frame() %&gt;% # with a column for the variable names. # We use dplyr's mutate to create this column but it could be made with $ # the . here means \"the data set we're working with\" mutate(Variable = row.names(.)) # Use tidyr's pivot_longer to reshape to long format # There are other ways to reshape too C_Long &lt;- pivot_longer(C, cols = c(mpg, cyl, disp, hp, drat, wt, qsec), # We will want this option for sure if we dropped the # upper half of the triangle earlier values_drop_na = TRUE) %&gt;% # Make both variables into factors mutate(Variable = factor(Variable), name = factor(name)) %&gt;% # Reverse the order of one of the variables so that the x and y variables have # Opposing orders, common for a correlation matrix mutate(Variable = factor(Variable, levels = rev(levels(.$Variable)))) # Now we graph! ggplot(C_Long, # Our x and y axis are Variable and name # And we want to fill each cell with the value aes(x = Variable, y = name, fill = value))+ # geom_tile to draw the graph geom_tile() + # Color the graph as we like # Here our negative correlations are red, positive are blue # gradient2 instead of gradient gives us a \"mid\" color which we can make white scale_fill_gradient2(low = \"red\", high = \"blue\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name=\"Pearson\\nCorrelation\") + # Axis names don't make much sense labs(x = NULL, y = NULL) + # We don't need that background theme_minimal() + # If we need more room for variable names at the bottom, rotate them theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) + # We want those cells to be square! coord_fixed() + # If you also want the correlations to be written directly on there, add geom_text geom_text(aes(label = round(value,3))) . This results in: . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#r",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#r"
  },"449": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "SAS",
    "content": "See this guide. ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#sas",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#sas"
  },"450": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Stata",
    "content": "Stata has the installable package corrtable which produces heatmap correlation tables. Handily, it puts the variable labels (or names, if labels aren’t available) along the diagonal where they are easy to read. Note that it does run quite slowly. * Install corrtable if necessary * ssc install corrtable * Get auto data sysuse auto.dta, clear * Make correlation table * The half option just shows the lower triangle and puts variable names on the axis. * The flag1 and howflag1 options tell corrtable to plot positive correlations (r(rho &gt; 0)) * as blue (blue*.1) * and flag2 and howflag2 similarly tell it to plot negative correlations as pink. corrtable price-length, half flag1(r(rho) &gt; 0) howflag1(plotregion(color(blue * 0.1))) flag2(r(rho) &lt; 0) howflag2(plotregion(color(pink*0.1))) . This results in: . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#stata",
    
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#stata"
  },"451": {
    "doc": "Heckman Correction Model",
    "title": "Heckman Correction Model",
    "content": "The Heckman correction for sample selection is a method designed to be used in cases where the model can only be run on a subsample of the data that is not randomly selected. For example, a regression using \\(Wage\\) to predict \\(Hours Worked\\) cannot include people who don’t work, since we don’t observe their wage. The Heckman model views this sample selection process as a form of omitted variable bias. So, it (1) explicitly models the process of selecting into the sample, (2) transforms the predicted probability of being in the sample, and (3) adds a correction term to the model. For more information, see Wikipedia: Heckman correction. ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html"
  },"452": {
    "doc": "Heckman Correction Model",
    "title": "Keep in Mind",
    "content": ". | Conceptually, the Heckman model uses the regression covariates to predict selection, transforms the prediction, and then includes that transformation in the model. If there are no variables in the selection model that are excluded from the regression model, then the Heckman model is perfectly collinear, and is only statistically identified because the transformation is nonlinear (you may have heard the phrase “identified by nonlinearity” or “identified by the normality assumption”). That’s not ideal! You want to find an exclusion restriction - a variable that predicts selection, but does not belong in the final regression model - to avoid this collinearity. | . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#keep-in-mind"
  },"453": {
    "doc": "Heckman Correction Model",
    "title": "Also Consider",
    "content": ". | There are many ways to estimate a Heckman model. Maximum likelihood approaches generally have better statistical properties, but two-stage models are computationally simpler. Often you can look in the options of your Heckman estimator command to select an estimation method. | If your goal is to estimate the effect of a binary treatment by modeling selection into treatment, consider a Treatment Effect Model, or an Endogenous Switching Model which also allows predictors to work differently in different settings. | Standard Heckman models rely heavily on assumptions about the normality of error terms. You may want to consider Nonparametric Sample Selection Models. | . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#also-consider"
  },"454": {
    "doc": "Heckman Correction Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#implementations"
  },"455": {
    "doc": "Heckman Correction Model",
    "title": "Gretl",
    "content": "See here for a demonstration. ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#gretl",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#gretl"
  },"456": {
    "doc": "Heckman Correction Model",
    "title": "Python",
    "content": "See here for a demonstration. ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#python",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#python"
  },"457": {
    "doc": "Heckman Correction Model",
    "title": "R",
    "content": "# Install sampleSelection package if necessary # install.packages('sampleSelection') library(sampleSelection) # Get data from Mroz (1987, Econometrica) # which has Panel Study of Income Dynamics data for married women data(\"Mroz87\") # First consider our selection model # We only observe wages for labor force participants (lfp == 1) # So we model that as a function of work experience (linear and squared), # income from the rest of the family, education, and number of kids 5 or younger. # lfp ~ exper + I(exper^2) + faminc + educ + kids5 # Then we model the regression of interest. We're interested in modeling # wage as a function of work experience, education, and whether you're in a city # Here, we don't include family income or number of kids, under the assumption that they # do not belong in a wage model. These are our exclusion restrictions # (note these particular exclusion restrictions might be a little dubious! But hey, this paper's from 1987.) # wage ~ exper + I(exper^2) + educ + city # Put them together in a selection() command heck_model &lt;- selection(lfp ~ exper + I(exper^2) + faminc + educ + kids5, wage ~ exper + I(exper^2) + educ + city, Mroz87) summary(heck_model) . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#r",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#r"
  },"458": {
    "doc": "Heckman Correction Model",
    "title": "Stata",
    "content": "Stata allows to estimate the Heckman selection model using two approaches. A Maximum Likelikehood approach, and the two-step approach. * Get data from Mroz (1987, Econometrica) * which has Panel Study of Income Dynamics data for married women * (data via the sampleSelection package in R) import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Heckman_Correction_Model/Mroz87.csv\", clear * First, consider the regression of interest. * First consider our selection model * We only observe wages for labor force participants (lfp == 1) * So we model that as a function of work experience (linear and squared), * income from the rest of the family, education, and number of kids 5 or younger. * select(lfp = c.exper##c.exper faminc educ kids5) * Then we model the regression of interest. We're interested in modeling * wage as a function of work experience, education, and whether you're in a city * Here, we don't include family income or number of kids, under the assumption that they * do not belong in a wage model. These are our exclusion restrictions * (note these particular exclusion restrictions might be a little dubious! But hey, 1987.) * wage c.exper##c.exper educ city * Now we run our Heckman model! This is based on the ML approach heckman wage c.exper##c.exper educ city, select(lfp = c.exper##c.exper faminc educ kids5) * And this would be estimating the model using a two step-approach, also known as Heckit. heckman wage c.exper##c.exper educ city, select(lfp = c.exper##c.exper faminc educ kids5) two . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#stata"
  },"459": {
    "doc": "Histograms",
    "title": "Histograms",
    "content": "Histograms are an indespensible tool of research across disciplines. They offer a helpful way to represent the distribution of a variable of interest. Specifically, their function is to record how frequently data values fall within pre-specified ranges called “bins.” Such visual representations can help researchers easily detect whether their data are distributed in a skewed or symmetric way, and can help detect outliers. Despite being such a popular tool for scientific research, choosing the bin width (alternatively, number of bins) is ultimately a choice by the researcher. Histograms are intended to convey information about the variable, and choosing the “right” bin size to convey the information helpfully can be something of an art. The relationship between bin width \\(h\\) and the number of bins \\(k\\) is given by: . \\[k = \\frac{ \\max x - \\min x}{h}\\] For this reason, statistical softwares such as R and Stata will often accept either custom bin width specifications, or a number of bins. ",
    "url": "/Presentation/Figures/histograms.html",
    
    "relUrl": "/Presentation/Figures/histograms.html"
  },"460": {
    "doc": "Histograms",
    "title": "Histogram vs. bar graph",
    "content": "Because histograms represent data frequency using rectangular bars, they might be mistaken for bar graphs at first glance. Whereas bar graphs (sometimes called bar charts) plot values for categorical data, histograms represent the distribution of continuous variables such as income, height, weight, etc. ",
    "url": "/Presentation/Figures/histograms.html#histogram-vs-bar-graph",
    
    "relUrl": "/Presentation/Figures/histograms.html#histogram-vs-bar-graph"
  },"461": {
    "doc": "Histograms",
    "title": "Implementations",
    "content": "When feeding data to visualise using a histogram, one will notice that both R and Stata will attempt to “guess” what the “best” bin width/number of bins are. These may be overridden by user commands, as we will see. ",
    "url": "/Presentation/Figures/histograms.html#implementations",
    
    "relUrl": "/Presentation/Figures/histograms.html#implementations"
  },"462": {
    "doc": "Histograms",
    "title": "Python",
    "content": "There are many plotting libraries in Python, including declarative (say what you want) and imperative (build what you want) options. In the example below, we’ll explore several different options for plotting histogram data. By far the quickest way to plot a histogram is to use data analysis package pandas’ built-in bar chart option (which uses plotting library matplotlib). import pandas as pd df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/PSID.csv\", index_col=0) df['earnings'].plot.hist() . Note that, by default, the hist function shows counts on the y-axis. We can make this plot a bit more appealing by calling on the customisation features from matplotlib. Calling df['column-name'].plot.hist() returns a matplotlib ax object that accepts further customisation. We will use this and a style to make the plot more appealing, while also switching to show a density rather than counts, setting the number of bins explicitly, and using a logarithmic scale. import matplotlib.pyplot as plt plt.style.use('seaborn') ax = df['earnings'].plot.hist(density=True, log=True, bins=80) ax.set_title('Earnings in the PSID', loc='left') ax.set_ylabel('Density') ax.set_xlabel('Earnings'); . An alternative to the matplotlib-pandas combination is seaborn, a declarative plotting library. Using seaborn, we can quickly compared histograms of different cuts of the data. In the example below, a binary variable that sorts individuals into two groups based on age is added and used as the ‘hue’ of the keyword argument. import seaborn as sns age_cut_off = 45 df[f'Older than {age_cut_off}'] = df['age']&gt;age_cut_off ax = sns.histplot(df, x=\"earnings\", hue=f\"Older than {age_cut_off}\", element=\"step\", stat=\"density\") ax.set_yscale('log') . Finally, let’s look at a different declarative library, plotnine, which is inspired by (and has very similar syntax to) R’s plotting library ggplot. from plotnine import ggplot, aes, geom_histogram ( ggplot(df, aes(x='earnings', y='stat(density)') ) + geom_histogram(bins=80) ) . ",
    "url": "/Presentation/Figures/histograms.html#python",
    
    "relUrl": "/Presentation/Figures/histograms.html#python"
  },"463": {
    "doc": "Histograms",
    "title": "R",
    "content": "Histograms can be represented using base R, or more elegantly with ggplot. R comes with a built-in state.x77 dataset containing per-capita income in the US states for the year 1974, which we will be using. # loading the data incomes = data.frame(income = state.x77[,'Income']) # first using base R hist(incomes$income) # now using ggplot library(ggplot2) ggplot(data = incomes) + geom_histogram(aes(x = income)) # showing how we can adjust number of bins... ggplot(data = incomes) + geom_histogram(aes(x = income), bins = 15) # ...or the width of each bin ggplot(data = incomes) + geom_histogram(aes(x = income), binwidth = 500) . ",
    "url": "/Presentation/Figures/histograms.html#r",
    
    "relUrl": "/Presentation/Figures/histograms.html#r"
  },"464": {
    "doc": "Histograms",
    "title": "Stata",
    "content": "To illustrate the basic histogram function in Stata we will use the “auto” dataset. ** loading the data webuse auto * histogram with default bin width * The frequency option puts a count of observations on the y-axis * rather than a proportion histogram mpg, frequency * we can adjust the number of bins... histogram mpg, bin(15) frequency * ...or the bin width hist mpg, width(2) frequency . ",
    "url": "/Presentation/Figures/histograms.html#stata",
    
    "relUrl": "/Presentation/Figures/histograms.html#stata"
  },"465": {
    "doc": "Import a Foreign Data File",
    "title": "Import a Foreign Data File",
    "content": "Commonly, data will be distributed in a format that is not native to the software that you are using, such as Excel. How can you import it? . This page is specifically about importing data files from formats specific to particular foreign software. For importing standard shared formats, see Import a Delimited Data File (CSV, TSV) or Import a Fixed-Width Data File. ",
    "url": "/Other/import_a_foreign_data_file.html",
    
    "relUrl": "/Other/import_a_foreign_data_file.html"
  },"466": {
    "doc": "Import a Foreign Data File",
    "title": "Keep in Mind",
    "content": ". | Check your data after it’s imported to make sure it worked properly. Sometimes special characters will have trouble converting, or variable name formats are inconsistent, and so on. It never hurts to check! | Before doing this you will probably find it useful to Set a Working Directory | . ",
    "url": "/Other/import_a_foreign_data_file.html#keep-in-mind",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#keep-in-mind"
  },"467": {
    "doc": "Import a Foreign Data File",
    "title": "Also Consider",
    "content": ". | Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | . ",
    "url": "/Other/import_a_foreign_data_file.html#also-consider",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#also-consider"
  },"468": {
    "doc": "Import a Foreign Data File",
    "title": "Implementations",
    "content": "Because there are so many potential foreign formats, these implementations will be more about listing the appropriate commands with example syntax than providing full working examples. Make sure that you fill in the proper filename. The filename should include a filepath, or you should Set a Working Directory. ",
    "url": "/Other/import_a_foreign_data_file.html#implementations",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#implementations"
  },"469": {
    "doc": "Import a Foreign Data File",
    "title": "Julia",
    "content": "Julia ecosystem features many packages for working with various file formats. Here we’ll consider . | Arrow.jl | Avro.jl | Parquet2.jl | XLSX.jl | . # Uncomment if you want to install packages programmatically # using Pkg # We'll load all the data into DataFrames for uniform processing using DataFrames # Apache Arrow # To install the package # Pkg.add(\"Arrow\") using Arrow df = DataFrame(Arrow.Table(\"filename.arrow\")) # load (mmap) data and convert it to a DataFrame for analysis # Apache Avro # To install the package # Pkg.add(\"Avro\") using Avro df = DataFrame(Avro.readtable(\"filename.avro\")) # load data and convert it to a DataFrame for analysis # Apache Parquet # To install the package # Pkg.add(\"Parquet2\") using Parquet2 df = DataFrame(Parquet2.Dataset(\"filename.parq\"); copycols=false) # load data and convert it to a DataFrame for analysis # Apache Parquet # To install the package # Pkg.add(\"XLSX\") using XLSX # load data from the specified sheet in the file and convert it to a DataFrame for analysis df = DataFrame(XLSX.readtable(\"filename.xlsx\", \"mysheet\")) . ",
    "url": "/Other/import_a_foreign_data_file.html#julia",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#julia"
  },"470": {
    "doc": "Import a Foreign Data File",
    "title": "Python",
    "content": "You’ll most often be relying on Pandas to read in data. Though many other forms exist, the reason you’ll be pulling in data is usually to work with the data, transform, and manipulate it. Panda lends itself extremely well for this purpose. Sometime you may have to work with much more messy data with APIs where you’ll navigate through hierarchies of dictionaries using the .keys() method and selecting levels, but that is handled on a case-by-case basis and impossible to cover here. However, some of the most common will be covered. Those are csv, excel (xlsx), and .RData files. You, of course, always have the default open() function, but that can get much more complex. # Reading .RData files import pyreadr rds_data = pyreadr.read_r('sales_data.Rdata') #Object is a dictionary #Sales is the name of the dataframe, if unnamed, you may have to pass \"None\" as the name (no quotes) df_r = rds_data['sales'] df_r.head() # Other common file reads, all use pandas. Most common two shown (csv/xlsx) import pandas as pd csv_file = pd.read_csv('filename.csv') xlsx_file = pd.read_excel('filename.xlsx', sheet_name='Sheet1') #Pandas can also read html, jsons, etc.... ",
    "url": "/Other/import_a_foreign_data_file.html#python",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#python"
  },"471": {
    "doc": "Import a Foreign Data File",
    "title": "R",
    "content": "# Generally, you may use the rio package to import any tabular data type to be read in fluently without requiring a specification of the file type. library(rio) data &lt;- import('filename.xlsx') data &lt;- import('filename.dta') data &lt;- import('filename.sav') library(readxl) data &lt;- read_excel('filename.xlsx') # Read Stata, SAS, and SPSS files with the haven package # install.packages('haven') library(haven) data &lt;- read_stata('filename.dta') data &lt;- read_spss('filename.sav') # read_sas also supports .sas7bcat, or read_xpt supports transport files data &lt;- read_sas('filename.sas7bdat') # Read lots of other types with the foreign package # install.packages('foreign') library(foreign) data &lt;- read.arff('filename.arff') data &lt;- read.dbf('filename.dbf') data &lt;- read.epiinfo('filename.epiinfo') data &lt;- read.mtb('filename.mtb') data &lt;- read.octave('filename.octave') data &lt;- read.S('filename.S') data &lt;- read.systat('filename.systat') . ",
    "url": "/Other/import_a_foreign_data_file.html#r",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#r"
  },"472": {
    "doc": "Import a Foreign Data File",
    "title": "Stata",
    "content": "Stata can import foreign files using the File -&gt; Import menu. Alternately, you can use the import command: . import type using filename . where type can be excel, spss, sas, haver, or dbase (import can also be used to download data directly from sources like FRED). ",
    "url": "/Other/import_a_foreign_data_file.html#stata",
    
    "relUrl": "/Other/import_a_foreign_data_file.html#stata"
  },"473": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Import a Delimited Data File (CSV, TSV)",
    "content": "Often, data is stored in delimited files. In delimited files, each record has its own line, but the columns or variables are separated by a character, or delimiter. The most common delimiter used is a comma. As a result, you will often encounter comma-delimited files by their more common name, comma-separated values or CSV files. Importing these files is often the first step of any data analysis project, so we show you how to import CSVs (and other delimited files) below. ",
    "url": "/Other/importing_delimited_files.html",
    
    "relUrl": "/Other/importing_delimited_files.html"
  },"474": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Keep in Mind",
    "content": ". | Sometimes delimiting characters also appear in strings in the data - this can cause your program to read the data improperly since it assumes that a new column is starting every time it sees that character. Good data stewards won’t let this happen, but when it does happen it can be a real headache. Be on the lookout for that if your data seems to be reading in improperly. | When starting out, it can be confusing to know that you are working with a CSV file because you can open CSVs in Excel and they look like normal spreadsheets. Because many software packages have different procedures for importing CSVs and Excel workbooks, the ability to open CSVs in Excel (and the fact that they often appear in your GUI with an Excel icon next to them because that is the default program used to open them) often leads users to want to use the import commands appropriate for Excel. Don’t be caught up by this pitfall; the failsafe way to look at the extension connected with your file name. CSV files will have a .csv extension, while Excel files end in .xls or .xlsx | Other common delimiters include tabs (TSV) and pipes: |. | . ",
    "url": "/Other/importing_delimited_files.html#keep-in-mind",
    
    "relUrl": "/Other/importing_delimited_files.html#keep-in-mind"
  },"475": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Also Consider",
    "content": ". | Before doing this you will probably find it useful to Set a Working Directory | Import a foreign data file | Import a fixed-width data file | . ",
    "url": "/Other/importing_delimited_files.html#also-consider",
    
    "relUrl": "/Other/importing_delimited_files.html#also-consider"
  },"476": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Implementations",
    "content": " ",
    "url": "/Other/importing_delimited_files.html#implementations",
    
    "relUrl": "/Other/importing_delimited_files.html#implementations"
  },"477": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Julia",
    "content": "import Pkg; Pkg.add(\"CSV\") # This line and the next add the packages CSV and DataFrames to your Julia installation Pkg.add(\"DataFrames\") # They need to only be run once and not at all if you have previously installed the packages # Initialize the CSV and DataFrames packages (import also works in place of using, to make the analogy to Python's import more direct) using CSV, DataFrames # Import a CSV File from your local computer, if Scorecard.csv is in your working directory df = CSV.read(\"Scorecard.csv\", DataFrame) # Note, the DataFrame argument tells Julia to read the dataset into a DataFrame object # Read a CSV File from the web using HTTP # Bring in Julia's HTTP package to pull from the web df_web = CSV.read(HTTP.get(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\").body, DataFrame) . ",
    "url": "/Other/importing_delimited_files.html#julia",
    
    "relUrl": "/Other/importing_delimited_files.html#julia"
  },"478": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Python",
    "content": "The approach in Python uses pandas’s read_csv function and looks quite similar to Julia’s syntax. # Import a CSV File from your local machine df = pd.read_csv(\"Scorecard.csv\") # Import a CSV File from the web import pandas as pd # Make pandas available to your Python session df = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\") . ",
    "url": "/Other/importing_delimited_files.html#python",
    
    "relUrl": "/Other/importing_delimited_files.html#python"
  },"479": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "R",
    "content": "# Import a CSV file with the base-R (utils package) read.csv function df &lt;- read.csv('Scorecard.csv') # If you are working in the tidyverse, there is the improved read_csv library(tidyverse) df &lt;- read_csv('Scorecard.csv') # The fastest way to read in large CSV files is fread() in the data.table package library(data.table) df &lt;- fread('Scorecard.csv') # In each of these cases you can open a CSV on the internet by just putting the URL in place of the file path df &lt;- read.csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') df &lt;- read_csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') df &lt;- fread('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') . ",
    "url": "/Other/importing_delimited_files.html#r",
    
    "relUrl": "/Other/importing_delimited_files.html#r"
  },"480": {
    "doc": "Import a Delimited Data File (CSV, TSV)",
    "title": "Stata",
    "content": "* Import a CSV File from your local machine import delimited Scorecard.csv, clear * Note that the \", clear\" option on all Stata import commands clears any data in memory before importing the dataset * Import a CSV File from the web import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\", clear . ",
    "url": "/Other/importing_delimited_files.html#stata",
    
    "relUrl": "/Other/importing_delimited_files.html#stata"
  },"481": {
    "doc": "Instrumental Variables",
    "title": "Instrumental Variables",
    "content": "In the regression model . \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] where \\(\\epsilon\\) is an error term, the estimated \\(\\hat{\\beta}_1\\) will not give the causal effect of \\(X\\) on \\(Y\\) if \\(X\\) is endogenous - that is, if \\(X\\) is related to \\(\\epsilon\\) and so determined by forces within the model (endogenous). One way to recover the causal effect of \\(X\\) on \\(Y\\) is to use instrumental variables. If there exists a variable \\(Z\\) that is related to \\(X\\) but is completely unrelated to \\(\\epsilon\\) (perhaps after adding some controls), then you can use instrumental variables estimation to isolate only the part of the variation in \\(X\\) that is explained by \\(Z\\). Naturally, then, this part of the variation is unrelated to \\(\\epsilon\\) because \\(Z\\) is unrelated to \\(\\epsilon\\), and you can get the causal effect of that part of \\(X\\). For more information, see Wikipedia: Instrumental variables estimation. ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html"
  },"482": {
    "doc": "Instrumental Variables",
    "title": "Keep in Mind",
    "content": ". | Technically, all the variables in the model except for the dependent variable and the endogenous variables are “instruments”, including controls. However, it is also common to refer to only the excluded instruments (i.e., variables that are only used to predict the endogenous variable, not the dependent variable) as instruments. This page will follow that convention. | For instrumental variables to work, it must be the case that the instrument is only related to the outcome variable through other variables already included in the model like the endogenous variables or the controls. This is called the “validity” assumption and it cannot be verified in the data, only theoretically. Give serious consideration as to whether validity applies to your instrument before using instrumental variables. | You can check for the relevance of your instrument, which is how strongly related it is to your endogenous variable. A rule of thumb is that an joint F-test of the instruments should be at least 10, but this is only a rule of thumb, and imprecise (see Stock and Yogo 2005 for a more precise version of this test). In general, if the instruments are not very strong predictors of the endogenous variables, you should consider whether your analysis fits the assumptions necessary to run a weak-instrument-robust estimation method. See Hahn &amp; Hausman 2003 for an overview. | Instrumental variables estimates a local average treatment effect - in other words, a weighted average of each individual observation’s treatment effect, where the weights are based on the strength of the effect of the instrument on the endogenous variable. Note both that this is not the same thing as an average treatment effect, which is an average of each individual’s treatment effect, which is usually what is desired, and also that if the instrumental variable has effects of different signs for different people (non-monotonicity), then the estimate isn’t really anything of interest. Be sure that monotonicity makes sense in your context before using instrumental variables. | Instrumental variables is a consistent estimator of a causal effect, but it is biased in finite samples. Be wary of using instrumental variables in small samples. | . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#keep-in-mind"
  },"483": {
    "doc": "Instrumental Variables",
    "title": "Also Consider",
    "content": ". | Instrumental variables methods generally rely on linearity assumptions, and if your dependent or endogenous variables are not continuous, their assumptions may not hold. Consider methods specially designed for nonlinear instrumental variables estimation. | There are many ways to estimate instrumental variables, not just two stage least squares. Different estimators such as GMM or k-class limited-information maximum likelihood estimators perform better or worse depending on heterogeneous treatment effects, heteroskedasticity, and sample size. Many instrumental variables estimation commands allow for multiple different estimation methods, described below. Note that in the just-identified case (where the number of instruments is the same as the number of endogenous variables), several common estimators produce identical results. | . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#also-consider",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#also-consider"
  },"484": {
    "doc": "Instrumental Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#implementations",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#implementations"
  },"485": {
    "doc": "Instrumental Variables",
    "title": "Python",
    "content": "The easiest way to run instrument variables regressions in Python is probably the linearmodels package, although there are other packages available. # Conda install linearmodels, pandas, and numpy, if you don't have them already from linearmodels.iv import IV2SLS import pandas as pd import numpy as np df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/AER/CigarettesSW.csv', index_col=0) # We will use cigarette taxes as an instrument for cigarette prices # to evaluate the effect of cigarette price on log number of packs smoked # With income per capita as a control # Adjust everything for inflation df['rprice'] = df['price']/df['cpi'] df['rincome'] = df['income']/df['population']/df['cpi'] df['tdiff'] = (df['taxs'] - df['tax'])/df['cpi'] # Specify formula in format of 'y ~ exog + [endog ~ instruments]'. # The '1' on the right-hand side of the formula adds a constant. formula = 'np.log(packs) ~ 1 + np.log(rincome) + [np.log(rprice) ~ tdiff]' # Specify model and data mod = IV2SLS.from_formula(formula, df) # Fit model res = mod.fit() # Show model summary res.summary . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#python",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#python"
  },"486": {
    "doc": "Instrumental Variables",
    "title": "R",
    "content": "There are several ways to run instrumental variables in R. Here we will cover two - AER::ivreg(), which is probably the most common, and lfe::felm(), which is more flexible and powerful. You may also want to consider looking at estimatr::iv_robust, which combines much of the flexibility of lfe::felm() with the simple syntax of AER::ivreg(), although it is not as powerful. # If necessary, install both packages. # install.packages(c('AER','lfe')) # Load AER library(AER) # Load the Cigarettes data from ivreg, following the example data(CigarettesSW) # We will be using cigarette taxes as an instrument for cigarette prices # to evaluate the effect of cigarette price on log number of packs smoked # With income per capita as a control # Adjust everything for inflation CigarettesSW$rprice &lt;- CigarettesSW$price/CigarettesSW$cpi CigarettesSW$rincome &lt;- CigarettesSW$income/CigarettesSW$population/CigarettesSW$cpi CigarettesSW$tdiff &lt;- (CigarettesSW$taxs - CigarettesSW$tax)/CigarettesSW$cpi # The regression formula takes the format # dependent.variable ~ endogenous.variables + controls | instrumental.variables + controls ivmodel &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | tdiff + log(rincome), data = CigarettesSW) summary(ivmodel) # Now we will run the same model with lfe::felm library(lfe) # The regression formula takes the format # dependent vairable ~ # controls | # fixed.effects | # (endogenous.variables ~ instruments) | # clusters.for.standard.errors # So if need be it is straightforward to adjust this example to account for # fixed effects and clustering. # Note the 0 indicating no fixed effects ivmodel2 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW) summary(ivmodel2) # felm can also use several k-class estimation methods; see help(felm) for the full list. # Let's run it with a limited-information maximum likelihood estimator with # the fuller adjustment set to minimize squared error (4). ivmodel3 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW, kclass = 'liml', fuller = 4) summary(ivmodel3) . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#r",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#r"
  },"487": {
    "doc": "Instrumental Variables",
    "title": "Stata",
    "content": "Instrumental variables estimation in Stata typically uses the built-in ivregress command. This command can be used to implement linear instrumental variables regression using two-stage least squares, GMM, or LIML . * Get Stock and Watson Cigarette data import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/Zelig/CigarettesSW.csv\", clear * Adjust everything for inflation g rprice = price/cpi g rincome = (income/population)/cpi g tdiff = (taxs - tax)/cpi * And take logs g lpacks = ln(packs) g lrincome = ln(rincome) g lrprice = ln(rprice) * The syntax for the regression is * name_of_estimator dependent_variable controls (endogenous_variable = instruments) * where name_of_estimator can be two stage least squares (2sls), * limited information maximum likelihood (liml, note that ivregress doesn't support k-class estimators), * or generalized method of moments (gmm) * Here we can run two stage least squares ivregress 2sls lpacks rincome (lrprice = tdiff) * Or gmm. ivregress gmm lpacks rincome (lrprice = tdiff) . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#stata",
    
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#stata"
  },"488": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Interaction Terms and Polynomials",
    "content": "Regression models generally assume that the outcome variable is a function of an index, which is a linear function of the independent variables, for example in ordinary least squares: . \\[Y = \\beta_0+\\beta_1X_1+\\beta_2X_2\\] However, if the independent variables have a nonlinear effect on the outcome, the model will be incorrectly specified. This is fine as long as that nonlinearity is modeled by including those nonlinear terms in the index. The two most common ways this occurs is by including interactions or polynomial terms. With an interaction, the effect of one variable varies according to the value of another: . \\[Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_1X_2\\] and with polynomial terms, the effect of one variable one the outcome is allowed to take a non-linear shape: . \\[Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_2^2 + \\beta_4X_2^3\\] ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html"
  },"489": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Keep in Mind",
    "content": ". | When you have interaction terms or polynomials, the effect of a variable can no longer be described with a single coefficient, and in some senses the individual coefficients lose meaning without the others. You can understand the effect of a single variable by taking the derivative of the index with respect to that variable. For example, in \\(Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_1X_2\\), the effect of \\(X_2\\) on \\(Y\\) is \\(\\partial Y/\\partial X_2 = \\beta_2 + \\beta_3X_1\\). You must plug in the value of \\(X_1\\) to get the effect of \\(X_2\\). Or in \\(Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_2^2 + \\beta_4X_2^3\\), the effect of \\(X_2\\) is \\(\\partial Y/\\partial X_2 = \\beta_2 + 2\\beta_3X_2 + 3\\beta_4X_2^2\\). You must plug in a value of \\(X_2\\) to get the marginal effect of \\(X_2\\) at that value. | In almost all cases, if you are including an interaction term, you should also include each of the interacted variables on their own. Otherwise, the coefficients become very difficult to interpret. | In almost all cases, if you are including a polynomial, you should include all terms of the polynomial. In other words, include the linear and squared term, not just the squared term. | . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#keep-in-mind"
  },"490": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Also Consider",
    "content": ". | Interaction terms tend to have low statistical power. Consider performing a power analysis of interaction terms before running your analysis. | Polynomials are not the only way to model a nonlinear relationship. You could, for example, run one of many kinds of nonparametric regression. | You may want to get the average marginal effects or the marginal effects at the mean of your variables after running your model. | One common way to display the effects of a model with interactions is to graph them. See marginal effects plots for interactions with continuous variables and Marginal effects plots for interactions with continuous variables | . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#also-consider",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#also-consider"
  },"491": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#implementations",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#implementations"
  },"492": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Julia",
    "content": "Thanks to StatsModels.jl and GLM packages from the JuliaStats project we can match R and Python code very closely. using StatsModels, GLM, DataFrames, CSV # Load the R mtcars dataset from a URL mtcars = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Data/mtcars.csv\"), DataFrame) # Here we specify a model with linear, quadratic and cubic `hp` terms. # We can use any Julia functions and operators, including user-defined ones, # in a `@formula` expression. # We also specify `dropcollinear=false` otherwise `lm` function will drop # the intercept during fitting, as soon as the model's terms are not linearly # independent. That's a dubious thing to have in a presumably linear model, # but here we show only how to write down a particular model, and not what model # is the right one for the given data. :) model1 = lm(@formula(mpg ~ hp + hp^2 + hp^3 + cyl), mtcars, dropcollinear=false) print(model1) # Include an interaction term and the variables by themselves using `*` # The interaction term is represented by hp:cyl model2 = lm(@formula(mpg ~ hp * cyl), mtcars) print(model2) # Include only the interaction term and not the variables themselves with `&amp;` # Hard to interpret! Occasionally useful though. model3 = lm(@formula(mpg ~ hp&amp;cyl), mtcars) print(model3) . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#julia",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#julia"
  },"493": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Python",
    "content": "Using the statsmodels package, we can use a similar formulation as the R example below. # Standard imports import numpy as np import pandas as pd import statsmodels.formula.api as sms from matplotlib import pyplot as plt # Load the R mtcars dataset from a URL df = pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv') # Include a linear, squared, and cubic term using the I() function. # N.B. Python uses ** for exponentiation (^ means bitwise xor) model1 = sms.ols('mpg ~ hp + I(hp**2) + I(hp**3) + cyl', data=df) print(model1.fit().summary()) # Include an interaction term and the variables by themselves using * # The interaction term is represented by hp:cyl model2 = sms.ols('mpg ~ hp * cyl', data=df) print(model2.fit().summary()) # Equivalently, you can request \"all quadratic interaction terms\" by doing model3 = sms.ols('mpg ~ (hp + cyl) ** 2', data=df) print(model3.fit().summary()) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 = sms.ols('mpg ~ hp : cyl', data=df) print(model4.fit().summary()) . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#python",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#python"
  },"494": {
    "doc": "Interaction Terms and Polynomials",
    "title": "R",
    "content": "# Load mtcars data data(mtcars) # Include a linear, squared, and cubic term using the I() function model1 &lt;- lm(mpg ~ hp + I(hp^2) + I(hp^3) + cyl, data = mtcars) # Include a linear, squared, and cubic term using the poly() function # The raw = TRUE option will give the exact same result as model1 # Omitting this will give you orthogonal polynomial terms, # which are not correlated with each other but are more difficult to interpret model2 &lt;- lm(mpg ~ poly(hp, 3, raw = TRUE) + cyl, data = mtcars) # Include an interaction term and the variables by themselves using * model3 &lt;- lm(mpg ~ hp*cyl, data = mtcars) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 &lt;- lm(mpg ~ hp:cyl, data = mtcars) . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#r",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#r"
  },"495": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Stata",
    "content": "Stata allows interaction and polynomial terms using hashtags ## to join together variables to make interactions, or joining a variable with itself to get a polynomial. You must also specify whether each variable is continuous (prefix the variable with c.) or a factor (prefix with i.). * Load auto data sysuse auto.dta, clear * Use ## to interact variables together and also include the variables individually * foreign is a factor variable so we prefix it with i. * weight is continuous so we prefix it with c. reg mpg c.weight##i.foreign * Use # to include just the interaction term and not the variables themselves * If one is a factor, this will include the effect of the continuous variable * For each level of the factor reg mpg c.weight#i.foreign * Interact a variable with itself to create a polynomial term reg mpg c.weight##c.weight##c.weight foreign . It is also possible to use other type of functions and obtain correct marginal effects. For example: Say that you want to estimate the model: . \\[y = a_0 + a_1 * x + a_2 * 1/x + e\\] and you want to estimate the marginal effects with respect to \\(x\\). You can do this as follows: . * requires package f_able ssc install f_able * Load auto data sysuse auto.dta, clear * create function using \"fgen\" fgen _1_price = 1/price reg mpg _1_price price * indicates which variable is a \"constructed\" variable f_able _1_price, auto * estimate marginal effects margins, dydx(price) * How do you know it works? Use NL to verify nl (mpg = {a0} + {a1} * price + {a2}*1/price), var(price) margins, dydx(price) . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#stata",
    
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#stata"
  },"496": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Line Graph with Labels at the Beginning or End of Lines",
    "content": "A line graph is a common way of showing how a value changes over time (or over any other x-axis where there’s only one observation per x-axis value). It is also common to put several line graphs on the same set of axes so you can see how multiple values are changing together. When putting multiple line graphs on the same set of axes, a good idea is to label the different lines on the lines themselves, rather than in a legend, which generally makes things easier to read. ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html"
  },"497": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Keep in Mind",
    "content": ". | Check the resulting graph to make sure that labels are legible, visible in the graph area, and don’t overlap. | . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#keep-in-mind"
  },"498": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Also Consider",
    "content": ". | More generally, see Line graph and Styling line graphs. In particular, consider Styling line graphs in order to distinguish the lines by color, pattern, etc. in addition to labels | If there are too many lines to be able to clearly follow them, labels won’t help too much. Instead, consider Faceted graphs. | . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#also-consider",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#also-consider"
  },"499": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#implementations",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#implementations"
  },"500": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Python",
    "content": "There isn’t a quick, declarative way to add text labels to lines with the most popular libraries. So, in the example below, we’ll add labels to lines using the imperative (build what you want) tools of plotting library matplotlib, creating the lines themselves with declarative plotting library seaborn. You may need to install the packages using pip install packagename or conda install packagename before you begin. import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np import matplotlib.dates as mdates # Read in the data df = pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv', parse_dates=['date']) # Create the column we wish to plot title = 'Log of Google Trends Index' df[title] = np.log(df['hits']) # Set a style for the plot plt.style.use('ggplot') # Make a plot fig, ax = plt.subplots() # Add lines to it sns.lineplot(ax=ax, data=df, x=\"date\", y=title, hue=\"name\", legend=None) # Add the text--for each line, find the end, annotate it with a label, and # adjust the chart axes so that everything fits on. for line, name in zip(ax.lines, df.columns.tolist()): y = line.get_ydata()[-1] x = line.get_xdata()[-1] if not np.isfinite(y): y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float(\"nan\")) if not np.isfinite(y) or not np.isfinite(x): continue text = ax.annotate(name, xy=(x, y), xytext=(0, 0), color=line.get_color(), xycoords=(ax.get_xaxis_transform(), ax.get_yaxis_transform()), textcoords=\"offset points\") text_width = (text.get_window_extent( fig.canvas.get_renderer()).transformed(ax.transData.inverted()).width) if np.isfinite(text_width): ax.set_xlim(ax.get_xlim()[0], text.xy[0] + text_width * 1.05) # Format the date axis to be prettier. ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d')) ax.xaxis.set_minor_locator(mdates.DayLocator()) ax.xaxis.set_major_locator(mdates.AutoDateLocator(interval_multiples=False)) plt.tight_layout() plt.show() . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#python",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#python"
  },"501": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "R",
    "content": "# If necessary, install ggplot2, lubridate, and directlabels # install.packages(c('ggplot2','directlabels', 'lubridate')) library(ggplot2) library(directlabels) # Load in Google Trends Nobel Search Data # Which contains the Google Trends global search popularity index for the four # research-based Nobel prizes over a month. df &lt;- read.csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv') # Properly treat our date variable as a date # Not necessary in all applications of this technique. df$date &lt;- lubridate::ymd(df$date) # Construct our standard ggplot line graph # Drawing separate lines by name # And using the log of hits for visibility ggplot(df, aes(x = date, y = log(hits), color = name)) + labs(x = \"Date\", y = \"Log of Google Trends Index\")+ geom_line()+ # Since we are about to add line labels, we don't need a legend theme(legend.position = \"none\") + # Add, from the directlabels package, # geom_dl, using method = 'last.bumpup' to put the # labels at the end, and make sure that if they intersect, # one is bumped up geom_dl(aes(label = name), method = 'last.bumpup') + # Extend the x axis so the labels are visible - # Try the graph a few times until you find a range that works scale_x_date(limits = c(min(df$date), lubridate::ymd('2019-10-25'))) . This results in: . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#r",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#r"
  },"502": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Stata",
    "content": "Unfortunately, performing this technique in Stata requires placing each text() label on the graph. However, this can be automated with the use of a for loop to build the code using locals. * Load in Google Trends Nobel Search Data * Which contains the Google Trends global search popularity index for the four * research-based Nobel prizes over a month. import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv\", clear * Convert the date variable to an actual date * (not necessary in all implementations) g ymddate = date(date, \"YMD\") * Format the new variable as a date so we see it properly on the x-axis format ymddate %td * Graph log(hits) for visibility g loghits = log(hits) * Get the different prize types to graph levelsof name, l(names) * Figure out the last time period in the data set quietly summarize ymddate local lastday = r(max) * Start constructing a local that contains all the line graphs to graph local lines * Start constructing a local that contains the text labels to add local textlabs * Loop through each one foreach n in `names' { * Add in the line graph code * by building on the local we already have (`lines') and adding a new twoway segment local lines `lines' (line loghits ymddate if name == \"`n'\") * Figure out the value this line hits on the last point on the graph quietly summ loghits if name == \"`n'\" &amp; ymddate == `lastday' * The text command takes the y-value (from the mean we just took) * the x-value (the last day on the graph), * and the text label (the name we are working with) * Plus place(r) to put it to the RIGHT of that point local textlabs `textlabs' text(`r(mean)' `lastday' \"`n'\", place(r)) } * Finally, graph our lines * with the twoway lines we've specified, followed by the text labels * We're sure to remove the legend with legend(off) * and extend the x-axis so we can see the labels with xscale(range()) quietly summarize ymddate local start = r(min) local end = r(max) + 5 twoway `lines', `textlabs' legend(off) xscale(range(`start' `end')) xtitle(\"Date\") ytitle(\"Log of Google Trends Index\") . This results in: . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#stata",
    
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#stata"
  },"503": {
    "doc": "Line Graphs",
    "title": "Line Graphs",
    "content": "A line graph is a visualization tool that shows how a value changes over time. A line graph can contain a single line or multiple lines in order to compare how multiple different values change over time. ",
    "url": "/Presentation/Figures/line_graphs.html",
    
    "relUrl": "/Presentation/Figures/line_graphs.html"
  },"504": {
    "doc": "Line Graphs",
    "title": "Keep in Mind",
    "content": ". | Keep things simple. With line graphs, more is not always better. It’s important that line graphs are kept clean and concise so that they can be interpreted quickly and easily. Including too many lines or axis tick marks can make your graph messy and difficult to read. | The time variable should be on the x-axis for straightforward interpretation. | . ",
    "url": "/Presentation/Figures/line_graphs.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/line_graphs.html#keep-in-mind"
  },"505": {
    "doc": "Line Graphs",
    "title": "Also Consider",
    "content": ". | To enhance a basic line graph, see Styling Line Graphs and Line Graph with Labels at the Beginning or End of Lines. | . ",
    "url": "/Presentation/Figures/line_graphs.html#also-consider",
    
    "relUrl": "/Presentation/Figures/line_graphs.html#also-consider"
  },"506": {
    "doc": "Line Graphs",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/line_graphs.html#implementations",
    
    "relUrl": "/Presentation/Figures/line_graphs.html#implementations"
  },"507": {
    "doc": "Line Graphs",
    "title": "Python",
    "content": "Here we will use seaborn.lineplot from the seaborn package, which builds on top of matplotlib. # Load packages import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load in data Orange = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/Orange.csv') # Specify a line plot in Seaborn using # age and circumference on the x and y axis # and picking just Tree 1 from the data sns.lineplot(x = 'age', y = 'circumference', data = Orange.loc[Orange.Tree == 1]) # And title the axes plt.xlabel('Age (days since 12/31/1968)') plt.ylabel('Circumference') . The result is: . If we want to include all the trees on the graph, with color to distinguish them, we add a hue argument: . # Add on a hue axis to add objects of different color by tree # So we can graph all the trees sns.lineplot(x = 'age', y = 'circumference', hue = 'Tree', data = Orange) # And title the axes plt.xlabel('Age (days since 12/31/1968)') plt.ylabel('Circumference') . Which results in: . ",
    "url": "/Presentation/Figures/line_graphs.html#python",
    
    "relUrl": "/Presentation/Figures/line_graphs.html#python"
  },"508": {
    "doc": "Line Graphs",
    "title": "R",
    "content": "Basic Line Graph in R . To make a line graph in R, we’ll be using a dataset that’s already built in to R, called ‘Orange’. This dataset tracks the growth in circumference of several trees as they age. library(dplyr) library(lubridate) library(ggplot2) #load in dataset data(Orange) . This dataset has measurements for four different trees. To start off, we’ll only be graphing the growth of Tree #1, so we first need to subset our data. #subset data to just tree #1 tree_1_df &lt;- Orange %&gt;% filter(Tree == 1) . Then we will construct our plot using ggplot(). We’ll create our line graph using the following steps: . | First, call ggplot() and specify the tree_1_df dataset. Next, we need to specify the aesthetics of our graph (what variables go where). Do so with the aes() function, setting x = age and y = circumference. | To make the actual line of the line graph, we will add the line geom_line() to our ggplot line using the + symbol. Using the + symbol allows us to add different lines of code to the same graph in order to create new elements within it. | Putting those steps together, we get the following code resulting in our first line graph: | . ggplot(tree_1_df, aes(x = age, y = circumference)) + geom_line() . This does show us how the tree grows over time, but it’s rather plain and lacks important identifying information like a title and units of measurements for the axes. In order to enhance our graph, we again use the + symbol to add additional elements like line color, titles etc. and to change things like axis labels and title/label position. | We can specify the color of our line within the geom_line() function. | The function labs() allows us to add a title and also change the labels for the axes | Using the function theme() allows us to manipulate the apperance of our labels through the element_text function | Let’s change the line color, add a title and center it, and also add more information to our axes labels. | . ggplot(tree_1_df, aes(x = age, y = circumference)) + geom_line(color = \"orange\") + labs(x = \"Age (days since 12/31/1968)\", y = \"Circumference (mm)\", title = \"Orange Tree Circumference Growth by Age\") + theme(plot.title = element_text(hjust = 0.5)) . Line Graph with Multiple Lines in R . A great way to employ line graphs is to compare the changes of different values over the same time period. For this instance, we’ll be looking at how the four trees differ in their growth over time. We will be employing the full Orange dataset for this graph. To add multiple lines using data from the same dataframe, simply add the color argument to the aes() function within our ggplot() line. Set the color argument to the identifying variable within your data set, here, that variable is Tree, so we will set color = Tree. ggplot(Orange, aes(x = age, y = circumference, color = Tree)) + geom_line() + labs(x = \"Age (days since 12/31/1968)\", y = \"Circumference (mm)\", title = \"Orange Tree Circumference Growth by Age\") + theme(plot.title = element_text(hjust = 0.5)) . The steps will get you started with creating graphs in R. For more information on styling your graphs, again, visit Styling Line Graphs and Line Graph with Labels at the Beginning or End of Lines. Another great resource for line graph styling tips is this blog post created by Jodie Burchell. ",
    "url": "/Presentation/Figures/line_graphs.html#r",
    
    "relUrl": "/Presentation/Figures/line_graphs.html#r"
  },"509": {
    "doc": "Line Graphs",
    "title": "Stata",
    "content": "We can create a line graph in Stata using the twoway function with the line setting. * Load data on orange trees import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/Orange.csv\", clear * Let's just graph the first tree using if Tree == 1 * We specify the y-axis variable circumference first followed by the x-axis variable age * We can add axis labels with xtitle and ytitle * And specify a color with lcolor (for line color) twoway line circumference age if tree == 1, xtitle(\"Age (days since 12/31/1968)\") ytitle(\"Circumference\") lcolor(red) . The result is: . We can also include all the trees on the same line graph: . * If we want all of our trees graphed on the same axis * We can specify each line separately using () * Use legend() so we know which line is which * Or label the lines directly using /Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html twoway (line circumference age if tree == 1) (line circumference age if tree == 2) (line circumference age if tree == 3) (line circumference age if tree == 4) (line circumference age if tree == 5), xtitle(\"Age (days since 12/31/1968)\") ytitle(\"Circumference\") legend(lab(1 \"Tree 1\") lab(2 \"Tree 2\") lab(3 \"Tree 3\") lab(4 \"Tree 4\") lab(5 \"Tree 5\")) . The result is: . ",
    "url": "/Presentation/Figures/line_graphs.html#stata",
    
    "relUrl": "/Presentation/Figures/line_graphs.html#stata"
  },"510": {
    "doc": "Linear Hypothesis Tests",
    "title": "Linear Hypothesis Tests",
    "content": "Most regression output will include the results of frequentist hypothesis tests comparing each coefficient to 0. However, in many cases, you may be interested in whether a linear sum of the coefficients is 0. For example, in the regression . \\[Outcome = \\beta_0 + \\beta_1\\times GoodThing + \\beta_2\\times BadThing\\] You may be interested to see if \\(GoodThing\\) and \\(BadThing\\) (both binary variables) cancel each other out. So you would want to do a test of \\(\\beta_1 - \\beta_2 = 0\\). Alternately, you may want to do a joint significance test of multiple linear hypotheses. For example, you may be interested in whether \\(\\beta_1\\) or \\(\\beta_2\\) are nonzero and so would want to jointly test the hypotheses \\(\\beta_1 = 0\\) and \\(\\beta_2=0\\) rather than doing them one at a time. Note the and here, since if either one or the other is rejected, we reject the null. ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html"
  },"511": {
    "doc": "Linear Hypothesis Tests",
    "title": "Keep in Mind",
    "content": ". | Be sure to carefully interpret the result. If you are doing a joint test, rejection means that at least one of your hypotheses can be rejected, not each of them. And you don’t necessarily know which ones can be rejected! | Generally, linear hypothesis tests are performed using F-statistics. However, there are alternate approaches such as likelihood tests or chi-squared tests. Be sure you know which on you’re getting. | Conceptually, what is going on with linear hypothesis tests is that they compare the model you’ve estimated against a more restrictive one that requires your restrictions (hypotheses) to be true. If the test you have in mind is too complex for the software to figure out on its own, you might be able to do it on your own by taking the sum of squared residuals in your original unrestricted model (\\(SSR_{UR}\\)), estimate the alternate model with the restriction in place (\\(SSR_R\\)) and then calculate the F-statistic for the joint test using \\(F_{q,n-k-1} = ((SSR_R - SSR_{UR})/q)/(SSR_{UR}/(n-k-1))\\). | . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#keep-in-mind"
  },"512": {
    "doc": "Linear Hypothesis Tests",
    "title": "Also Consider",
    "content": ". | The process for testing a nonlinear combination of your coefficients, for example testing if \\(\\beta_1\\times\\beta_2 = 1\\) or \\(\\sqrt{\\beta_1} = .5\\), is generally different. See Nonlinear hypothesis tests. | . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#also-consider",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#also-consider"
  },"513": {
    "doc": "Linear Hypothesis Tests",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#implementations",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#implementations"
  },"514": {
    "doc": "Linear Hypothesis Tests",
    "title": "R",
    "content": "Linear hypothesis test in R can be performed for most regression models using the linearHypothesis() function in the car package. See this guide for more information. # If necessary # install.packages('car') library(car) data(mtcars) # Run our model m1 &lt;- lm(mpg ~ hp + disp + am + wt, data = mtcars) # Test a linear combination of coefficients linearHypothesis(m1, c('hp + disp = 0')) # Test joint significance of multiple coefficients linearHypothesis(m1, c('hp = 0','disp = 0')) # Test joint significance of multiple linear combinations linearHypothesis(m1, c('hp + disp = 0','am + wt = 0')) . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#r",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#r"
  },"515": {
    "doc": "Linear Hypothesis Tests",
    "title": "Stata",
    "content": "Tests of coefficients in Stata can generally be performed using the built-in test command. * Load data sysuse auto.dta reg mpg headroom trunk prince rep78 * Make sure to run tests while the previous regression is still in memory * Test joint significance of multiple coefficients test headroom trunk * testparm does the same thing but allows wildcards to select coefficients * this will test the joint significance of every variable with an e in it testparm *e* * Test a linear combination of the coefficients test headroom + trunk = 0 * Test multiple linear combinations by accumulating them one at a time test headroom + trunk = 0 test price + rep78 = 0, accumulate . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#stata",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#stata"
  },"516": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Linear Mixed-Effects Regression",
    "content": "Mixed-effects regression goes by many names, including hierarchical linear model, random coefficient model, and random parameter models. In a mixed-effects regression, some of the parameters are “random effects” which are allowed to vary over the sample. Others are “fixed effects”, which are not. Note that this use of the term “fixed effects” is not the same as in fixed effects regression. For example, consider the model . \\[y_{ij} = \\beta_{0j} + \\beta_{1j}X_{1ij} + \\beta_{2}X_{2ij} + e_{ij}\\] The intercept \\(\\beta_{0j}\\) has a \\(j\\) subscript and is allowed to vary over the sample at the \\(j\\) level, where \\(j\\) may indicate individual or group, depending on context. The slope on \\(X_{1ij}\\), \\(\\beta_{1j}\\), is similarly allowed to vary over the sample. These are random effects. \\(\\beta_{2}\\) is not allowed to vary over the sample and so is fixed. The random parameters have their own “level-two” equations, which may or may not include level-two covariates. \\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01}W_j + u_{0j}\\] \\[\\beta_{1j} = \\gamma_{10} + u_{1j}\\] For more information see Wikipedia. ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html"
  },"517": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Keep in Mind",
    "content": ". | The assumptions necessary to use a mixed-effects model in general are the same as for most linear models. However, in addition, mixed-effects models assume that the error terms at different levels are unrelated. | At the second level, statistical power depends on the number of different \\(j\\) values there are. Mixed-effects models may perform poorly if the coefficient is allowed to vary over only a few groups. | There’s no need to stop at two levels - the second-level coefficients can also be allowed to vary at a higher level. | . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#keep-in-mind"
  },"518": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Also Consider",
    "content": ". | There are many variations of mixed-effects models for working with non-linear data, see nonlinear mixed-effects models. | If the goal is making predictions within subgroups, you may want to consider multi-level regression with poststratification. | . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#also-consider",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#also-consider"
  },"519": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#implementations",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#implementations"
  },"520": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "R",
    "content": "One common way to fit mixed-effects models in R is with the lmer function in the lme4 package. To fit fully Bayesian models you may want to consider instead using STAN with the rstan package. See the multi-level regression with poststratification page for more information. # Install lme4 if necessary # install.packages('lme4') # Load up lme4 library(lme4) # Load up university instructor evaluations data from lme4 data(InstEval) # We'll be treating lecture age as a numeric variable InstEval$lectage &lt;- as.numeric(InstEval$lectage) # Let's look at the relationship between lecture ratings andhow long ago the lecture took place # with a control for whether the lecture was a service lecture ols &lt;- lm(y ~ lectage + service, data = InstEval) summary(ols) # Now we will use lmer to allow the intercept to vary at the department level me1 &lt;- lmer(y ~ lectage + service + (1 | dept), data = InstEval) summary(me1) # Now we will allow the slope on lectage to vary at the department level me2 &lt;- lmer(y ~ lectage + service + (-1 + lectage | dept), data = InstEval) summary(me2) # Now both the intercept and lectage slope will vary at the department level me3 &lt;- lmer(y ~ lectage + service + (lectage | dept), data = InstEval) summary(me3) . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#r",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#r"
  },"521": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Stata",
    "content": "Stata has a family of functions based around the mixed command that can estimate mixed-effects models. * Load NLS-W data sysuse nlsw88.dta, clear * We are going to estimate the relationship between hourly wage and job tenure * with a contorl for marital status * Without mixed effects reg wage tenure married * Now we will allow the intercept to vary with occupation mixed wage tenure married || occupation: * Next we will allow the slope on tenure to vary with occupation mixed wage tenure married || occupation: tenure, nocons * Now, both! mixed wage tenure married || occupation: tenure * Finally we will allow the intercept and tenure slope to vary over both occupation * and age mixed wage tenure married || occupation: tenure || age: tenure . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#stata",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#stata"
  },"522": {
    "doc": "Logit Model",
    "title": "Logit Regressions",
    "content": "A logistical regression (Logit) is a statistical method for a best-fit line between a binary [0/1] outcome variable \\(Y\\) and any number of independent variables. Logit regressions follow a logistical distribution and the predicted probabilities are bounded between 0 and 1. For more information about Logit, see Wikipedia: Logit. ",
    "url": "/Model_Estimation/GLS/logit_model.html#logit-regressions",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#logit-regressions"
  },"523": {
    "doc": "Logit Model",
    "title": "Keep in Mind",
    "content": ". | The beta coefficients from a logit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of \\(X\\) on \\(Y\\). | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the logit beta coefficient by 4. | . ",
    "url": "/Model_Estimation/GLS/logit_model.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#keep-in-mind"
  },"524": {
    "doc": "Logit Model",
    "title": "Also Consider",
    "content": ". | See Marginal Effects in Nonlinear Regression for more details on the different kinds of marginal effects. | . ",
    "url": "/Model_Estimation/GLS/logit_model.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#also-consider"
  },"525": {
    "doc": "Logit Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/logit_model.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#implementations"
  },"526": {
    "doc": "Logit Model",
    "title": "Gretl",
    "content": "# Load auto data open auto.gdt # Run logit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors logit mpg const headroom trunk weight . ",
    "url": "/Model_Estimation/GLS/logit_model.html#gretl",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#gretl"
  },"527": {
    "doc": "Logit Model",
    "title": "Python",
    "content": "There are a number of Python packages that can perform logit regressions but the most comprehensive is probably statsmodels. The code below is an example of how to use it. # Install pandas and statsmodels using pip or conda, if you don't already have them. import pandas as pd import statsmodels.formula.api as smf df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv', index_col=0) # Specify the model, regressing vs on mpg and cyl mod = smf.logit('vs ~ mpg + cyl', data=df) # Fit the model res = mod.fit() # Look at the results res.summary() # Compute marginal effects marg_effect = res.get_margeff(at='mean', method='dydx') # Show marginal effects marg_effect.summary() . ",
    "url": "/Model_Estimation/GLS/logit_model.html#python",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#python"
  },"528": {
    "doc": "Logit Model",
    "title": "R",
    "content": "R can run a logit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. # If necessary, install the mfx package # install.packages('mfx') # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run logit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_logit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = 'logit')) # The family argument says we are working with binary data # and using a logit link function (rather than, say, probit) # The results summary(my_logit) # Marginal effects logitmfx(vs ~ mpg + cyl, data = mtcars) . ",
    "url": "/Model_Estimation/GLS/logit_model.html#r",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#r"
  },"529": {
    "doc": "Logit Model",
    "title": "Stata",
    "content": "* Load auto data sysuse auto.dta * Logit Estimation logit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) . ",
    "url": "/Model_Estimation/GLS/logit_model.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html#stata"
  },"530": {
    "doc": "Logit Model",
    "title": "Logit Model",
    "content": " ",
    "url": "/Model_Estimation/GLS/logit_model.html",
    
    "relUrl": "/Model_Estimation/GLS/logit_model.html"
  },"531": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "Marginal effects plots for interactions with categorical variables",
    "content": "In many contexts, the effect of one variable on another might be allowed to vary. For example, the relationship between income and mortality might be different between someone with no degree, a high school degree, or a college degree. A marginal effects plot for a categorical interaction displays the effect of $X$ on $Y$ on the y-axis for different values of a categorical variable $Z$ on the x-axis. The plot will often include confidence intervals as well. In some cases the categorical variable may be ordered, so you’d want the $Z$ values to show up in that order. ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html"
  },"532": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "Keep in Mind",
    "content": ". | Some versions of these graphs normalize the effect of one of the categories to 0, and shows the effect for other values relative to that one. | . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#keep-in-mind"
  },"533": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "Also Consider",
    "content": ". | Consider performing a power analysis of interaction terms before running your analysis to see whether you have the statistical power for your interactions | Marginal effects plots for interactions with continuous variables | . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#also-consider",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#also-consider"
  },"534": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "Implementations",
    "content": "In each of these examples, we will be using data on organ donation rates by state from Kessler &amp; Roth 2014. The example is of a 2x2 difference-in-difference model extended to estimate dynamic treatment effects, where treatment is interacted with the number of time periods until/since treatment goes into effect. All of these examples directly retrieve effect and confidence interval information from the regression by hand rather than relying on a package; packages for graphing interactions often focus on continuous interactions. The original code snippets for the Python, R, and Stata examples comes from the textbook The Effect. ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#implementations",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#implementations"
  },"535": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "Python",
    "content": "# PYTHON CODE import pandas as pd import matplotlib.pyplot as plt import linearmodels as lm # Read in data od = pd.read_csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Figures/Data/Marginal_Effects_Plots_For_Interactions_With_Categorical_Variables/organ_donation.csv') # Create Treatment Variable od['California'] = od['State'] == 'California' # PanelOLS requires a numeric time variable od['Qtr'] = 1 od.loc[od['Quarter'] == 'Q12011', 'Qtr'] = 2 od.loc[od['Quarter'] == 'Q22011', 'Qtr'] = 3 od.loc[od['Quarter'] == 'Q32011', 'Qtr'] = 4 od.loc[od['Quarter'] == 'Q42011', 'Qtr'] = 5 od.loc[od['Quarter'] == 'Q12012', 'Qtr'] = 6 # Create our interactions by hand, # skipping quarter 3, the last one before treatment for i in range(1, 7): name = f\"INX{i}\" od[name] = 1 * od['California'] od.loc[od['Qtr'] != i, name] = 0 # Set our individual and time (index) for our data od = od.set_index(['State','Qtr']) mod = lm.PanelOLS.from_formula('''Rate ~ INX1 + INX2 + INX4 + INX5 + INX6 + EntityEffects + TimeEffects''',od) # Specify clustering when we fit the model clfe = mod.fit(cov_type = 'clustered', cluster_entity = True) # Get coefficients and CIs res = pd.concat([clfe.params, clfe.std_errors], axis = 1) # Scale standard error to CI res['ci'] = res['std_error']*1.96 # Add our quarter values res['Qtr'] = [1, 2, 4, 5, 6] # And add our reference period back in reference = pd.DataFrame([[0,0,0,3]], columns = ['parameter', 'lower', 'upper', 'Qtr']) res = pd.concat([res, reference]) # For plotting, sort and add labels res = res.sort_values('Qtr') res['Quarter'] = ['Q42010','Q12011', 'Q22011','Q32011', 'Q42011','Q12012'] # Plot the estimates as connected lines with error bars plt.errorbar(x = 'Quarter', y = 'parameter', yerr = 'ci', data = res) # Add a horizontal line at 0 plt.axhline(0, linestyle = 'dashed') . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#python",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#python"
  },"536": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "R",
    "content": "If you happen to be using the fixest package to run your model, there is actually a single convenient command coefplot that will make the graph for you. However, this requires your analysis to use some other tools from fixest too. So below I’ll show both the fixest approach as well as a more general approach (which also uses a fixest model but doesn’t need to). First, prepare the data: . library(tidyverse) library(fixest) library(broom) od &lt;- read_csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Figures/Data/Marginal_Effects_Plots_For_Interactions_With_Categorical_Variables/organ_donation.csv') # Treatment variable od &lt;- od %&gt;% mutate(Treated = State == 'California' &amp; Quarter %in% c('Q32011','Q42011','Q12012')) %&gt;% # Create an ordered version of Quarter so we can graph it # and make sure we drop the last pre-treatment interaction, # which is quarter 2 of 2011 mutate(Quarter = relevel(factor(Quarter), ref = 'Q22011')) %&gt;% # The treated group is the state of California # The 1* is only necessary for the first fixest method below; optional for the second, more general method mutate(California = 1*(State == 'California')) . Next, our steps to do the fixest-specific method: . # in the *specific example* of fixest, there is a simple and easy method: od &lt;- od %&gt;% mutate(fQuarter = factor(Quarter, levels = c('Q42010','Q12011','Q22011', 'Q32011','Q42011','Q12012'))) femethod &lt;- feols(Rate ~ i(California, fQuarter, drop = 'Q22011') | State + Quarter, data = od) coefplot(femethod, ref = c('Q22011' = 3), pt.join = TRUE) . However, for other packages this may not work, so I will also do it by hand in a way that will work with models more generally (even though we’ll still run the model in fixest): . # Interact quarter with being in the treated group clfe &lt;- feols(Rate ~ California*Quarter | State, data = od) coefplot(clfe, ref = 'Q22011') # Use broom::tidy to get the coefficients and SEs res &lt;- tidy(clfe) %&gt;% # Keep only the interactions filter(str_detect(term, ':')) %&gt;% # Pull the quarter out of the term mutate(Quarter = str_sub(term, -6)) %&gt;% # Add in the term we dropped as 0 add_row(estimate = 0, std.error = 0, Quarter = 'Q22011') %&gt;% # and add 95% confidence intervals mutate(ci_bottom = estimate - 1.96*std.error, ci_top = estimate + 1.96*std.error) %&gt;% # And put the quarters in order mutate(Quarter = factor(Quarter, levels = c('Q42010','Q12011','Q22011', 'Q32011','Q42011','Q12012'))) # And graph # \"group = 1\" is necessary to get ggplot to add the line graph # when the x-axis is a factor ggplot(res, aes(x = Quarter, y = estimate, group = 1)) + # Add points for each estimate and connect them geom_point() + geom_line() + # Add confidence intervals geom_linerange(aes(ymin = ci_bottom, ymax = ci_top)) + # Add a line so we know where 0 is geom_hline(aes(yintercept = 0), linetype = 'dashed') + # Always label! labs(caption = '95% Confidence Intervals Shown') . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#r",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#r"
  },"537": {
    "doc": "Marginal effects plots for interactions with categorical variables",
    "title": "Stata",
    "content": "* For running the model: * ssc install reghdfe import delimited using https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Figures/Data/Marginal_Effects_Plots_For_Interactions_With_Categorical_Variables/organ_donation.csv, clear * Create value-labeled version of quarter * So we can easily graph it g Qtr = 1 replace Qtr = 2 if quarter == \"Q12011\" replace Qtr = 3 if quarter == \"Q22011\" replace Qtr = 4 if quarter == \"Q32011\" replace Qtr = 5 if quarter == \"Q42011\" replace Qtr = 6 if quarter == \"Q12012\" label def quarters 1 \"Q42010\" 2 \"Q12011\" 3 \"Q22011\" 4 \"Q32011\" 5 \"Q42011\" 6 \"Q12012\" label values Qtr quarters * Interact being in the treated group * with Qtr, using ib3 to drop the third * quarter (the last one before treatment) g California = state == \"California\" reghdfe rate California##ib3.Qtr, a(state Qtr) vce(cluster state) * Pull out the coefficients and SEs g coef = . g se = . forvalues i = 1(1)6 { replace coef = _b[1.California#`i'.Qtr] if Qtr == `i' replace se = _se[1.California#`i'.Qtr] if Qtr == `i' } * Make confidence intervals g ci_top = coef+1.96*se g ci_bottom = coef - 1.96*se * Limit ourselves to one observation per quarter keep Qtr coef se ci_* duplicates drop * Create connected scatterplot of coefficients * with CIs included with rcap * and a line at 0 from function twoway (sc coef Qtr, connect(line)) (rcap ci_top ci_bottom Qtr) (function y = 0, range(1 6)), xtitle(\"Quarter\") caption(\"95% Confidence Intervals Shown\") . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#stata",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_categorical_variables.html#stata"
  },"538": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Marginal Effects Plots for Interactions with Continuous Variables",
    "content": "In many contexts, the effect of one variable on another might be allowed to vary. For example, the relationship between income and mortality is nonlinear, so the effect of an additional dollar of income on mortality is different for someone earning $20,000/year than for someone earning $100,000/year. Or maybe the relationship between income and mortality differs depending on how many years of education you have. A marginal effects plot displays the effect of \\(X\\) on \\(Y\\) for different values of \\(Z\\) (or \\(X\\)). The plot will often include confidence intervals as well. The same code will often work if there’s not an explicit interaction, but you are, for example, estimating a logit model where the effect of one variable changes with the values of the others. ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html"
  },"539": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Keep in Mind",
    "content": ". | Interactions often have poor statistical power, and you will generally need a lot of observations to tell if the effect of $X$ on \\(Y\\) is different for two given different values of \\(Z\\). | Make sure your graph has clearly labeled axes, so readers can tell whether your y-axis is the predicted value of $Y$ or the marginal effect of \\(X\\) on \\(Y\\). | . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#keep-in-mind"
  },"540": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Also Consider",
    "content": ". | Consider performing a power analysis of interaction terms before running your analysis to see whether you have the statistical power for your interactions | Average marginal effects or marginal effects at the mean can be used to get a single marginal effect averaged over your sample, rather than showing how it varies across the sample. | Marginal effects plots for interactions with categorical variables | . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#also-consider",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#also-consider"
  },"541": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#implementations",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#implementations"
  },"542": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "R",
    "content": "The interplot package can plot the marginal effect of a variable \\(X\\) (y-axis) against different values of some variable. If instead you want the predicted values of \\(Y\\) on the y-axis, look at the ggeffects package. # Install relevant packages, if necessary: # install.packages(c('ggplot2', 'interplot')) # Load in ggplot2 and interplot library(ggplot2) library(interplot) # Load in the txhousing data data(txhousing) # Estimate a regression with a nonlinear term cubic_model &lt;- lm(sales ~ listings + I(listings^2) + I(listings^3), data = txhousing) # Get the marginal effect of var1 (listings) # at different values of var2 (listings), with confidence ribbon. # This will return a ggplot object, so you can # customize using ggplot elements like labs(). interplot(cubic_model, var1 = \"listings\", var2 = \"listings\")+ labs(x = \"Number of Listings\", y = \"Marginal Effect of Listings\") # Try setting adding listings*date to the regression model # and then in interplot set var2 = \"date\" to get the effect of listings at different values of date . This results in: . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#r",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#r"
  },"543": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Stata",
    "content": "We will use the marginsplot command, which requires Stata 12 or higher. * Load in the National Longitudinal Survey of Youth - Women sample sysuse nlsw88.dta * Perform a regression with a nonlinear term regress wage c.tenure##c.tenure * Use margins to calculate the marginal effects * Put the variable we're interested in getting the effect of in dydx() * And the values we want to evaluate it at in at() margins, dydx(tenure) at(tenure = (0(1)26)) * (If we had interacted with another variable, say age, we would specify similarly, * with at(age = (start(count-by)end))) * Then, marginsplot * The recast() and recastci() options make the effect/CI show up as a line/area * Remove to get points/lines instead. marginsplot, xtitle(\"Tenure\") ytitle(\"Marginal Effect of Tenure\") recast(line) recastci(rarea) . This results in: . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#stata",
    
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#stata"
  },"544": {
    "doc": "Matching",
    "title": "Matching",
    "content": " ",
    "url": "/Model_Estimation/Matching/matching.html",
    
    "relUrl": "/Model_Estimation/Matching/matching.html"
  },"545": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "McFadden’s Choice Model (Alternative-Specific Conditional Logit)",
    "content": "Discrete choice models are a regression method used to predict a categorical dependent variable with more than two categories. For example, a discrete choice model might be used to predict whether someone is going to take a train, car, or bus to work. McFadden’s Choice Model is a discrete choice model that uses conditional logit, in which the variables that predict choice can vary either at the individual level (perhaps tall people are more likely to take the bus), or at the alternative level (perhaps the train is cheaper than the bus). For more information, see Wikipedia: Discrete Choice . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#mcfaddens-choice-model-alternative-specific-conditional-logit",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#mcfaddens-choice-model-alternative-specific-conditional-logit"
  },"546": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Keep in Mind",
    "content": ". | Just like other regression methods, the McFadden model does not guarantee that the estimates will be causal. Similarly, while the McFadden model is designed so that the results can be interpreted in terms of a “random utility” function, making inferences about utility functions does require additional assumptions. | The standard McFadden model assumes that the choice follows the Independence of Irrelevant Alternatives, which may be a strong assumption. There are variants of the McFadden model that relax this assumption. | If you are working with an estimation command that only allows alternative-specific predictors and not case-specific predictors, you can add them yourself by interacting the case-specific predictors with binary variables for the different alternatives. If \\(Income\\) is your case-specific variable and your alternatives are “train”, “bus”, and “car”, you’d add \\(Income \\times (mode == \"train\")\\), \\(Income \\times (mode == \"bus\")\\), and \\(Income \\times (mode == \"car\")\\) to your model. These are your case-specific predictors. | Choice model regressions often have specific demands on how your data is structured. These vary across estimation commands and software packages. However, a common one is this (others will be pointed out in specific Implementations below): The data must contain a variable indicating the choice cases (i.e. you choose a car, that’s one case, then I choose a car, that’s a different case), a variable with the alternatives being chosen between, a binary variable equal to 1 for the alternative actually chosen (this should be 1 or TRUE exactly once within each choice case), and then variables that are case-specific or alternative-specific. | . In the below table, \\(I\\) gives the choice case, \\(Alts\\) gives the options, \\(Chose\\) gives the choice, \\(X\\) is a variable that varies at the alternative level, and \\(Y\\) is a variable that varies at the case level. | I | Alts | Chose | X | Y | . | 1 | A | 1 | 10 | 3 | . | 1 | B | 0 | 20 | 3 | . | 1 | C | 0 | 10.5 | 3 | . | 2 | A | 0 | 8 | 5 | . | 2 | B | 1 | 9 | 5 | . | 3 | C | 0 | 1 | 5 | . This might be referred to as “long” choice data. “Wide” choice data is also common, and looks like: . | I | Chose | Y | XA | XB | XC | . | 1 | A | 3 | 10 | 20 | 10.5 | . | 2 | B | 5 | 8 | 9 | 1 | . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#keep-in-mind"
  },"547": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Also Consider",
    "content": ". | In order to relax the independence of irrelevant alternatives assumption and/or more closely model individual preferences, consider the mixed logit, nested logit or hierarchical Bayes conditional logit models. | . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#also-consider"
  },"548": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#implementations"
  },"549": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "R",
    "content": "We will implement McFadden’s choice model in R using the mlogit package, which can accept “wide” or “long” data in the mlogit.data function. library(mlogit) # Get Car data, in \"wide\" choice format data(Car) # If we look at the data, the choice-specific variables are named # e.g. \"speed1\" \"speed2\" \"speed3\" and so on. # So we need our choice variable to be 1, 2, 3 ,to match # Right now instead it's choice1, choice2, choice3. So we edit. Car$choice &lt;- substr(Car$choice, 7, 7) # For this we need to specify the choice variable with choice # whether it's currently in wide or long format with shape # the column numbers of the alternative-specific variables with varying. # We need alt.levels to tell us what our alternatives are (1-6, as seen in choice). # We also need sep = \"\" since our wide-format variable names are type1, type2, etc. # If the variable names were type_1, type_2, etc., we'd need sep = \"_\". # If this were long data we'd also want: # the case identifier with id.var (for individuals) and/or chid.var # (for multiple choices within individuals) # And a variable indicating the alternatives with alt.var # But could skip the alt.levels and sep arguments mlogit.Car &lt;- mlogit.data(Car, choice = 'choice', shape = 'wide', varying = 5:70, sep=\"\") # mlogit.Car is now in \"long\" format # Note that if we did start with \"long\" format we could probably skip the mlogit.data() step. # Now we can run the regression with mlogit(). # We \"regress\" the choice on the alternative-specific variables like type, fuel, and price # Then put a pipe separator | # and add our case-specific variables like college model &lt;- mlogit(choice ~ type + fuel + price | college, data = mlogit.Car) # Look at the results summary(model) . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#r",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#r"
  },"550": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Stata",
    "content": "Stata has the McFadden model built in. We will estimate the model using the older asclogit command as well as the cmclogit command that comes with Stata 16. These commands require “long” choice data, as described in the Keep in Mind section. * Load in car choice data webuse carchoice * To use asclogit, we \"regress\" our choice variable (purchase) * on any alternative-specific variables (dealers) * then we put our case ID variable consumerid in case() * and our variable specifying alternatives, car, in alternatives() * then finally we put any case-specific variables like gender and income, in casevars() asclogit purchase dealers, case(consumerid) alternatives(car) casevars(gender income) * To use cmclogit, we first declare our data to be choice data with cmset * specifying our case ID variable and then the set of alternatives cmset consumerid car * Now that Stata knows the structure, we can omit those parts from the asclogit * specification, but the rest stays the same! cmclogit purchase dealers, casevars(gender income) . Why bother with the cmclogit version? cmset gives you a lot more information about your data, and makes it easy to transition between different choice model types, including those incorporating panel data (each person makes multiple choices). ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#stata"
  },"551": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "content": " ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html",
    
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html"
  },"552": {
    "doc": "Merging Shape Files",
    "title": "Merging Shape Files",
    "content": "When we work with spatial anaylsis, it is quite often we need to deal with data in different format and at different scales. For example, I have nc data with global pm2.5 estimation with \\(0.01\\times 0.01\\) resolution. But I want to see the pm2.5 estimation in municipal level. I need to integrate my nc file into my municipality shp file so that I can group by the data into municipal level and calculate the mean. Then, I can make a map of it. In this page, I will use Brazil’s pm2.5 estimation and its shp file in municipal level. ",
    "url": "/Geo-Spatial/merging_shape_files.html",
    
    "relUrl": "/Geo-Spatial/merging_shape_files.html"
  },"553": {
    "doc": "Merging Shape Files",
    "title": "Keep in Mind",
    "content": ". | It doesn’t have to be nc file to map into the shp file, any format that can read in and convert to a sf object works. But the data has to have geometry coordinates(longitude and latitude). | . ",
    "url": "/Geo-Spatial/merging_shape_files.html#keep-in-mind",
    
    "relUrl": "/Geo-Spatial/merging_shape_files.html#keep-in-mind"
  },"554": {
    "doc": "Merging Shape Files",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/merging_shape_files.html#implementations",
    
    "relUrl": "/Geo-Spatial/merging_shape_files.html#implementations"
  },"555": {
    "doc": "Merging Shape Files",
    "title": "R",
    "content": "Unusually for LOST, the example data files cannot be accessed from the code directly. Please visit this page and download both files to your working directory before running this code. It is also strongly recommended that you find a high-powered computer or cloud service before attempting to run this code, as it requires a lot of memory. # If necesary # install.packages(c('ncdf4','sp','raster','dplyr','sf','ggplot2','reprex','ggsn')) # Load packages library(ncdf4) library(sp) library(raster) library(dplyr) library(sf) library(ggplot2) library(reprex) ### Step 1: Read in nc file as a dataframe* pm2010 = nc_open(\"https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Geo-Spatial/Data/Merging_Shape_Files/GlobalGWRwUni_PM25_GL_201001_201012-RH35_Median_NoDust_NoSalt.nc?raw=true\") nc.brick = brick(\"https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Geo-Spatial/Data/Merging_Shape_Files/GlobalGWRwUni_PM25_GL_201001_201012-RH35_Median_NoDust_NoSalt.nc?raw=true\") # Check the dimensions dim(nc.brick) # Turn into a data frame for use nc.df = as.data.frame(nc.brick[[1]], xy = T) head(nc.df) ### Step 2: Filter out a specific country. # Global data is very big. I am going to focus only on Brazil. nc.brazil = nc.df %&gt;% filter(x &gt;= -73.59 &amp; x &lt;= 34.47 &amp; y &gt;= -33.45 &amp; y &lt;= 5.16) rm(nc.df) head(nc.brazil) ### Step 3: Change the dataframe to a sf object using the st_as_sf function pm25_sf = st_as_sf(nc.brazil, coords = c(\"x\", \"y\"), crs = 4326, agr = \"constant\") rm(nc.brazil) head(pm25_sf) ### Step 4: Read in the Brazil shp file. we plan to merge to Brazil_map_2010 = st_read(\"https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Geo-Spatial/Data/Merging_Shape_Files/geo2_br2010.shp?raw=true\") head(Brazil_map_2010) ### Step 5: Intersect pm25 sf object with the shp file.* # Now let's use a sample from pm25 data and intersect it with the shp file. Since the sf object is huge, I recommend running the analysis on a cloud server pm25_sample = sample_n(pm25_sf, 1000, replace = FALSE) # Now look for the intersection between the pollution data and the Brazil map to merge them pm25_municipal_2010 = st_intersection(pm25_sample, Brazil_map_2010) head(pm25_municipal_2010) ### Step 6: Make a map using ggplot pm25_municipal_2010 = pm25_municipal_2010 %&gt;% select(1,6) pm25_municipal_2010 = st_drop_geometry(pm25_municipal_2010) Brazil_pm25_2010 = left_join(Brazil_map_2010, pm25_municipal_2010) ggplot(Brazil_pm25_2010) + # geom_sf creates the map we need geom_sf(aes(fill = -layer), alpha=0.8, lwd = 0, col=\"white\") + # and we fill with the pollution concentration data scale_fill_viridis_c(option = \"viridis\", name = \"PM25\") + ggtitle(\"PM25 in municipals of Brazil\")+ ggsn::blank() . ",
    "url": "/Geo-Spatial/merging_shape_files.html#r",
    
    "relUrl": "/Geo-Spatial/merging_shape_files.html#r"
  },"556": {
    "doc": "Mixed Logit Model",
    "title": "Keep in Mind",
    "content": ". | The mixed logit model estimates a distribution. Parameters are then generated from that distribution via a simulation with a specified number of draws. | The estimates from a mixed logit model cannot simply be interpreted as marginal effects, as they are maximum likelihood estimations. Further, the variation at the individual level means estimated effects are relative to the individual. | The estimation of mixed logit models is very difficult and there are quite a few details and different approaches. So you can’t really assume that one package will produce the same results as another. Read the documentation of the command you’re using so you at least know what paper produced the estimation method! | . ",
    "url": "/Model_Estimation/Multilevel_Models/mixed_logit.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/mixed_logit.html#keep-in-mind"
  },"557": {
    "doc": "Mixed Logit Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Multilevel_Models/mixed_logit.html#implementations",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/mixed_logit.html#implementations"
  },"558": {
    "doc": "Mixed Logit Model",
    "title": "R",
    "content": "To estimate a mixed logit model in R, we will first transform the data using the dfidx package. Then we will use the mlogit package to carry out the estimation. # Install mlogit which also includes the Electricity dataset for the example. # The package dfidx will be used to transform our data # install.packages(\"mlogit\", \"dfidx\") library(mlogit) library(dfidx) # Load the Electricity dataset data(\"Electricity\", package = \"mlogit\") # First, we need to coerce the data to a dfidx object # This allows for a panel with multiple indices # For further documentation, see dfidx. Electricity$index &lt;- 1:nrow(Electricity) elec = dfidx(Electricity, idx = list(c(\"index\", \"id\")), choice = \"choice\", varying = 3:26, sep = \"\") # We then estimate individual choice over electricity providers for # different cost and contract structures with a suppressed intercept my_mixed_logit = mlogit(data = elec, formula = choice ~ 0 + pf + cl + loc + wk + tod + seas, # Specify distributions for random parameter estimates # \"n\" indicates we have specified a normal distribution # note pf is omitted from rpar, so it will not be estimated as random rpar = c(cl = \"n\", loc = \"u\", wk = \"n\", tod = \"n\", seas = \"n\"), # R is the number of simulation draws R = 100, # For simplicity, we won't include correlated parameter estimates correlation = FALSE, # This data is from a panel panel = TRUE) # Results summary(my_mixed_logit) # Note that this output will include the simulated coefficient estimates, # simulated standard error estimates, and distributional details for the # random coefficients (all, in this case) # Note also that pf is given as a point estimate, and mlogit does not generate # a distribution for it as it does the others # You can extract and summarize coefficient estimates using the rpar function marg_loc = rpar(my_mixed_logit, \"loc\") summary(marg_loc) # You can also normalize coefficients and distributions by, say, price cl_by_pf = rpar(my_mixed_logit, \"cl\", norm = \"pf\") summary(cl_by_pf) . For further examples, visit the CRAN vignette here. For a very detailed example using the Electricity dataset, see here. ",
    "url": "/Model_Estimation/Multilevel_Models/mixed_logit.html#r",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/mixed_logit.html#r"
  },"559": {
    "doc": "Mixed Logit Model",
    "title": "Stata",
    "content": "As of Stata 17, there is the base-Stata xtmlogit command which is probably preferable to mixlogit. However, many people do not have Stata 17, so this example uses mixlogit, which requires installation from ssc install mixlogit. For more information on xtmlogit, see this page. mixlogit requires data of the form (although not necessarily with the variable names): . | choice | X | group | id | . | 1 | 10 | 1 | 1 | . | 0 | 12 | 1 | 1 | . | 0 | 11 | 2 | 1 | . | 1 | 14 | 2 | 1 | . | 1 | 9 | 3 | 2 | . | 0 | 11 | 3 | 2 | . where choice is the dependent variable and is binary, indicating which of the options was chosen. X is (one of) the predictors, group is an identifying variable for the different choice occasions, and id is a vector of individual-decision-maker identifiers, if this is panel data where the same decision-making makes multiple decisions. * If necessary: * ssc install mixlogit import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Multilevel_Models/Data/Electricity.csv\", clear * Reshape data into \"long\" format like we need for mixlogit g decision_id = _n reshape long pf cl loc wk tod seas, i(choice id decision_id) j(option) * Remember, the dependent variable should be binary, indicating that this option * was chosen g chosen = choice == option * Let's fix the parameters on all the predictors * except for pf, which we'll allow to vary * (this is for speed in the example) mixlogit chose cl loc wk tod seas, /// group(decision_id) /// each individual choice is identified by decision_id id(id) /// each person is identified by id rand(pf) * Options to consider: * corr allows multiple parameter distributions to be correlated * ln() allows some of the parameter distributions to be log-normal * We can get individual parameter estimates with mixlbeta, which will * save the estimates to file mixlbeta pf, saving(pf_coefs.dta) . ",
    "url": "/Model_Estimation/Multilevel_Models/mixed_logit.html#stata",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/mixed_logit.html#stata"
  },"560": {
    "doc": "Mixed Logit Model",
    "title": "Mixed Logit Model",
    "content": "A mixed logit model (sometimes referred to as a random parameters logit model) estimates distributional parameters that allow for individual-level heterogeneity in tastes that are not compatible with a traditional logit framework. Mixed logit models can also provide for additional flexibility as it pertains to correlated random parameters and can be used with panel data. For more information about mixed logit models, see Wikipedia: Mixed Logit. ",
    "url": "/Model_Estimation/Multilevel_Models/mixed_logit.html",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/mixed_logit.html"
  },"561": {
    "doc": "Nested Logit Model",
    "title": "Keep in Mind",
    "content": ". | Returned beta coefficients are not the marginal effects normally returned from an OLS regression. They are maximum likelihood estimations. A beta coefficient can not be interpreted as “a unit increase in $X$ leads to a $\\beta$ unit change in the probability of $Y$.” . | The marginal effect can be obtained by performing a transformation after you estimate. A rough estimation technique is to divide the beta coefficient by 4. | Another transformation that may be helpful is the odds ratio. This value is found by raising $e$ to the power of the beta coefficient. $e^\\beta$ can be interpreted as : the percentage change in likelihood of $Y$, given a unit change in $X$. | . ",
    "url": "/Model_Estimation/GLS/nested_logit.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/nested_logit.html#keep-in-mind"
  },"562": {
    "doc": "Nested Logit Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/nested_logit.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/nested_logit.html#implementations"
  },"563": {
    "doc": "Nested Logit Model",
    "title": "R",
    "content": "R has multiple packages that can estimate a nested logit model. To show a simple example, we will use the mlogit package. # Install mlogit and AER packages and load them. Latter is just for a dataset we'll be using. # install.packages(\"mlogit\", \"AER\") library(\"mlogit\", \"AER\") # Load dataset TravelMode data(\"TravelMode\", package = \"AER\") # Use the mlogit() function to run a nested logit estimation # Here, we will predict what mode of travel individuals # choose using cost and wait times nestedlogit = mlogit( choice ~ gcost + wait, data = TravelMode, ##The variable from which our nests are determined alt.var = 'mode', #The variable that dictates the binary choice choice = 'choice', #List of nests as named vectors nests = list(Fast = c('air','train'), Slow = c('car','bus')) ) # The results summary(nestedlogit) # In this case, air travel is treated as the base level. # others maximum likelihood estimators relative # to air are reported as separate intercepts # The elasticities for each cluster are displayed # as iv:Fast and iv:Slow . Another set of more robust examples comes from Kenneth Train and Yves Croissant . ",
    "url": "/Model_Estimation/GLS/nested_logit.html#r",
    
    "relUrl": "/Model_Estimation/GLS/nested_logit.html#r"
  },"564": {
    "doc": "Nested Logit Model",
    "title": "Nested Logit Model",
    "content": "A nested logistical regression (nested logit, for short) is a statistical method for finding a best-fit line when the the outcome variable $Y$ is a binary variable, taking values of 0 or 1. Logit regressions, in general, follow a logistical distribution and restrict predicted probabilities between 0 and 1. Traditional logit models require that the Independence of Irrelevant Alternatives(IIA) property holds for all possible outcomes of some process. Nested logit models differ by allowing ‘nests’ of outcomes that satisfy IIA, but not requiring that all outcomes jointly satisfy IIA. For an example of violating the IIA property, see Red Bus/Blue Bus Paradox. For a more thorough theoretical treatment, see SAS Documentation: Nested Logit. ",
    "url": "/Model_Estimation/GLS/nested_logit.html",
    
    "relUrl": "/Model_Estimation/GLS/nested_logit.html"
  },"565": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "Nonlinear Significance Tests",
    "content": "Most regression output, or output from other methods that produce multiple coefficients, will include the results of frequentist hypothesis tests comparing each coefficient to 0. However, in many cases, you may be interested in a hypothesis test of a null restriction that involves a nonlinear combination of the coefficients, or producing an estimate and sampling distriubtion for that nonlinear combination. For example, in the model . \\[Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\varepsilon\\] You may be interested in the ratio of the two effects, \\(\\beta_1/\\beta_2\\), and would want an estimate of that combination, along with a standard error, and a hypothesis test comparing that estimate to 0 or some other value. Estimates and tests of nonlinear combinations of coefficients are different than for linear combinations, because they imply restrictions on estimation that cannot be expressed in the form of a matrix of linear restrictions. The most common approach to producing a sampling distribution for a nonlinear combination of coefficients is the delta method and that is what all the commands on this page use. ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#nonlinear-significance-tests",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#nonlinear-significance-tests"
  },"566": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "Keep in Mind",
    "content": ". | Depending on your goal, you may be able to avoid doing a test of nonlinear combinations of coefficients by converting the combination into a linear one. For example, if you do not want to estimate \\(\\beta_1/\\beta_2\\) itself, but instead are only interested in testing the null hypothesis \\(\\beta_1/\\beta_2 = 1\\), this null hypothesis can be manipulated to instead be \\(\\beta_1 = \\beta_2\\) or \\(\\beta_1 - \\beta_2 = 0\\), either of which can be evaluated as a hypothesis test on a linear combination of coefficients. | . ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#keep-in-mind"
  },"567": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "Also Consider",
    "content": ". | Linear Hypothesis Tests. | . ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#also-consider",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#also-consider"
  },"568": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#implementations",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#implementations"
  },"569": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "R",
    "content": "In R, the marginaleffects package contains a number of useful functions for postestimation, including nonlinear combinations of coefficients via the deltamethod() function. It is used here with lm(), but is also compatible with regression output from many other packages and functions. library(marginaleffects) data(mtcars) # Run the model m = lm(mpg ~ hp + wt, data = mtcars) # Specify the combination of coefficients in the form of a null-hypothesis equation deltamethod(m, 'hp/wt = 1') # This produces an estimate, standard error, p-value, and confidence interval . ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#r",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#r"
  },"570": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "Stata",
    "content": "Stata has the nlcom postestimation command for producing estimates and standard errors for nonlinear tests of coefficients. It will also produce the results of hypothesis tests comparing the combination to 0, so to compare to other values, subtract the desired value from the combination. * Load auto data sysuse https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.dta regress mpg trunk weight nlcom _b[trunk]/_b[weight] - 1 . ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#stata",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html#stata"
  },"571": {
    "doc": "Nonlinear Hypothesis Tests",
    "title": "Nonlinear Hypothesis Tests",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/nonlinear_hypothesis_tests.html"
  },"572": {
    "doc": "Nonstandard Errors",
    "title": "Nonstandard errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html#nonstandard-errors",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html#nonstandard-errors"
  },"573": {
    "doc": "Nonstandard Errors",
    "title": "Nonstandard Errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html",
    
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html"
  },"574": {
    "doc": "Ordered Probit/Logit",
    "title": "Ordered Probit / Ordered Logit",
    "content": "Ordered probit and ordered logit are regression methods intended for use when the dependent variable is ordinal. That is, there is a natural ordering to the different (discrete) values, but no cardinal value. So we might know \\(A &gt; B\\) but not by how much \\(A\\) is greater than \\(B\\). Examples of ordinal data include responses on a Likert scale (“Very satisfied” is more satisfied than “Satisfied”, and “Satisfied” is more satisfied than “Not Satisfied”, but the difference between “Very satisfied” and “Satisfied” may not be the same as the difference between “Satisfied” and “Not Satisfied” but we may not know by how much) or education levels (a Master’s degree is more education than a Bachelor’s degree, but how much more?). When the dependent variable is ordinal, typical linear regression may not work well because it relies on absolute differences in value. Ordered probit and ordered logit take a latent-variable approach to this problem. They assume that the discrete dependent variable simply represents a continuous latent variable. In the Likert scale example this might be “satisfied-ness”. In ordered probit this latent variable is normally distributed, and in ordered logit it is distributed according to a logistic distribution. Then, the actual values just carve up the regions of that latent variable. So if satisfied-ness is distributed \\(S\\sim N(0,1)\\), then perhaps “very satisfied” is \\(S &gt; .892\\), “satisfied” is \\(.321 &lt; S \\leq .892\\), and so on. The .321 and .892 are “cutoff values” separating the categories. These cutoff values are estimated by ordered probit and ordered logit. These models assume that predictors affect the latent variable the same no matter which level you’re at. There isn’t a predictor that, for example, makes you more likely to be “satified” and less likely to be either “very satisfied” or “not satisfied” (or a predictor that has a slight positive effect going from “not satisfied” to “satisfied” but a huge effect going from “satisfied” to “very sastisfied”). You can imagine taking your ordinal variable and collapsing it into a binary one: comparing, say, “very not satisfied” and “not satisfied” as one group vs. “satisfied” and “very satisfied” as the other in a typical probit or logit. Ordered logit/probit assumes that this will give the same results as if you’d split somewhere else, comparing “very not satisfied”, “not satisfied”, and “satisfied” vs. “very satisfied”. This is the “parallel lines” or “parallel regression” assumption, or for ordered logit “proportional odds”. ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html#ordered-probit--ordered-logit",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html#ordered-probit--ordered-logit"
  },"575": {
    "doc": "Ordered Probit/Logit",
    "title": "Keep in Mind",
    "content": ". | Coefficients on predictors are scaled in terms of the latent variable and in general are difficult to interpret. You can calculate marginal effects from ordered probit/logit results, which report how changes in a predictor are related to people moving from one category to another. For example, if the marginal effect of \\(X\\) is +.03 for “very not satisfied”, +.02 for “not satisfied”, .+.02 for “satisfied”, and -.07 for “very satisfied”, that means that a one-unit increase in \\(X\\) results in a drop in the proportion of the sample predicted to be “very satisfied” and that drop is reallocated across the other three levels, everyone shifting down a bit and some ending up in a new category. | To identify the model, one of the cutoff parameters (the lowest one, separating the lowest category and the second-lowest) is usually fixed at 0. The cutoff values are in general only meaningful relative to each other for this reason and don’t mean anything on their own. | It is a good idea to test the parallel lines assumption. This is commonly done using a Brant (1990) test, which basically checks the different above/below splits possible with the dependent variable and sees how much the coefficients differ (hoping they don’t differ a lot!). If the test fails, you may want to use a generalized ordered logit, which has less explanatory power but does not rely on the parallel trends assumption. Code for both these steps is below. Doing the test rather than just starting with generalized ordered logit is a good idea because you do lose power and interpretability with generalized ordered logit; see Williams 2016. | . ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html#keep-in-mind"
  },"576": {
    "doc": "Ordered Probit/Logit",
    "title": "Also Consider",
    "content": ". | If the dependent variable is not ordered, consider a multinomial model instead. | . ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html#also-consider"
  },"577": {
    "doc": "Ordered Probit/Logit",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html#implementations"
  },"578": {
    "doc": "Ordered Probit/Logit",
    "title": "R",
    "content": "The necessary tools to work with ordered probit and logit are unfortunately scattered across several packages in R. MASS contains the ordered probit/logit estimator, brant has the Brant test, and if that fails you’re off to VGAM for generalized ordered logit. # For the ordered probit/logit model library(MASS) # For the brant test library(brant) # For the generalized ordered logit library(VGAM) # For marginal effects library(erer) # Data on marital happiness and affairs # Documentation: https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Fair.html mar &lt;- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Fair.csv') # See how various factors predict marital happiness m &lt;- polr(factor(rate) ~ age + child + religious + education + nbaffairs, data = mar, method = 'logistic' # change to 'probit' for ordered probit ) summary(m) # Brant test of proportional odds brant(m) # The \"Omnibus\" probability is .03, if we have alpha = .05 then we reject proportional odds # Specifically the test tells us that education is the problem. Dang. # We can use vglm for the generalized ordered logit gologit &lt;- vglm(factor(rate) ~ age + child + religious + education + nbaffairs, cumulative(link = 'logitlink', parallel = FALSE), # parallel = FALSE tells it not to assume parallel lines data = mar) summary(gologit) # Notice how each predictor now has many coefficients - one for each level # and we have other problems denoted in its warnings! # If we want marginal effects for our original ordered logit... ocME(m) . ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html#r",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html#r"
  },"579": {
    "doc": "Ordered Probit/Logit",
    "title": "Stata",
    "content": "Ordered logit / probit requires a few packages to be installed, including gologit2 for the generalized ordered logit, and for the Brant test spost13, which is not on ssc. * For the brant test we must install spost13 * which is not on ssc, so do \"findit spost13\" and install \"spost13_ado\" * for generalized ordered logit, do \"ssc install gologit2\" * Data on marital happiness and affairs * Documentation: https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Fair.html import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Fair.csv\", clear * strings can't be factors encode child, g(child_n) * Run ologit or oprobit ologit rate age i.child_n religious education nbaffairs * Use the brant test brant * The \"All\" probability is .03, if we have alpha = .05 then we reject proportional odds * Specifically the test tells us that education is the problem. Dang. * Running generalized ordered logit instead gologit2 rate age i.child_n religious education nbaffairs * Notice how each predictor now has many coefficients - one for each level * and we have a negative predicted probability denoted in the warnings! * We can get marginal effects for either model using margins ologit rate age i.child_n religious education nbaffairs margins, dydx(*) gologit2 rate age i.child_n religious education nbaffairs margins, dydx(*) . ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html#stata"
  },"580": {
    "doc": "Ordered Probit/Logit",
    "title": "Ordered Probit/Logit",
    "content": " ",
    "url": "/Model_Estimation/GLS/ordered_probit_logit.html",
    
    "relUrl": "/Model_Estimation/GLS/ordered_probit_logit.html"
  },"581": {
    "doc": "Penalized Regression",
    "title": "Penalized Regression",
    "content": "When running a regression, especially one with many predictors, the results have a tendency to overfit the data, reducing out-of-sample predictive properties. Penalized regression eases this problem by forcing the regression estimator to shrink its coefficients towards 0 in order to avoid the “penalty” term imposed on the coefficients. This process is closely related to the idea of Bayesian shrinkage, and indeed standard penalized regression results are equivalent to regression performed using certain Bayesian priors. Regular OLS selects coefficients \\(\\hat{\\beta}\\) to minimize the sum of squared errors: . \\[\\min\\sum_i(y_i - X_i\\hat{\\beta})^2\\] Non-OLS regressions similarly select coefficients to minimize a similar objective function. Penalized regression adds a penalty term \\(\\lambda\\lVert\\beta\\rVert_p\\) to that objective function, where \\(\\lambda\\) is a tuning parameter that determines how harshly to penalize coefficients, and \\(\\lVert\\beta\\rVert_p\\) is the \\(p\\)-norm of the coefficients, or \\(\\sum_j\\lvert\\beta\\rvert^p\\). \\[\\min\\left(\\sum_i(y_i - X_i\\hat{\\beta})^2 + \\lambda\\left\\lVert\\beta\\right\\rVert_p \\right)\\] Typically \\(p\\) is set to 1 for LASSO regression (least absolute shrinkage and selection operator), which has the effect of tending to set coefficients to 0, i.e. model selection, or to 2 for Ridge Regression. Elastic net regression provides a weighted mix of LASSO and Ridge penalties, commonly referring to the weight as \\(\\alpha\\). ",
    "url": "/Machine_Learning/penalized_regression.html",
    
    "relUrl": "/Machine_Learning/penalized_regression.html"
  },"582": {
    "doc": "Penalized Regression",
    "title": "Keep in Mind",
    "content": ". | To avoid being penalized for a constant term, or by differences in scale between variables, it is a very good idea to standardize each variable (subtract the mean and divide by the standard deviation) before running a penalized regression. | Penalized regression can be run for logit and other kinds of regression, not just linear regression. Using penalties with general linear models like logit is common. | Penalized regression coefficients are designed to improve out-of-sample prediction, but they are biased. If the goal is estimation of a parameter, rather than prediction, this should be kept in mind. A common procedure is to use LASSO to select variables, and then run regular regression models with the variables that LASSO has selected. | The \\(\\lambda\\) parameter is often chosen using cross-validation. Many penalized regression commands include an option to select \\(\\lambda\\) by cross-validation automatically. | LASSO models commonly include variables along with polynomial transformation of those variables and interactions, allowing LASSO to determine which transformations are worth keeping. | . ",
    "url": "/Machine_Learning/penalized_regression.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/penalized_regression.html#keep-in-mind"
  },"583": {
    "doc": "Penalized Regression",
    "title": "Also Consider",
    "content": ". | If it is not important to estimate coefficients but the goal is simply to predict an outcome, then there are many other machine learning methods that do so, and in some cases can handle higher dimensionality or work with smaller samples. | . ",
    "url": "/Machine_Learning/penalized_regression.html#also-consider",
    
    "relUrl": "/Machine_Learning/penalized_regression.html#also-consider"
  },"584": {
    "doc": "Penalized Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/penalized_regression.html#implementations",
    
    "relUrl": "/Machine_Learning/penalized_regression.html#implementations"
  },"585": {
    "doc": "Penalized Regression",
    "title": "Python",
    "content": "This is an example of running penalised regressions in Python. The main takeaways are that the ubiquitous machine learning package sklearn can perform lasso, ridge, and elastic net regressions. In the example below, we’ll see all three in action. The level of penalisation will be set automatically by cross-validation, although a user may also supply the number directly. This example will use the seaborn package (for data), the patsy package (to create matrices from formulae), the matplotlib package (for plotting), the pandas package (for data manipulation), and the sklearn package (for machine learning). To run the example below, you may need to first install these packages. First, we need to import these packages for use. import seaborn as sns from patsy import dmatrices, dmatrix from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LassoCV, ElasticNetCV, RidgeCV import matplotlib.pyplot as plt import pandas as pd . Now let’s load the data and transform it into a vector of endogeneous variables, and a matrix of exogenous variables. Using patsy, we’ll ask for all interaction variables among sepal width, petal length, and petal width (and exclude having an intercept). iris = sns.load_dataset(\"iris\") formula = (\"sepal_length ~ (sepal_width + petal_length + petal_width)**2 - 1\") y, X = dmatrices(formula, iris) . Some machine learning algorithms are more performant with data that are scaled before being used. One should be careful when scaling data if using test and training sets; here, we’re not worried about a test set though, so we just use the standard scaler (which transforms data to have 0 mean and unit standard deviation) on all of the \\(X\\) and \\(y\\) data. scale_X = StandardScaler().fit(X).transform(X) scale_y = StandardScaler().fit(y).transform(y) scale_y = scale_y.ravel() # ravel collapses a (150, 1) vector to (150,) . Now we run lasso with cross-validation. reg_lasso = LassoCV(cv=10).fit(scale_X, scale_y) . Let’s display the results so we can see for which value of \\(\\alpha\\) the lowest mean squared error occurred. Note that sklearn uses the convention that \\(\\alpha\\) (rather than \\(\\lambda\\)) is the shrinkage parameter. EPSILON = 1e-4 # This is to avoid division by zero while taking the base 10 logarithm plt.figure() plt.semilogx(reg_lasso.alphas_ + EPSILON, reg_lasso.mse_path_, ':') plt.plot(reg_lasso.alphas_ + EPSILON, reg_lasso.mse_path_.mean(axis=-1), 'k', label='Average across the folds', linewidth=2) plt.axvline(reg_lasso.alpha_ + EPSILON, linestyle='--', color='k', label=r'$\\alpha$: CV estimate') plt.legend() plt.xlabel(r'$\\alpha$') plt.ylabel('Mean square error') plt.title('Mean square error on each fold: coordinate descent ') plt.axis('tight') plt.show() . Let’s look at the coefficients that are selected with this optimal value of \\(\\alpha\\) (which you can access via reg_lasso.alpha_): . for coef, name in zip(reg_lasso.coef_, dmatrix(formula.split('~')[1], iris).design_info.term_names): print(f'Coeff {name} = {coef:.2f}') . Coeff sepal_width = 0.36 Coeff petal_length = 1.38 Coeff petal_width = -0.39 Coeff sepal_width:petal_length = -0.00 Coeff sepal_width:petal_width = -0.32 Coeff petal_length:petal_width = 0.33 . Now let’s see what coefficients we get with ridge regression and elastic net (a mixture between ridge and lasso; here we use the default setting of a half-mixture between the two). reg_elastic = ElasticNetCV(cv=10).fit(scale_X, scale_y) reg_ridge = RidgeCV(cv=10).fit(scale_X, scale_y) # For convenient comparison, let's pop these into a dataframe df = pd.DataFrame({'Lasso': reg_lasso.coef_, 'Elastic Net (0.5)': reg_elastic.coef_, 'Ridge': reg_ridge.coef_}, index=dmatrix(formula.split('~')[1], iris).design_info.term_names).T df[r'$\\alpha$'] = [reg_lasso.alpha_, reg_elastic.alpha_, reg_ridge.alpha_] df = df.T df . | | Lasso | Elastic Net (0.5) | Ridge | . | sepal_width | 0.362891 | 0.357877 | 0.288003 | . | petal_length | 1.383851 | 1.321840 | 0.931508 | . | petal_width | -0.386780 | -0.320669 | -0.148416 | . | sepal_width:petal_length | -0.000000 | 0.039810 | 0.363751 | . | sepal_width:petal_width | -0.322053 | -0.362515 | -0.497244 | . | petal_length:petal_width | 0.327846 | 0.321951 | 0.326384 | . | α | 0.000901 | 0.001802 | 1.000000 | . ",
    "url": "/Machine_Learning/penalized_regression.html#python",
    
    "relUrl": "/Machine_Learning/penalized_regression.html#python"
  },"586": {
    "doc": "Penalized Regression",
    "title": "R",
    "content": "We will use the glmnet package. # Install glmnet and tidyverse if necessary # install.packages('glmnet', 'tidyverse') # Load glmnet library(glmnet) # Load iris data data(iris) # Create a matrix with all variables other than our dependent vairable, Sepal.Length # and interactions. # -1 to omit the intercept M &lt;- model.matrix(lm(Sepal.Length ~ (.)^2 - 1, data = iris)) # Add squared terms of numeric variables numeric.var.names &lt;- names(iris)[2:4] M &lt;- cbind(M,as.matrix(iris[,numeric.var.names]^2)) colnames(M)[16:18] &lt;- paste(numeric.var.names,'squared') # Create a matrix for our dependent variable too Y &lt;- as.matrix(iris$Sepal.Length) # Standardize all variables M &lt;- scale(M) Y &lt;- scale(Y) # Use glmnet to estimate penalized regression # We pick family = \"gaussian\" for linear regression; # other families work for other kinds of data, like binomial for binary data # In each case, we use cv.glmnet to pick our lambda value using cross-validation # using nfolds folds for cross-validation # Note that alpha = 1 picks LASSO cv.lasso &lt;- cv.glmnet(M, Y, family = \"gaussian\", nfolds = 20, alpha = 1) # We might want to see how the choice of lambda relates to out-of-sample error with a plot plot(cv.lasso) # After doing CV, we commonly pick the lambda.min for lambda, # which is the lambda that minimizes out-of-sample error # or lambda.1se, which is one standard error above lambda.min, # which penalizes more harshly. The choice depends on context. lasso.model &lt;- glmnet(M, Y, family = \"gaussian\", alpha = 1, lambda = cv.lasso$lambda.min) # coefficients are shown in the beta element. means LASSO dropped it lasso.model$beta # Running Ridge, or mixing the two with elastic net, simply means picking # alpha = 0 (Ridge), or 0 &lt; alpha &lt; 1 (Elastic Net) cv.ridge &lt;- cv.glmnet(M, Y, family = \"gaussian\", nfolds = 20, alpha = 0) ridge.model &lt;- glmnet(M, Y, family = \"gaussian\", alpha = 0, lambda = cv.ridge$lambda.min) cv.elasticnet &lt;- cv.glmnet(M, Y, family = \"gaussian\", nfolds = 20, alpha = .5) elasticnet.model &lt;- glmnet(M, Y, family = \"gaussian\", alpha = .5, lambda = cv.elasticnet$lambda.min) . ",
    "url": "/Machine_Learning/penalized_regression.html#r",
    
    "relUrl": "/Machine_Learning/penalized_regression.html#r"
  },"587": {
    "doc": "Penalized Regression",
    "title": "Stata",
    "content": "Penalized regression is one of the few machine learning algorithms that Stata does natively. This requires Stata 16. If you do not have Stata 16, you can alternately perform some forms of penalized regression by installing the lars package using ssc install lars. * Use NLSY-W data sysuse nlsw88.dta, clear * Construct all squared and interaction terms by loop so we don't have to specify them all * by hand in the regression function local numeric_vars = \"age grade hours ttl_exp tenure\" local factor_vars = \"race married never_married collgrad south smsa c_city industry occupation union\" * Add all squares foreach x in `numeric_vars' { g sq_`x' = `x'^2 } * Turn all factors into dummies so we can standardize them local faccount = 1 local dummy_vars = \"\" foreach x in `factor_vars' { xi i.`x', pre(f`count'_) local count = `count' + 1 } * Add all numeric-numeric interactions; these are easy * factor interactions would need a more thorough loop forvalues i = 1(1)5 { local next_i = `i'+1 forvalues j = `next_i'(1)5 { local namei = word(\"`numeric_vars'\",`i') local namej = word(\"`numeric_vars'\",`j') g interact_`i'_`j' = `namei'*`namej' } } * Standardize everything foreach var of varlist `numeric_vars' f*_* interact_* { qui summ `var' qui replace `var' = (`var' - r(mean))/r(sd) } * Use the lasso command to run LASSO * using sel(cv) to select lambda using cross-validation * we specify a linear model here, but logit/probit/poisson would work lasso linear wage `numeric_vars' f*_* interact_*, sel(cv) * get list of included coefficients lassocoef * We can use elasticnet to run Elastic Net * By default, alpha will be selected by cross-validation as well elasticnet linear wage `numeric_vars' f*_* interact_*, sel(cv) . ",
    "url": "/Machine_Learning/penalized_regression.html#stata",
    
    "relUrl": "/Machine_Learning/penalized_regression.html#stata"
  },"588": {
    "doc": "Probit Model",
    "title": "Probit Regressions",
    "content": "A Probit regression is a statistical method for a best-fit line between a binary [0/1] outcome variable \\(Y\\) and any number of independent variables. Probit regressions follow a standard normal probability distribution and the predicted values are bounded between 0 and 1. For more information about Probit, see Wikipedia: Probit. ",
    "url": "/Model_Estimation/GLS/probit_model.html#probit-regressions",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#probit-regressions"
  },"589": {
    "doc": "Probit Model",
    "title": "Keep in Mind",
    "content": ". | The beta coefficients from a probit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of \\(X\\) on \\(Y\\). | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the probit beta coefficient by 2.5. | . ",
    "url": "/Model_Estimation/GLS/probit_model.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#keep-in-mind"
  },"590": {
    "doc": "Probit Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/probit_model.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#implementations"
  },"591": {
    "doc": "Probit Model",
    "title": "Gretl",
    "content": "# Load auto data open auto.gdt # Run probit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors probit mpg const headroom trunk weight . ",
    "url": "/Model_Estimation/GLS/probit_model.html#gretl",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#gretl"
  },"592": {
    "doc": "Probit Model",
    "title": "Python",
    "content": "The statsmodels package has methods that can perform probit regressions. # Use pip or conda to install pandas and statsmodels import pandas as pd import statsmodels.formula.api as smf # Read in the data df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv', index_col=0) # Specify the model mod = smf.probit('vs ~ mpg + cyl', data=df) # Fit the model res = mod.fit() # Look at the results res.summary() # Compute marginal effects marge_effect = res.get_margeff(at='mean', method='dydx') # Show marginal effects marge_effect.summary() . ",
    "url": "/Model_Estimation/GLS/probit_model.html#python",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#python"
  },"593": {
    "doc": "Probit Model",
    "title": "R",
    "content": "R can run a probit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. # If necessary, install the mfx package # install.packages('mfx') # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run probit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_probit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = 'probit')) # The family argument says we are working with binary data # and using a probit link function (rather than, say, logit) # The results summary(my_probit) # Marginal effects probitmfx(vs ~ mpg + cyl, data = mtcars) . ",
    "url": "/Model_Estimation/GLS/probit_model.html#r",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#r"
  },"594": {
    "doc": "Probit Model",
    "title": "Stata",
    "content": "* Load auto data sysuse auto.dta * Probi Estimation probit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) . ",
    "url": "/Model_Estimation/GLS/probit_model.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html#stata"
  },"595": {
    "doc": "Probit Model",
    "title": "Probit Model",
    "content": " ",
    "url": "/Model_Estimation/GLS/probit_model.html",
    
    "relUrl": "/Model_Estimation/GLS/probit_model.html"
  },"596": {
    "doc": "Propensity Score Matching",
    "title": "Propensity Score Matching",
    "content": "Propensity Score Matching (PSM) is a non-parametric method of estimating a treatment effect in situations where randomization is not possible. This method comes from Rosenbaum &amp; Rubin, 1983 and works by estimating a propensity score which is the predicted probability that someone received treatment based on the explanatory variables of interest. As long as all confounders are included in the propensity score estimation, this reduces bias in observational studies by controlling for variation in treatment that is driven by confounding, essentially attempting to replicate a randomized control trial. ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html"
  },"597": {
    "doc": "Propensity Score Matching",
    "title": "Inverse Probability Weighting",
    "content": "The recommendation of the current literature, by King and Nielsen 2019, is that propensity scores should be used with inverse probability weighting (IPW) rather than matching. With this in mind, there will be examples of how to implement IPWs first followed by the process for implementing a matching method. ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#inverse-probability-weighting",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#inverse-probability-weighting"
  },"598": {
    "doc": "Propensity Score Matching",
    "title": "Workflow for Inverse Probability Weighting",
    "content": ". | Run a logistic regression where the outcome variable is a binary indicator for whether or not someone received the treatment, and gather the predicted value of the propensity score. The explanatory variables in this case are the covariates that we might reasonably believe influence treatment | Filter out observations in our data that are not inside the range of our propensity scores, or that have extremely high or low values. | Create the inverse probability weights | Run a regression using the IPWs | . ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#workflow-for-inverse-probability-weighting",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#workflow-for-inverse-probability-weighting"
  },"599": {
    "doc": "Propensity Score Matching",
    "title": "Workflow for Matching",
    "content": ". | The same as step one from the IPW section. | Match those that received treatment with those that did not based on propensity score. There are a number of different ways to perform this matching including, but not limited to : | . | Nearest neighbor matching | Exact matching | Stratification matching | . In this example we will focus on nearest neighbor matching. ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#workflow-for-matching",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#workflow-for-matching"
  },"600": {
    "doc": "Propensity Score Matching",
    "title": "Checking Balance",
    "content": "Unlike methods like Entropy Balancing and Coarsened Exact Matching, propensity score approaches do not ensure that the covariates are balanced between the treated and control groups. It is a good idea to check whether decent balance has been achieved, and if it hasn’t, go back and modify the model, perhaps adding more matching variables or allowing polynomial terms in the logistic regression, until there is acceptable balance. | Check the balance of the matched sample. That is, see whether the averages (and perhaps variances and other summary statistics) of the covariates are similar in the matched/weighted treated and control groups. | In the case of inverse probability weighting, also check whether the post-weighting propensity score distributions are similar in the treated and control groups. | . Once the workflow is finished, the treatment effect can be estimated using the treated and matched sample with matching, or using the weighted sample with inverse probability weighting. ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#checking-balance",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#checking-balance"
  },"601": {
    "doc": "Propensity Score Matching",
    "title": "Keep in Mind",
    "content": ". | Propensity Score Matching is based on selection on observable characteristics. This assume that the potential outcome is independent of the treatment D conditional on the covariates, or the Conditional Independence Assumption: | . \\[Y_i(1),Y_i(0)\\bot|X_i\\] . | Propensity Score Matching also requires us to make the Common Support or Overlap Assumption: | . \\[0&lt;Pr(D_i = 1 | X_i = x)&lt;1\\] The overlap assumption says that the probability that the treatment is equal to 1 for each level of x is between zero and one, or in other words there are both treated and untreated units for each level of x. | Treatment effect estimation will produce incorrcect standard errors unless they are specifically tailored for matching results, since they will not account for noise in the matching process. Use software designed for treatment effect estimates with matching. Or, for inverse probability weighting, you can bootstrap the entire procedure (from matching to estimation) and produce standard errors that way. | . ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#keep-in-mind"
  },"602": {
    "doc": "Propensity Score Matching",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#implementations",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#implementations"
  },"603": {
    "doc": "Propensity Score Matching",
    "title": "R",
    "content": "The matching implementation will use the MatchIt package. A great place to find more information about the MatchIt package is on the package’s github site or CRAN Page. The inverse probability implementation uses the causalweight package. Here is a handy link to get more information about the causalweight package which is an alternative way of creating inverse probability weights, courtesy of Hugo Bodory and Martin Huber. Inverse Probability Weights in R . Data comes from OpenIntro.org . # First follow basic workflow without causalweights package library(pacman) p_load(tidyverse, causalweight) #Load data on smoking in the United Kingdom. smoking = read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Matching/Data/smoking.csv\") #Turn smoking and married into numeric variables smoking = smoking %&gt;% mutate(smoke = 1*(smoke == \"Yes\"), married = 1*(marital_status == \"Married\")) # Pull out the variables # Outcome Y = smoking %&gt;% pull(married) # Treatment D &lt;- smoking %&gt;% pull(smoke) # Matching variables X &lt;- model.matrix(~-1+gender+age+marital_status+ethnicity+region, data = smoking) # Note this estimats the propensity score for us, trims propensity # scores based on extreme values, # and then produces appropriate bootstrapped standard errors IPW &lt;- treatweight(Y, D, X, trim = .001, logit = TRUE) # Estimate and SE IPW$effect IPW$se . Matching in R . ##load the packages and data we need. library(pacman) p_load(tidyverse, MatchIt) smoking = read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Matching/Data/smoking.csv\") smoking = smoking %&gt;% mutate(smoke = 1*(smoke == \"Yes\")) # Mapping the categories to new categorical values 1 to 8 and giving NA to \"Refused\" and \"Unknown\" smoking$new_income &lt;- NA smoking$new_income[smoking$gross_income == \"Under 2,600\"] &lt;- 1 smoking$new_income[smoking$gross_income == \"2,600 to 5,200\"] &lt;- 2 smoking$new_income[smoking$gross_income == \"5,200 to 10,400\"] &lt;- 3 smoking$new_income[smoking$gross_income == \"10,400 to 15,600\"] &lt;- 4 smoking$new_income[smoking$gross_income == \"15,600 to 20,800\"] &lt;- 5 smoking$new_income[smoking$gross_income == \"20,800 to 28,600\"] &lt;- 6 smoking$new_income[smoking$gross_income == \"28,600 to 36,400\"] &lt;- 7 smoking$new_income[smoking$gross_income == \"Above 36,400\"] &lt;- 8 smoking$new_income[smoking$gross_income == \"Refused\"] &lt;- NA smoking$new_income[smoking$gross_income == \"Unknown\"] &lt;- NA ##Step One: Run the logistic regression. ps_model = glm(smoke ~ gender+age+marital_status+ethnicity+region, data=smoking) ##Step Two: Match on propensity score. #Does not apply in this situation, but need to make sure there are no missing values in the covariates we are choosing. #In order to match use the matchit command, passing the function a formula, the data to use and the method, in this case, nearest neighor estimation. Match = matchit(smoke ~ gender+age+marital_status+ethnicity+region, method = \"nearest\", data =smoking) ##Step Three: Check for Balance. summary(match) ##Create a data frame from matches using the match.data function. match_data = match.data(match) #Check the dimensions. dim(match_data) ##Step Four: Conduct Analysis using the new sample. ##We can now get the treatment effect of smoking on gross income with and without controls # Note these standard errors will be incorrect, see Caliendo and Kopeinig (2008) for fixes # https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-6419.2007.00527.x lm_nocontrols = lm(new_income ~ smoke, data= match_data) #With controls, standard errors also wrong here ##Turn marital status into a factor variable so that we can use it in our regression match_data = match_data %&gt;% mutate(marital_status = as.factor(marital_status)) lm_controls =lm(new_income ~ smoke+age+gender+ethnicity+marital_status, data=match_data) . ",
    "url": "/Model_Estimation/Matching/propensity_score_matching.html#r",
    
    "relUrl": "/Model_Estimation/Matching/propensity_score_matching.html#r"
  },"604": {
    "doc": "Quantile Regression",
    "title": "Quantile Regression",
    "content": "Quantile Regression is an extension of linear regression analysis. Quantile Regression differs from OLS in how it estimates the response variable. OLS estimates the conditional mean of \\(Y\\) across the predictor variables (\\(X_1, X_2, X_3...\\)), whereas quantile regression estimates the conditional median (or quantiles) of \\(Y\\) across the predictor variables (\\(X_1, X_2, X_3...\\)). It is useful in situations where OLS assumptions are not met (heteroskedasticity, bi-modal or skewed distributions). To specify the desired quantile, select a \\(\\tau\\) value between 0 to 1 (.5 gives the median). For more information on Quantile Regression, see Wikipedia: Quantile Regression . ",
    "url": "/Model_Estimation/GLS/quantile_regression.html",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html"
  },"605": {
    "doc": "Quantile Regression",
    "title": "Keep in Mind",
    "content": ". | This method allows for the dependent variable to have any distributional form, however it cannot be a dummy variable and must be continuous. | This method is robust to outliers, so there is no need to remove outlier observations. | Either the intercept term or at least one predictor is required to run an analysis. | LASSO regression cannot be used for feature selection in this framework due to it requiring OLS assumptions to be satisfied. | This method does not restrict the use of polynomial or interaction terms. A unique functional form can be specified. | . ",
    "url": "/Model_Estimation/GLS/quantile_regression.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html#keep-in-mind"
  },"606": {
    "doc": "Quantile Regression",
    "title": "Also Consider",
    "content": ". | While Quantile Regression can be useful in applications where OLS assumptions are not met, it can actually be used to detect heteroskedasticity. This makes is a useful tool to ensure this assumption is met for OLS. | Several different standard error calculations can be used with this method, however bootstrapped standard errors are generally the best for complex modeling situations. Clustered standard errors are also possible by estimating a quantile regression with pooled OLS clustered errors. | . ",
    "url": "/Model_Estimation/GLS/quantile_regression.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html#also-consider"
  },"607": {
    "doc": "Quantile Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/quantile_regression.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html#implementations"
  },"608": {
    "doc": "Quantile Regression",
    "title": "Python",
    "content": "The quantreg function in statsmodels allows for quantile regression. import statsmodels.api as sm import statsmodels.formula.api as smf mtcars = sm.datasets.get_rdataset(\"mtcars\", \"datasets\").data mod = smf.quantreg('mpg ~ cyl + hp + wt', mtcars) # Specify the quantile when you fit res = mod.fit(q=.2) print(res.summary()) . ",
    "url": "/Model_Estimation/GLS/quantile_regression.html#python",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html#python"
  },"609": {
    "doc": "Quantile Regression",
    "title": "R",
    "content": "The main package to implement Quantile Regression in R is through the quantreg package. The main function in this package is qr(), which fits a Quantile Regression model with a default \\(\\tau\\) value of .5 but can be changed. # Load package library(quantreg) # Load data data(mtcars) # Run quantile regression with mpg as outcome variable # and cyl, hp, and wt as predictors # Using a tau value of .2 for quantiles quantreg_model = rq(mpg ~ cyl + hp + wt, data = mtcars, tau = .2) # Look at results summary(quantreg_model) . ",
    "url": "/Model_Estimation/GLS/quantile_regression.html#r",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html#r"
  },"610": {
    "doc": "Quantile Regression",
    "title": "Stata",
    "content": "Quantile regression can be performed in Stata using the qreg function. By default it fits a median (q(.5)). See help qreg for some variants, including a bootstrapped quantile regression bsqreg. sysuse auto qreg mpg price trunk weight, q(.2) . ",
    "url": "/Model_Estimation/GLS/quantile_regression.html#stata",
    
    "relUrl": "/Model_Estimation/GLS/quantile_regression.html#stata"
  },"611": {
    "doc": "Random Forest",
    "title": "Random Forest",
    "content": "Random forest is one of the most popular and powerful machine learning algorithms. A random forest works by building up a number of decision trees, each built using a bootstrapped sample and a subset of the variables/features. Each node in each decision tree is a condition on a single feature, selecting a way to split the data so as to maximize predictive accuracy. Each individual tree gives a classification. The average, or vote-counting of that classification across trees provides an overall prediction. More trees in the forest are associated with higher accuracy. A random forest classifier can be used for both classification and regression tasks. In terms of regression, it takes the average of the outputs by different trees. Random forest can work with large datasets with multiple dimensions. However, it may overfit data, especially for regression problems. ",
    "url": "/Machine_Learning/random_forest.html",
    
    "relUrl": "/Machine_Learning/random_forest.html"
  },"612": {
    "doc": "Random Forest",
    "title": "Keep in Mind",
    "content": ". | Individual features need to have low correlations with each other, and sometimes we may remove features that are strongly correlated with other features. | Random forest can deal with missing values, and may simply treat “missing” as another value that the variable can take. | . ",
    "url": "/Machine_Learning/random_forest.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/random_forest.html#keep-in-mind"
  },"613": {
    "doc": "Random Forest",
    "title": "Also Consider",
    "content": ". | If you are not familiar with decision tree, please go to the decision tree page first as decision trees are building blocks of random forests. | . ",
    "url": "/Machine_Learning/random_forest.html#also-consider",
    
    "relUrl": "/Machine_Learning/random_forest.html#also-consider"
  },"614": {
    "doc": "Random Forest",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/random_forest.html#implementations",
    
    "relUrl": "/Machine_Learning/random_forest.html#implementations"
  },"615": {
    "doc": "Random Forest",
    "title": "Python",
    "content": "Random forests can be used to perform both regression and classification tasks. In the example below, we’ll use the RandomForestClassifier from the popular sklearn machine learning library. RandomForestClassifier is an ensemble function that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. We’ll use this classifier to predict the species of iris based on its properties, using data from the iris dataset. You may need to install packages on the command line, using pip install package-name or conda install package-name, to run these examples (if you don’t already have them installed). import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Read data df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\") # Prepare data X = df[[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"]] y = df[[\"Species\"]] # Split data into training and test set X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1996 ) # Creating model using random forest model = RandomForestClassifier(max_depth=2) model.fit(X_train, y_train) # Predict values for test data y_pred = model.predict(X_test) # Evaluate model prediction print(f\"Accuracy is {accuracy_score(y_pred, y_test)*100:.2f} %\") . ",
    "url": "/Machine_Learning/random_forest.html#python",
    
    "relUrl": "/Machine_Learning/random_forest.html#python"
  },"616": {
    "doc": "Random Forest",
    "title": "R",
    "content": "There are a number of packages in R capable of training a random forest, including randomForest and ranger. Here we will use randomForest. We’ll be using a built-in dataset in R, called “Iris”. There are five variables in this dataset, including species, petal width and length as well as sepal length and width. #Load packages library(tidyverse) library(rvest) library(dplyr) library(caret) library(randomForest) library(Metrics) library(readr) #Read data in R data(iris) iris #Create features and target X &lt;- iris %&gt;% select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) y &lt;- iris$Species #Split data into training and test sets index &lt;- createDataPartition(y, p=0.75, list=FALSE) X_train &lt;- X[ index, ] X_test &lt;- X[-index, ] y_train &lt;- y[index] y_test&lt;-y[-index] #Train the model iris_rf &lt;- randomForest(x = X_train, y = y_train , maxnodes = 10, ntree = 10) print(iris_rf) #Make predictions predictions &lt;- predict(iris_rf, X_test) result &lt;- X_test result['Species'] &lt;- y_test result['Prediction']&lt;- predictions head(result) #Check the classification accuracy (number of correct predictions out of total datapoints used to test the prediction) print(sum(predictions==y_test)) print(length(y_test)) print(sum(predictions==y_test)/length(y_test)) . ",
    "url": "/Machine_Learning/random_forest.html#r",
    
    "relUrl": "/Machine_Learning/random_forest.html#r"
  },"617": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Random/Mixed Effects in Linear Regression",
    "content": "In panel data, we often have to deal with unobserved heterogeneity among the units of observation that are observed over time. If we assume that the unobserved heterogeneity is uncorrelated with the independent variables, we can use random effects model. Otherwise, we may consider fixed effects. In practice, random effects and fixed effects are often combined to implement a mixed effects model. Mixed refers to the fact that these models contain both fixed, and random effects. For more information, see Wikipedia: Random Effects Model . ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html"
  },"618": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Keep in Mind",
    "content": ". | To use random effects model, you must observe the same person multiple times (panel data). | If unobserved heterogeneity is correlated with independent variables, the random effects estimator is biased and inconsistent. | However, even if unobserved heterogeneity is expected to be correlated with independent variables, the fixed effects model may have high standard errors if the number of observation per unit of observation is very small. Random effects maybe considered in such cases. | Additionally, modeling the correlation between the indepdendent variables and the random effect by using variables in predicting the random effect can account for this problem | . ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#keep-in-mind"
  },"619": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Also Consider",
    "content": ". | Consider Fixed effects if unobserved heterogeneity and independent variables are correlated or if only within-variation is desired. | Hauman Tests are often used to inform us about the appropiateness of fixed effects models vs. random effects models in which only the intercept is random. | Clustering your error | . ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#also-consider",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#also-consider"
  },"620": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Implementations",
    "content": "We continue from our the example in Fixed effects. In that example we estimated a fixed effect model of the form: . \\[earnings_{it} = \\beta_0 + \\beta_1 prop\\_ working_{it} + \\delta_t + \\delta_i + \\epsilon_{it}\\] That is, average earnings of graduates of an institution depends on proportion employed, after controlling for time and institution fixed effects. But, some institutions have one observation, and the average number of observations is 5.1. We may be worried about the precision of our estimates. So, we may choose to use random effects for intercepts by institution to estimate the model even if we think \\(corr(prop\\_ working_{it}, \\delta_{i}) \\ne 0\\). That is, we choose possiblity of bias over variance. ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#implementations",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#implementations"
  },"621": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "R",
    "content": "Several packages can be used to implement a random effects model in R - such as lme4 and nlme. lme4 is more widely used. The example that follows uses the lme4 package. # If necessary, install lme4 if(!require(lme4)){install.packages(\"lme4\")} library(lme4) # Read in data from the College Scorecard df &lt;- read.csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # We write the mixed effect formula for estimation in lme4 as: # dependent_var ~ # covariates (that can include fixed effects) + # random effects - we need to specify if our model is random effects in intercepts or in slopes. In our example, we suspect random effects in intercepts at institutions. So we write \"...+(1 | inst_name), ....\" If we wanted to specify a model where the coefficient on prop_working was also varying by institution - we would use (1 + open | inst_name). # Here we regress average earnings graduates in an institution on prop_working, year fixed effects and random effects in intercepts for institutions. relm_model &lt;- lmer(earnings_med ~ prop_working + factor(df$year) + (1 | inst_name), data = df) # Display results summary(relm_model) # We note that comparing with the fixed effects model, our estimates are more precise. But, the correlation between X`s and errors suggest bias in our mixed effect model, and we do see a large increase in estimated beta. ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#r",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#r"
  },"622": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Stata",
    "content": "We will estimate a mixed effects model using Stata using the built in xtreg command. * Obtain same data from Fixed Effect tutorial import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fix ed_Effects_in_Linear_Regression/Scorecard.csv\", clear * Data cleaning * We are turning missings are written as \"NA\" into numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the \"re\" option to run random effects on institution intercepts * Regressing earnings_med on prop_working * with random effects for name_number (implied by re) * and also year fixed effects (which we'll add manually with i.year) xtreg earnings_med prop_working i.year, re * We note that comparing with the fixed effects model, our estimates are more precise. But, correlation between X`s and errors suggest bias in our random effect model, and we do see a large increase in estimated beta. ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#stata",
    
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#stata"
  },"623": {
    "doc": "Regression Discontinuity Design",
    "title": "Regression Discontinuity Design",
    "content": "Regression discontinuity (RDD) is a research design for the purposes of causal inference. It can be used in cases where treatment is assigned based on a cutoff value of a “running variable”. For example, perhaps students in a school take a test in 8th grade. Students who score 30 or below are assigned to remedial classes, while students scoring above 30 stay in regular classes. Regression discontinuity could be applied to this setting with test score as a running variable and 30 as the cutoff to look at the effects of remedial classes. Regression discontinuity works by focusing on the cutoff. It makes an estimate of what the outcome is within a narrow bandwidth to the left of the cutoff, and also makes an estimate of what the outcome is to the right of the cutoff. Then it compares them to generate a treatment effect estimate. See Wikpedia: Regression Discontinuity Design for more information. Regression discontinuity receives a lot of attention because it relies on what some consider to be plausible assumptions. If the running variable is finely measured and is not being manipulated, then one can argue that being just to the left or the right of a cutoff is effectively random (someone getting a 30 or 31 on the test can basically be down to bad luck on the day) and so this approach by itself can remove confounding from lots of factors. ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html"
  },"624": {
    "doc": "Regression Discontinuity Design",
    "title": "Keep in Mind",
    "content": ". | There are many, many options to choose when performing an RDD. Bandwidth selection procedure, polynomial terms, bias correction, etc. etc.. Please check the help file for your command of choice closely, and ensure you know what kind of analysis you’re about to run. Don’t assume the defaults are correct. | Regression discontinuity relies on the absence of manipulation of the running variable. In the test score example, if the teachers scoring the exam nudge a few students from 30 to 31 so they can avoid remedial classes, RDD doesn’t work any more. | Because the method relies on isolating a narrow bandwidth around the cutoff, RDD doesn’t work quite the same if the running variable is discrete and split into a small number of groups. You want a running variable with a lot of different values! See Kolesár and Rothe (2018) for more information. | In order to improve statistical performance, regression discontinuity designs often incorporate information from data points far away from the cutoff to improve the estimate of what the outcome is near the cutoff. This can be done nonparametrically, but is most often done by fitting a separate polynomial function for the running variable on either side of the cutoff. A temptation is to use a very high-order polynomial (say, \\(x, x^2, x^3, x^4\\) and \\(x^5\\)) to improve fit. However, in general a low-order polynomial is probably a better idea. See Gelman and Imbens 2019 for more information. | Regression discontinuity designs are very well-suited to graphical demonstrations of the method. Software packages designed for RDD specifically will almost always provide an easy method for creating these graphs, and it is rare that you will not want to do this. However, do keep in mind that graphs can sometimes obscure meaningfully large effects. See Kirabo Jackson for an explanation. | Regression discontinuities can be sharp, where everyone to one side of the cutoff is treated and nobody on the other side is, or fuzzy, where the probability of treatment changes across the cutoff but assignment isn’t perfect. Most RDD packages can handle both. The intuition for both is similar, but the statistical properties of sharp designs are generally stronger. Fuzzy RDD can be thought of as similar to using an instrumental variables estimator in a case of imperfect random assignment in an experiment. Covariates are generally not necessary in a sharp RDD but may be advisable in a fuzzy one. | . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#keep-in-mind"
  },"625": {
    "doc": "Regression Discontinuity Design",
    "title": "Also Consider",
    "content": ". | The Regression Kink Design is an extension of RDD that looks for a change in a relationship between the running variable and the outcome, i.e. the slope, at the cutoff, rather than a change in the predicted outcome. | It is common to run a Density Discontinuity Test to check for manipulation in the running vairiable before performing a regression discontinuity. | Regression discontinuity designs are often accompanied by placebo tests, where the same RDD is run again, but with a covariate or some other non-outcome measure used as the outcome. If the RDD shows a significant effect for the covariates, this suggests that balancing did not occur properly and there may be an issue with the RDD assumptions. | Part of performing an RDD is selecting a bandwidth around the cutoff to focus on. This can be done by context, but more commonly there are data-based methods for selecting a bandwidth Check your RDD command of choice to see what methods are available for selecting a bandwidth. | . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#also-consider",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#also-consider"
  },"626": {
    "doc": "Regression Discontinuity Design",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#implementations",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#implementations"
  },"627": {
    "doc": "Regression Discontinuity Design",
    "title": "Stata",
    "content": "A standard package for performing regression discontinuity in Stata is rdrobust, installable from scc. * If necessary * ssc install rdrobust * Load RDD of house elections from the R package rddtools, * and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Regression_Discontinuity_Design/house.csv\", clear * x is \"vote margin in the previous election\" and y is \"vote margin in this election\" * If we want to specify options for bandwidth selection, we can run rdbwselect directly. * Otherwise, rdrobust will run it with default options by itself * c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) rdbwselect y x, c(0) * Run a sharp RDD with a second-order polynomial term rdrobust y x, c(0) p(2) * Run a fuzzy RDD * We don't have a fuzzy RDD in this data, but let's create one, where * probability of treatment jumps from 20% to 60% at the cutoff g treatment = (runiform() &lt; .2)*(x &lt; 0) + (runiform() &lt; .6)*(x &gt;= 0) rdrobust y x, c(0) fuzzy(treatment) * Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot y x, c(0) p(2) . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#stata",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#stata"
  },"628": {
    "doc": "Regression Discontinuity Design",
    "title": "R",
    "content": "There are several packages in R designed for the estimation of RDD. Three prominent options are rdd, rddtools, and rdrobust. See this article for comparisons between them in terms of their strengths and weaknesses. The article, considering the verisons of the packages available in 2017, recommends rddtools for assumption and sensitivity checks, and rdrobust for bandwidth selection and treatment effect estimation. We will consider rdrobust here. See the rddtools walkthrough for a detailed example of the use of rddtools. # If necessary # install.packages('rdrobust') library(rdrobust) # Load RDD of house elections from the R package rddtools, # and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 df &lt;- read.csv(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Regression_Discontinuity_Design/house.csv\") # x is \"vote margin in the previous election\" and y is \"vote margin in this election\" # If we want to specify options for bandwidth selection, we can run rdbwselect directly. # Otherwise, rdrobust will run it with default options by itself # c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) bandwidth &lt;- rdbwselect(df$y, df$x, c=0) # Run a sharp RDD with a second-order polynomial term rdd &lt;- rdrobust(df$y, df$x, c=0, p=2) summary(rdd) # Run a fuzzy RDD # We don't have a fuzzy RDD in this data, but let's create one, where # probability of treatment jumps from 20% to 60% at the cutoff N &lt;- nrow(df) df$treatment &lt;- (runif(N) &lt; .2)*(df$x &lt; 0) + (runif(N) &lt; .6)*(df$x &gt;= 0) rddfuzzy &lt;- rdrobust(df$y, df$x, c=0, p=2, fuzzy = df$treatment) summary(rddfuzzy) # Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot(df$y, df$x, c = 0, p = 2) . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#r",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#r"
  },"629": {
    "doc": "Regression Discontinuity Design",
    "title": "Stata",
    "content": "A standard package for performing regression discontinuity in Stata is rdrobust, installable from scc. * If necessary * ssc install rdrobust * Load RDD of house elections from the R package rddtools, * and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Regression_Discontinuity_Design/house.csv\", clear * x is \"vote margin in the previous election\" and y is \"vote margin in this election\" * If we want to specify options for bandwidth selection, we can run rdbwselect directly. * Otherwise, rdrobust will run it with default options by itself * c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) rdbwselect y x, c(0) * Run a sharp RDD with a second-order polynomial term rdrobust y x, c(0) p(2) * Run a fuzzy RDD * We don't have a fuzzy RDD in this data, but let's create one, where * probability of treatment jumps from 20% to 60% at the cutoff g treatment = (runiform() &lt; .2)*(x &lt; 0) + (runiform() &lt; .6)*(x &gt;= 0) rdrobust y x, c(0) fuzzy(treatment) * Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot y x, c(0) p(2) . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#stata-1",
    
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#stata-1"
  },"630": {
    "doc": "Reshaping Data",
    "title": "Reshaping Data",
    "content": " ",
    "url": "/Data_Manipulation/Reshaping/reshape.html",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape.html"
  },"631": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Reshape Panel Data from Long to Wide",
    "content": "Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . | Individual | FixedCharacteristic | TimeVarying1990 | TimeVarying1991 | TimeVarying1992 | . | 1 | C | 16 | 20 | 22 | . | 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. In long format, there is one row per individual per time period: . | Individual | FixedCharacteristic | Year | TimeVarying | . | 1 | C | 1990 | 16 | . | 1 | C | 1991 | 20 | . | 1 | C | 1992 | 22 | . | 2 | H | 1990 | 23.4 | . | 2 | H | 1991 | 10 | . | 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. Reshaping is the method of converting wide-format data to long and vice versa. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html"
  },"632": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Keep in Mind",
    "content": ". | If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#keep-in-mind"
  },"633": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Also Consider",
    "content": ". | To go in the other direction, reshape from wide to long. | Determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#also-consider",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#also-consider"
  },"634": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#implementations",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#implementations"
  },"635": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Python",
    "content": "The pandas package has several functions to reshape data. For going from long data to wide data, there’s pivot and pivot_table, both of which are demonstrated in the example below. # Install pandas using pip or conda, if you don't already have it installed. import pandas as pd # Load WHO data on population as an example, which has 'country', 'year', # and 'population' columns. df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/tidyr/population.csv', index_col=0) # In this example, we would like to have one row per country but the data have # multiple rows per country, each corresponding with # a year-country value of population. # Let's take a look at the first 5 rows: print(df.head()) # To reshape this into a dataframe with one country per row, we can use # the pivot function and set 'country' as the index. As we'd like to # split out years into different columns, we set columns to 'years', and the # values within this new dataframe will be population: df_wide = df.pivot(index='country', columns='year', values='population') # What if there are multiple year-country pairs? Pivot can't work # because it needs unique combinations. In this case, we can use # pivot_table which can aggregate any duplicate year-country pairs. To test it, let's # create some synthetic duplicate data for France and add it to the original # data. We'll pretend there was a second count of population that came in with # 5% higher values for all years. # Copy the data for France synth_fr_data = df.loc[df['country'] == 'France'] # Add 5% for all years synth_fr_data['population'] = synth_fr_data['population']*1.05 # Append it to the end of the original data df = pd.concat([df, synth_fr_data], axis=0) # Compute the wide data - averaging over the two estimates for France for each # year. df_wide = df.pivot_table(index='country', columns='year', values='population', aggfunc='mean') . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#python",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#python"
  },"636": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "R",
    "content": "There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. We will be using the tidyr package function pivot_wider, which requires tidyr version 1.0.0 or later. # install.packages('tidyr') library(tidyr) # Load in population, which has one row per country per year data(\"population\") # If we look at the data, we'll see that we have: # identifying information in \"country\", # a time indicator in \"year\", # and our values in \"population\" head(population) . Now we think: . | Think about the set of variables that contain the values we’re interested in reshaping. Here’s it’s population. This list of variable names will be our values_from argument. | Think about what we want the new variables to be called. The variable variable says which variable we’re looking at. So that will be our names_from argument. And we want to specify that each variable represents population in a given year (rather than some other variable, so we’ll add “pop_” as our names_prefix. | . pop_wide &lt;- pivot_wider(population, names_from = year, values_from = population, names_prefix = \"pop_\") . Another way to do this is using data.table. #install.packages('data.table') library(data.table) # The second argument here is the formula describing the observation level of the data # The full set of variables together is the current observation level (one row per country and year) # The parts before the ~ are what we want the new observation level to be in the wide data (one row per country) # The parts after the ~ are for the variables we want to no longer be part of the observation level (we no longer want a row per year) population = as.data.table(population) pop_wide = dcast(population, country ~ year, value.var = \"population\" ) . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#r",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#r"
  },"637": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Stata",
    "content": "* Load blood pressure data in long format, which contains * blood pressure both before and after a treatment for some patients sysuse bplong.dta . The next steps involve thinking: . | Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp. This will be one of our “stub”s. | Think about which variable separates the different time periods within individual. Here we have “when”, and this goes in j(), so we have j(when). | . * Syntax is: * reshape wide stub, i(individualvars) j(newtimevar) * So we have reshape wide bp i(patient) j(when) * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. * First, we will create a toy dataset that is very large to demonstrate the speed gains * If necessary, first install gtools: * ssc install gtools * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create ten observations per person generate person_id = floor((_n-1)/10) * Number time periods from 1 to 10 for each person generate time_id = mod((_n-1), 10) + 1 *Create an income in each period generate income = round(rnormal(100, 20)) * Demonstrate the comparative speed of these two reshape approaches. * preserve and restore aren't a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. *The traditional reshape command preserve reshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command preserve greshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command, alternative syntax preserve greshape wide income, by(person_id) keys(time_id) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#stata",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#stata"
  },"638": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Reshape Panel Data from Wide to Long",
    "content": "Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . | Individual | FixedCharacteristic | TimeVarying1990 | TimeVarying1991 | TimeVarying1992 | . | 1 | C | 16 | 20 | 22 | . | 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. In long format, there is one row per individual per time period: . | Individual | FixedCharacteristic | Year | TimeVarying | . | 1 | C | 1990 | 16 | . | 1 | C | 1991 | 20 | . | 1 | C | 1992 | 22 | . | 2 | H | 1990 | 23.4 | . | 2 | H | 1991 | 10 | . | 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. Reshaping is the method of converting wide-format data to long and vice versa.. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html"
  },"639": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Keep in Mind",
    "content": ". | If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#keep-in-mind"
  },"640": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Also Consider",
    "content": ". | To go in the other direction, reshape from long to wide. | Determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#also-consider",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#also-consider"
  },"641": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#implementations",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#implementations"
  },"642": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Python",
    "content": "The most user friendly ways to use Python to reshape data from wide to long formats come from the pandas data analysis package. Its wide_to_long function is relatively easy to use, the alternative melt function can handle more complex cases. In this example, we will download the billboard dataset, which has multiple columns for different weeks when a record was in the charts (with the values in each column giving the chart position for that week). All of the columns that we would like to convert to long format begin with the prefix ‘wk’. The wide_to_long function accepts this prefix (as the stubnames= keyword parameter) and uses it to work out which columns to transform into a single column. # Install pandas using pip or conda, if you don't have it already installed import pandas as pd df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/tidyr/billboard.csv', index_col=0) # stubnames is the prefix for the columns we want to convert to long. i is the # unique id for each row, and j will be the name of the new column. Finally, # the values from the original wide columns (the chart position) adopt the # stubname, so we rename 'wk' to 'position' in the last step. long_df = (pd.wide_to_long(df, stubnames='wk', i=['artist', 'track', 'date.entered'], j='week') .rename(columns={'wk': 'position'})) # The wide_to_long function is a special case of the 'melt' function, which # can be used in more complex cases. Here we melt any columns that have the # string 'wk' in their names. In the final step, we extract the number of weeks # from the prefix 'wk' using regex. The final dataframe is the same as above. long_df = pd.melt(df, id_vars=['artist', 'track', 'date.entered'], value_vars=[x for x in df.columns if 'wk' in x], var_name='week', value_name='position') long_df['week'] = long_df['week'].str.extract(r'(\\d+)') # A more complex case taken from the pandas docs: import numpy as np # In this case, there are two different patterns in the many columns # that we want to convert to two different long columns. We can pass # stubnames a list of these prefixes. It then splits the columns that # have the year suffix into two different long columns depending on # their first letter (A or B) # Create some synthetic data df = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"}, \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"}, \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7}, \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1}, \"X\" : dict(zip(range(3), np.random.randn(3))) }) # Set an index df[\"id\"] = df.index # Wide to multiple long columns df_long = pd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\") . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#python",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#python"
  },"643": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "R",
    "content": "There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. There is also the incredibly fast data.table::melt(). We will be using the tidyr package function pivot_longer, which requires tidyr version 1.0.0 or later. # install.packages('tidyr') library(tidyr) # Load in billboard, which has one row per song # and one variable per week, for its chart position each week data(\"billboard\") # If we look at the data, we'll see that we have: # identifying information in \"artist\" and \"track\" # A variable consistent within individuals \"date.entered\" # and a bunch of variables containing position information # all named wk and then a number names(billboard) . Now we think: . | Think about the set of variables that contain time-varying information. Here’s it’s wk1-wk76. So we can give a list of all the variables we want to widen using the tidyselect helper function starts_with(): starts_with(\"wk\"). This list of variable names will be our col argument. | Think about what we want the new variables to be called. I’ll call the week time variable “week” (this will be the names_to argument), and the data values currently stored in wk1-wk76 is the “position” (values_to). | Think about the values you want to be in your new time variable. The column names are wk1-wk76 but we want the variable to have 1-76 instead, so we’ll take out the “wk” with names_prefix = \"wk\". | . billboard_long &lt;- pivot_longer(billboard, col = starts_with(\"wk\"), names_to = \"week\", names_prefix = \"wk\", values_to = \"position\", values_drop_na = TRUE) # values_drop_na says to drop any rows containing missing values of position. # If reshaping to create multiple variables, see the names_sep or names_pattern options. This task can also be done through data.table. #install.packages('data.table') library(data.table) billboard = as.data.table(billboard) billboard_long = melt(billboard, id = 1:3, na.rm=TRUE, variable.names = \"Week\", value.name = \"Position\" ) . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#r",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#r"
  },"644": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Stata",
    "content": "* Load blood pressure data in wide format, which contains * bp_before and bp_after sysuse bpwide.dta . The next steps involve thinking: . | Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp_. Note the inclusion of the _, so that “before” and “after” will be our time periods. This will be one of our “stub”s. | Think about what we want the new time variable to be called. I’ll just call it “time”, and this goes in j(), so we have j(time). | . * Syntax is: * reshape long stub, i(individualvars) j(newtimevar) * So we have reshape long bp_ i(patient) j(time) s * Where the s indicates that our time variable is a string (\"before\", \"after\") * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. * If necessary, install gtools * ssc install gtools * First, we will create a toy dataset that is very large to demonstrate the speed gains * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create an ID variable generate person_id = _n * Create 4 separate fake test scores per student generate test_score1 = round(rnormal(180, 30)) generate test_score2 = round(rnormal(180, 30)) generate test_score3 = round(rnormal(180, 30)) generate test_score4 = round(rnormal(180, 30)) * Demonstrate the comparative speed of these two reshape approaches * preserve and restore aren't a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. * The traditional reshape command preserve reshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command preserve greshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command, alternative syntax preserve greshape long test_score, by(person_id) keys(test_number) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#stata",
    
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#stata"
  },"645": {
    "doc": "Rowwise Calculations",
    "title": "Rowwise Calculations",
    "content": "When working with a table of data, it’s not uncommon to want to perform a calculations across many columns. For example, taking the mean of a bunch of columns for each row. This is generally not difficult to do by hand if the number of variables being handled is small. For example, in most software packages, you could take the mean of columns A and B for each row by just asking for (A+B)/2. This becomes more difficult, though, when the list of variables gets too long to type out by hand, or when the calculation doesn’t play nicely with being given columns. In these cases, approaches explicitly designed for rowwise calculations are necessary. ",
    "url": "/Data_Manipulation/rowwise_calculations.html",
    
    "relUrl": "/Data_Manipulation/rowwise_calculations.html"
  },"646": {
    "doc": "Rowwise Calculations",
    "title": "Keep in Mind",
    "content": ". | When incorporating lots of variables, rowwise calculations often allow you to select those variables by group, such as “all variables starting with r_”. When doing this, check ahead of time to make sure you aren’t accidentally incorporating unintended variables. | . ",
    "url": "/Data_Manipulation/rowwise_calculations.html#keep-in-mind",
    
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#keep-in-mind"
  },"647": {
    "doc": "Rowwise Calculations",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/rowwise_calculations.html#implementations",
    
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#implementations"
  },"648": {
    "doc": "Rowwise Calculations",
    "title": "Python",
    "content": "The pandas data analysis package provides several methods for performing row-wise (or column-wise) operations in Python. Many common operations, such as sum and mean, can be called directly (eg summing over multiple columns to create a new column). It’s useful to know the axis convention in pandas: operations that combine columns often require the user to pass axis=1 to the function, while operations that combine rows require axis=0. This convention follows the usual one for matrices of denoting individual elements first by the ith row and then by the jth column. Although not demonstrated in the example below, lambda functions can be used for more complex operations that aren’t built-in and apply to multiple rows or columns. # If necessary, install pandas using pip or conda import pandas as pd # Grab the data df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/midwest.csv\", index_col=0) # Let's assume that we want to sum, row-wise, every column # that contains 'perc' in its column name and check that # the total is 300. Use a list comprehension to get only # relevant columns, sum across them (axis=1), and create a # new column to store them: df['perc_sum'] = df[[x for x in df.columns if 'perc' in x]].sum(axis=1) # We can now check whether, on aggregate, each row entry of this new column # is 300 (it's not!) df['perc_sum'].describe() . ",
    "url": "/Data_Manipulation/rowwise_calculations.html#python",
    
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#python"
  },"649": {
    "doc": "Rowwise Calculations",
    "title": "R",
    "content": "There are a few ways to perform rowwise operations in R. If you are summing the columns or taking their mean, rowSums and rowMeans in base R are great. For something more complex, apply in base R can perform any necessary rowwise calculation, but pmap in the purrr package is likely to be faster. In all cases, the tidyselect helpers in the dplyr package can help you to select many variables by name. # If necessary # install.packages(c('purrr','ggplot2','dplyr')) # ggplot2 is only for the data data(midwest, package = 'ggplot2') # dplyr is for the tidyselect functions, the pipe %&gt;%, and select() to pick columns library(dplyr) # There are three sets of variables starting with \"perc\" - let's make sure they # add up to 300 as they maybe should # Use starts_with to select the variables # First, do it with rowSums, # either by picking column indices or using tidyselect midwest$rowsum_rowSums1 &lt;- rowSums(midwest[,c(12:16,18:20,22:26)]) midwest$rowsum_rowSums2 &lt;- midwest %&gt;% select(starts_with('perc')) %&gt;% rowSums() # Next, with apply - we're doing sum() here for the function # but it could be anything midwest$rowsum_apply &lt;- apply( midwest %&gt;% select(starts_with('perc')), MARGIN = 1, sum) # Next, two ways with purrr: library(purrr) # First, using purrr::reduce, which is good for some functions like summing # Note that . is the data set being sent by %&gt;% midwest &lt;- midwest %&gt;% mutate(rowsum_purrrReduce = reduce(select(., starts_with('perc')), `+`)) # More flexible, purrr::pmap, which works for any function # using pmap_dbl here to get a numeric variable rather than a list midwest &lt;- midwest %&gt;% mutate(rowsum_purrrpmap = pmap_dbl( select(.,starts_with('perc')), sum)) # So do we get 300? summary(midwest$rowsum_rowSums2) # Uh-oh... looks like we didn't understand the data after all. ",
    "url": "/Data_Manipulation/rowwise_calculations.html#r",
    
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#r"
  },"650": {
    "doc": "Rowwise Calculations",
    "title": "Stata",
    "content": "Stata has a series of built-in row operations that use the egen command. See help egen for the full list, and look for functions beginning with row like rowmean. The full list includes: rowfirst and rowlast (first or last non-missing observation), rowmean, rowmedian, rowmax, rowmin, rowpctile, and rowtotal (the mean, median, max, min, given percentile, or sum of all the variables), and rowmiss and rownonmiss (the count of the number of missing or nonmissing observations across the variables). The egenmore package, which can be installed with ssc install egenmore, adds rall, rany, and rcount (checks a condition for each variable and returns whether all are true, any are true, or the number that are true), rownvals and rowsvals (number of unique values for numeric and string variables, respectively), and rsum2 (rowtotal with some additional options). * Get data on midwestern states import delimited using \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/midwest.csv\" * There are three sets of variables starting with \"perc\" - let's make sure they * add up to 300 as they should * Use * as a wildcard for variable names egen total_perc = rowtotal(perc*) summ total_perc * They don't! Uh oh. * Let's just check the education variables - should add up to 100 * Use - to include all variables from one to the other * based on their current order in the data egen total_ed = rowtotal(perchsd-percprof) * Oh that explains it... * These aren't exclusive categories (HSD, college overlap) * and also leaves out non-HS graduates. summ total_ed . ",
    "url": "/Data_Manipulation/rowwise_calculations.html#stata",
    
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#stata"
  },"651": {
    "doc": "Sankey Diagrams",
    "title": "Sankey Diagrams",
    "content": "Sankey diagrams are visual displays that represent a data flow across sequential points of change, sorting, or decision. They can be used to track decision-making, behavioral patterns, resource flow, or as a method to display time series data, among other uses. ",
    "url": "/Presentation/Figures/sankey_diagrams.html",
    
    "relUrl": "/Presentation/Figures/sankey_diagrams.html"
  },"652": {
    "doc": "Sankey Diagrams",
    "title": "Keep in Mind",
    "content": ". | A Sankey diagram is comprised of stacked categorical variables, with each variable on its own vertical axis. | Categorical flow points are generally referred to as “nodes.” | Horizontal lines or bands show the density of variables at each node and the subsequent distribution onto the next variable. | . ",
    "url": "/Presentation/Figures/sankey_diagrams.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/sankey_diagrams.html#keep-in-mind"
  },"653": {
    "doc": "Sankey Diagrams",
    "title": "Also Consider",
    "content": ". | Variables should generally be categorical, as continuous values will typically not work in this setting. | Too few or too many categories can make a Sankey diagram less effective. Segmenting or grouping variables may be useful. | Sankey diagrams are sometimes known as alluvial diagrams, though the latter is often used to describe changes over time. | . ",
    "url": "/Presentation/Figures/sankey_diagrams.html#also-consider",
    
    "relUrl": "/Presentation/Figures/sankey_diagrams.html#also-consider"
  },"654": {
    "doc": "Sankey Diagrams",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/sankey_diagrams.html#implementations",
    
    "relUrl": "/Presentation/Figures/sankey_diagrams.html#implementations"
  },"655": {
    "doc": "Sankey Diagrams",
    "title": "R",
    "content": "There are many excellent packages in R for making Sankey diagrams (networkD3, alluvial, and ggforce among them), but let’s begin by looking at the highcharter package. It is an R wrapper for the Highcharts Javascript library and a powerful tool. It’s also easy to get up and running quickly, while some other packages may require more preliminary data wrangling. We begin by loading pacman and dplyr. library(pacman) p_load(dplyr) . Next, we bring in the highcharter package and import a csv file that includes data from the 2020-2021 NBA season, including team, division, winning percentage, playoff seeding, and appearance in the conference semifinals. We change the winning percentage “win_perc” variable to a character so that it functions appropriately in this setting and take a look at the first few rows. p_load(highcharter) nba = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Presentation/Figures/Data/Sankey_Diagrams/NBA.csv\") nba$win_perc &lt;- as.character(nba$win_perc) head(nba) . Now we simply use “data_to_sankey” within the hchart function to create our Sankey diagram. We see that the data flows in the same order as our data frame, from individual team to conference, and then from winning percentage and playoff position to whether the team made the conference semifinals. I have chosen the theme ggplot2 but there are many nice options. hchart(data_to_sankey(nba), \"sankey\", name = \"Number of teams\") %&gt;% hc_title(text= \"NBA 2020-2021 Season\") %&gt;% hc_subtitle(text= \"Team --- Conference --- Winning Percentage --- Playoff Position --- Advancement to Conference Semifinals\") %&gt;% hc_add_theme(hc_theme_ggplot2()) %&gt;% hc_plotOptions(series = list(dataLabels = list( style = list(fontSize = \"10px\")))) . Dynamically hovering the cursor over each node or branch gives us a count of how many teams went to each of the next nodes. For instance, we see that 3 teams from the West had a winning percentage of 0.4. Also, between the last two nodes we see that one top 4 seed did not advance to the conference semifinals and one 5 to 8 seed did. Next, we look at the ggalluvial package, which is an extension for the ggplot2 package. This, too, is simple to get started. In fact, the bulk of the code here is manipulating the familiar mtcars data set such that hp, wt, mpg, and qsec are made categorical from their original numeric values. This fact underscores one way the Sankey diagram is useful. Namely, that values can be essentially binned in order to see trends in data flow. We load the package and mtcars, do our data wrangling, and check out the first few rows. p_load(ggplot2, ggalluvial) data(mtcars) mtc = mtcars %&gt;% select(cyl, hp, wt, qsec, mpg) %&gt;% mutate( hp = case_when( hp &lt;= 100 ~ \"0-100\", hp &lt;= 150 ~ \"100-150\", hp &lt;= 200 ~ \"150-200\", hp &lt;= 500 ~ \"200-350\"), wt = case_when( wt &lt;= 2 ~ \"1-2\", wt &lt;= 3 ~ \"2-3\", wt &lt;= 4 ~ \"3-4\", wt &lt;= 7 ~ \"4-6\"), mpg = case_when( mpg &lt;= 20 ~ \"10-20 mpg\", mpg &lt;= 30 ~ \"20-30 mpg\", mpg &lt;= 50 ~ \"30-40 mpg\"), qsec = case_when( qsec &lt;= 16 ~ \"14-16\", qsec &lt;= 17 ~ \"16-17\", qsec &lt;= 18 ~ \"17-18\", qsec &lt;= 23 ~ \"18-23\" )) head(mtc) . Next, we use the familiar ggplot and include the line “geom_alluvium” to induce an alluvial diagram. We can interpret that weight and number of cylinders are highly correlated but that horsepower and the quarter-mile time are less so. ggplot(data = mtc, aes(axis1 = wt, axis2 = cyl, axis3 = hp, axis4 = qsec)) + scale_x_discrete(limits = c(\"Weight (1,000 lbs)\", \"Cylinders\", \"Horsepower\", \"1/4 mile time (seconds\"), expand = c(.05, .05)) + geom_alluvium(aes(fill = mpg)) + geom_stratum(color = \"grey\") + geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) + theme_minimal() + ggtitle(\"Miles per Gallon\", \"Stratified by weight, cylinders, horsepower, &amp; 1/4 mile time (n = 32 car models)\") . We see four variables (wt, cyl, hp, and qsec) in columns, with the proportion of each category represented by the height of the node. In this package, it is easier to see the distribution of each variable because columns are all the same height and frequency of categorical values is proportional. The y axis is a measure of the number of observations in our sample. Additionally, our fifth variable, mpg, is color coded in bands across the diagram, allowing us to highlight a particular aspect of this data set. These are relatively basic examples, but in a few lines of code demonstrate the usefulness of a Sankey diagram to track the flow and distribution of variables in a data set. ",
    "url": "/Presentation/Figures/sankey_diagrams.html#r",
    
    "relUrl": "/Presentation/Figures/sankey_diagrams.html#r"
  },"656": {
    "doc": "Sankey Diagrams",
    "title": "Stata",
    "content": "They sankey package can be used to easily conduct sankey plots in Stata. For further vignettes to master the subcommands, please reference Asjad Naqvi’s Github repository on the matter. First, install the package through SSC and be sure to replace in case of updates to the package. A dependency is the palettes package as well. ssc install sankey, replace ssc install palettes, replace ssc install colrspace, replace . While you can use the subcommands to elaborate on the process, the basic commands for the sankey plot is shown below. For this vingette, we will use the Sankey example dataset from Asjad Naqvi. # Import Data import excel using \"https://github.com/asjadnaqvi/stata-sankey/blob/main/data/sankey_example2.xlsx?raw=true\", clear first # Basic Sankey plot sankey value, from(source) to(destination) by(layer) . ",
    "url": "/Presentation/Figures/sankey_diagrams.html#stata",
    
    "relUrl": "/Presentation/Figures/sankey_diagrams.html#stata"
  },"657": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Scatterplot by Group on Shared Axes",
    "content": "Scatterplots are a standard data visualization tool that allows you to look at the relationship between two variables \\(X\\) and \\(Y\\). If you want to see how the relationship between \\(X\\) and \\(Y\\) might be different for Group A as opposed to Group B, then you might want to plot the scatterplot for both groups on the same set of axes, so you can compare them. ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html",
    
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html"
  },"658": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Keep in Mind",
    "content": ". | Scatterplots may not work well if the data is discrete, or if there are a large number of data points. | . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#keep-in-mind"
  },"659": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Also Consider",
    "content": ". | Sometimes, instead of putting both Group A and Group B on the same set of axes, it makes more sense to plot them separately, and put the plots next to each other. See Faceted Graphs. | There are many ways to make the scatterplots of the two groups distinct. See Styling Scatterplots. | . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#also-consider",
    
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#also-consider"
  },"660": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#implementations",
    
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#implementations"
  },"661": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "R",
    "content": "library(ggplot2) # Load auto data data(mtcars) # Make sure that our grouping variable is a factor # and labeled properly mtcars$Transmission &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\")) # Put wt on the x-axis, mpg on the y-axis, ggplot(mtcars, aes(x = wt, y = mpg, # distinguish the Transmission values by color, color = Transmission)) + # make it a scatterplot with geom_point() geom_point()+ # And label properly labs(x = \"Car Weight\", y = \"MPG\") . This results in: . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#r",
    
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#r"
  },"662": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Stata",
    "content": "* Load auto data sysuse auto.dta * Start a twoway command * Then, for each group, put its scatter command in () * Using if to plot each group separately * And specifying mcolor or msymbol (etc.) to differentiate them twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)) * Add a legend option so you know what the colors mean twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)), legend(lab(1 Domestic) lab(2 Foreign)) xtitle(\"Weight\") ytitle(\"MPG\") . This results in: . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#stata",
    
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#stata"
  },"663": {
    "doc": "Set a Working Directory",
    "title": "Set a Working Directory",
    "content": "When you want to refer to files on your computer in your code, for example to load a data file, you can usually refer to them in one of two ways: . | Using an absolute path, which starts from the root of the computer. On Windows it would look something like C:/Users/Name/Documents/MyFile.csv | Using a relative path which starts in the working directory and works from there. For example, if your workind directory were C:/Users/Name/ then you could refer to that same MyFile.csv file using Documents/MyFile.csv as a relative path. | . Using absolute paths is generally frowned upon because it makes it very difficult for anyone to run your code on their computer, since they won’t have the same folder structure. So, you want to use relative paths in your code. This means you need to know how to set the working directory so you know where your file searching starts from. ",
    "url": "/Other/set_a_working_directory.html",
    
    "relUrl": "/Other/set_a_working_directory.html"
  },"664": {
    "doc": "Set a Working Directory",
    "title": "Keep in Mind",
    "content": ". | If you set a working directory in your code, that’s basically the same as using an absolute path. Your code won’t work on anyone else’s computer! Setting a working directory is generally something you’ll do interactively (by hand, either using a menu or some code you type directly in the console) when you start your software package. | Once you are in a working directory, you can explore your folder structure using your filepath. As above, if your working directory is C:/Users/Name/, you can get to the file MyFile.csv in the C:/Users/Name/Documents/ folder with Documents/MyFile.csv. You can also go up folders with ... You can get at image.png in the Users folder with ../image.png Or if you want the file C:/Users/Admin/passwords.txt you could do ../Admin/passwords.txt. This means you can set your working directory once, and reach for files anywhere you like without having to change it again. Or if you got the working directory wrong, you can get at a new one with a relative filepath! If cd() is your language’s working-directory-setting command, you can go from C:/Users/Name/Documents/ to C:/Users/Name/ with cd('..') to go up one folder. | Because setting the working directory is often done by hand anyway, it’s common for it to be a point-and-click or menu feature in your software, even in software designed for use with text code. Some examples of this will be in the Implementations section. | Many editors and IDEs come with project managers. Most project managers have you designate a folder as being that project’s home. Then, when you open that project, most managers will automatically set the working directory to that home folder. | In Windows, if you copy a filepath in, it will often use \\ instead of / between folders. Many programming languages don’t like this. You may have to change them manually. | . ",
    "url": "/Other/set_a_working_directory.html#keep-in-mind",
    
    "relUrl": "/Other/set_a_working_directory.html#keep-in-mind"
  },"665": {
    "doc": "Set a Working Directory",
    "title": "Also Consider",
    "content": ". | Get a list of files from a directory. | . ",
    "url": "/Other/set_a_working_directory.html#also-consider",
    
    "relUrl": "/Other/set_a_working_directory.html#also-consider"
  },"666": {
    "doc": "Set a Working Directory",
    "title": "Implementations",
    "content": " ",
    "url": "/Other/set_a_working_directory.html#implementations",
    
    "relUrl": "/Other/set_a_working_directory.html#implementations"
  },"667": {
    "doc": "Set a Working Directory",
    "title": "Julia",
    "content": "In Julia, you can use the cd() function to change the working directory. cd(\"C:/My/New/Working/Directory/\") . You may use the pwd() function to check the current working directory. pwd() . ",
    "url": "/Other/set_a_working_directory.html#julia",
    
    "relUrl": "/Other/set_a_working_directory.html#julia"
  },"668": {
    "doc": "Set a Working Directory",
    "title": "Python",
    "content": "In Python, the os.chdir() function will let you change working directories. import os os.chdir('C:/My/New/Working/Directory/') # Or if you want to change the directory to your \"Home\" directory, you can use os.path.expanduser(\"~\") os.chdir(os.path.expanduser(\"~\")) . In the Spyder IDE, the working directory is listed by default in the top-right, and you can edit it directly. ",
    "url": "/Other/set_a_working_directory.html#python",
    
    "relUrl": "/Other/set_a_working_directory.html#python"
  },"669": {
    "doc": "Set a Working Directory",
    "title": "R",
    "content": "In R, the setwd() function can change the working directory. setwd('C:/My/New/Working/Directory/') . If you are working in an R project, there is also the here package. library(here) here() . here() will start in whatever your current working directory and look upwards into parent folders until it finds something that indicates that it’s found a folder containing a project: an .Rproj (R Project) file, a .git or .svn folder, or any of the files .here, .projectile, remake.yml, or DESCRIPTION, and will set the working directory to that folder. This won’t work if you haven’t set up a proper project folder structure. If you are using RStudio, there are several other ways to set the working directory. In the Session menu, you can choose to set the working directory to the Source File location (whatever folder the active code tab file is saved in), to the File Pane location (whatever folder the Files pane, in the bottom-right by default, has navigated to), or you can choose it using your standard operating system folder-picker. You can also navigate to the folder you want in the Files pane (which is in the bottom-right by default) and select More \\(\\rightarrow\\) Set as Working Directory. ",
    "url": "/Other/set_a_working_directory.html#r",
    
    "relUrl": "/Other/set_a_working_directory.html#r"
  },"670": {
    "doc": "Set a Working Directory",
    "title": "Stata",
    "content": "In Stata, you can use the cd command to change working directories. cd \"C:/My/New/Working/Directory/\" . You can also change the working directory in the File \\(\\rightarrow\\) Change Working Directory menu, which will pull up your standard operating system folder-picker. Additionally, if you open Stata by clicking on a .do file saved on your computer, the working directory will automatically be set to whatever folder that .do file is saved in. ",
    "url": "/Other/set_a_working_directory.html#stata",
    
    "relUrl": "/Other/set_a_working_directory.html#stata"
  },"671": {
    "doc": "Simple Linear Regression",
    "title": "Simple Linear Regression",
    "content": "Ordinary Least Squares (OLS) is a statistical method that produces a best-fit line between some outcome variable \\(Y\\) and any number of predictor variables \\(X_1, X_2, X_3, ...\\). These predictor variables may also be called independent variables or right-hand-side variables. For more information about OLS, see Wikipedia: Ordinary Least Squares. ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html"
  },"672": {
    "doc": "Simple Linear Regression",
    "title": "Keep in Mind",
    "content": ". | OLS assumes that you have specified a true linear relationship. | OLS results are not guaranteed to have a causal interpretation. Just because OLS estimates a positive relationship between \\(X_1\\) and \\(Y\\) does not necessarily mean that an increase in \\(X_1\\) will cause \\(Y\\) to increase. | OLS does not require that your variables follow a normal distribution. | . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#keep-in-mind"
  },"673": {
    "doc": "Simple Linear Regression",
    "title": "Also Consider",
    "content": ". | OLS standard errors assume that the model’s error term is IID, which may not be true. Consider whether your analysis should use heteroskedasticity-robust standard errors or cluster-robust standard errors. | If your outcome variable is discrete or bounded, then OLS is by nature incorrectly specified. You may want to use probit or logit instead for a binary outcome variable, or ordered probit or ordered logit for an ordinal outcome variable. | If the goal of your analysis is predicting the outcome variable and you have a very long list of predictor variables, you may want to consider using a method that will select a subset of your predictors. A common way to do this is a penalized regression method like LASSO. | In many contexts, you may want to include interaction terms or polynomials in your regression equation. | . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#also-consider",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#also-consider"
  },"674": {
    "doc": "Simple Linear Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#implementations",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#implementations"
  },"675": {
    "doc": "Simple Linear Regression",
    "title": "Gretl",
    "content": "# Load auto data open https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.gdt # Run OLS using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors ols mpg const headroom trunk weight . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#gretl",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#gretl"
  },"676": {
    "doc": "Simple Linear Regression",
    "title": "Julia",
    "content": "# Uncomment the next line to install all the necessary packages # import Pkg; Pkg.add([\"CSV\", \"DataFrames\", \"GLM\", \"StatsModels\"]) # We tap into JuliaStats ecosystem to solve our data and regression problems :) # In particular, DataFrames package provides dataset handling functions, # StatsModels gives us the `@formula` macro to specify our model in a concise and readable form, # while GLM implements (Generalized) Linear Models fitting and analysis. # And all these packages work together seamlessly. using StatsModels, GLM, DataFrames, CSV # Here we download the data set, parse the file with CSV and load into a DataFrame mtcars = CSV.read(download(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Data/mtcars.csv\"), DataFrame) # The following line closely follows the R and Python syntax, thanks to GLM and StatModels packages # Here we specify a linear model and fit it to our data set in one go ols = lm(@formula(mpg ~ cyl + hp + wt), mtcars) # This will print out the summary of the fitted model including # coefficients' estimates, standard errors, confidence intervals and p-values print(ols) . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#julia",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#julia"
  },"677": {
    "doc": "Simple Linear Regression",
    "title": "Matlab",
    "content": "% Load auto data load('https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.mat') % Run OLS using the auto data, with mpg as the outcome variable % and headroom, trunk, and weight as predictors intercept = ones(length(headroom),1); X = [intercept headroom trunk weight]; [b,bint,r,rint,stats] = regress(mpg,X); . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#matlab",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#matlab"
  },"678": {
    "doc": "Simple Linear Regression",
    "title": "Python",
    "content": "# Use 'pip install statsmodels' or 'conda install statsmodels' # on the command line to install the statsmodels package. # Import the relevant parts of the package: import statsmodels.api as sm import statsmodels.formula.api as smf # Get the mtcars example dataset mtcars = sm.datasets.get_rdataset(\"mtcars\").data # Fit OLS regression model to mtcars ols = smf.ols(formula='mpg ~ cyl + hp + wt', data=mtcars).fit() # Look at the OLS results print(ols.summary()) . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#python",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#python"
  },"679": {
    "doc": "Simple Linear Regression",
    "title": "R",
    "content": "# Load Data # data(mtcars) ## Optional: automatically loaded anyway # Run OLS using the mtcars data, with mpg as the outcome variable # and cyl, hp, and wt as predictors olsmodel &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars) # Look at the results summary(olsmodel) . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#r",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#r"
  },"680": {
    "doc": "Simple Linear Regression",
    "title": "SAS",
    "content": "/* Load Data */ proc import datafile=\"C:mtcars.dbf\" out=fromr dbms=dbf; run; /* OLS regression */ proc reg; model mpg = cyl hp wt; run; . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#sas",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#sas"
  },"681": {
    "doc": "Simple Linear Regression",
    "title": "Stata",
    "content": "* Load auto data sysuse https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.dta * Run OLS using the auto data, with mpg as the outcome variable * and headroom, trunk, and weight as predictors regress mpg headroom trunk weight . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#stata",
    
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#stata"
  },"682": {
    "doc": "Simple Web Scraping",
    "title": "Introduction",
    "content": "Webscraping is the processs of programmatically extracting information from the internet that was intended to be displayed in a browser. But it should only be used as a last resort; generally an API (appplication programming interface) is a much better way to obtain information, if one is available. If you do find yourself in a scraping situation, be really sure to check it’s legally allowed and that you are not violating the website’s robots.txt rules. robots.txt is a special file on almost every website that sets out what’s fair play to crawl (conditional on legality) and what your webscraper should not go poking around in. ",
    "url": "/Other/simple_web_scrape.html#introduction",
    
    "relUrl": "/Other/simple_web_scrape.html#introduction"
  },"683": {
    "doc": "Simple Web Scraping",
    "title": "Keep in Mind",
    "content": "  Remember that webscraping is an art as much a science so play around with a problem and figure out creative ways to solve issues, it might not pop out at you immediately. ",
    "url": "/Other/simple_web_scrape.html#keep-in-mind",
    
    "relUrl": "/Other/simple_web_scrape.html#keep-in-mind"
  },"684": {
    "doc": "Simple Web Scraping",
    "title": "Implementation",
    "content": " ",
    "url": "/Other/simple_web_scrape.html#implementation",
    
    "relUrl": "/Other/simple_web_scrape.html#implementation"
  },"685": {
    "doc": "Simple Web Scraping",
    "title": "Python",
    "content": "Five of the most well-known and powerful libraries for webscraping in Python, which between them cover a huge range of needs, are requests, lxml, beautifulsoup, selenium, and scrapy. Broadly, requests is for downloading webpages via code, beautifulsoup and lxml are for parsing webpages and extracting info, and scrapy and selenium are full web-crawling solutions. For the special case of scraping table from websites, pandas is the best option. For quick and simple webscraping of individual HTML tags, a good combo is requests, which does little more than go and grab the HTML of a webpage, and beautifulsoup, which then helps you to navigate the structure of the page and pull out what you’re actually interested in. For dynamic webpages that use javascript rather than just HTML, you’ll need selenium. To scale up and hit thousands of webpages in an efficient way, you might try scrapy, which can work with the other tools and handle multiple sessions, and all other kinds of bells and whistles… it’s actually a “web scraping framework”. Let’s see a simple example using requests and beautifulsoup, followed by an example of extracting a table using pandas. First we need to import the packages; remember you may need to install these first by running pip install packagename on your computer’s command line. import requests from bs4 import BeautifulSoup import pandas as pd . Now we’ll specify a URL to scrape, download it as a page, and show some of the HTML as downloaded (here, the first 500 characters) . url = \"https://blackopaldirect.com/product/black-opals/2-86-ct-black-opal-11-6x9-7x3-9mm/\" page = requests.get(url) print(page.text[:500]) . &lt;!DOCTYPE html&gt; &lt;html lang=\"en-US\"&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no\"&gt; &lt;link rel=\"profile\" href=\"http://gmpg.org/xfn/11\"&gt; &lt;link rel=\"pingback\" href=\"https://blackopaldirect.com/xmlrpc.php\"&gt; &lt;!-- Facebook Pixel Code --&gt; &lt;script&gt; !function(f,b,e,v,n,t,s) {if(f.fbq)return;n=f.fbq=function(){n.callMethod? n.callMethod.apply(n,arguments):n.queue.push(arguments)}; if(!f._fbq)f._fbq=n;n.push=n;n.loaded= . That’s a bit tricky to read, let alone get any useful data out of. So let’s now use beautifulsoup, which parses extracted HTML. To pretty print the page use .text. In the example below, we’ll just show the first 100 characters and we’ll also use rstrip and lstrip to trim leading and trailing whitespace: . soup = BeautifulSoup(page.text, 'html.parser') print(soup.text[:100].lstrip().rstrip()) . 2.86 ct black opal 11.6x9.7x3.9mm - Black Opal Direct . There are lots of different elements, with tags, that make up a page of HTML. For example, a title might have a tag ‘h1’ and a class ‘product_title’. Let’s see how we can retrieve anything with a class that is ‘price’ and a tag that is ‘p’ as these are the characteristics of prices displayed on the URL we are scraping. price_html = soup.find(\"p\", {\"class\": \"price\"}) print(price_html) . &lt;p class=\"price\"&gt;&lt;span class=\"woocommerce-Price-amount amount\"&gt;&lt;bdi&gt;&lt;span class=\"woocommerce-Price-currencySymbol\"&gt;US$&lt;/span&gt;2,500.00&lt;/bdi&gt;&lt;/span&gt;&lt;/p&gt; . This returns the first tag found that satisfies the conditions (to get all tags matching the criteria use soup.find_all). To extract the value, just use .text: . price_html.text . 'US$2,500.00' . Now let’s see an example of reading in a whole table of data. For this, we’ll use pandas, the ubiquitous Python library for working with data. We will read data from the first table on ‘https://simple.wikipedia.org/wiki/FIFA_World_Cup’ using pandas. The function we’ll use is read_html, which returns a list of dataframes of all the tables it finds when you pass it a URL. If you want to filter the list of tables, use the match= keyword argument with text that only appears in the table(s) you’re interested in. The example below shows how this works; looking at the website, we can see that the table we’re interested in (of past world cup results), has a ‘fourth place’ column while other tables on the page do not. Therefore we run: . df_list = pd.read_html('https://simple.wikipedia.org/wiki/FIFA_World_Cup', match='Fourth Place') # Retrieve first and only entry from list of dataframes df = df_list[0] df.head() . | | Year | Host | Winner | Score | Runners-up | Third Place | Score.1 | Fourth Place | . | 0 | 1930 Details | Uruguay | Uruguay | 4 - 2 | Argentina | United States | [note 1] | Yugoslavia | . | 1 | 1934 Details | Italy | Italy | 2 - 1(a.e.t.) | Czechoslovakia | Germany | 3 - 2 | Austria | . | 2 | 1938 Details | France | Italy | 4 - 2 | Hungary | Brazil | 4 - 2 | Sweden | . | 3 | 1950 Details | Brazil | Uruguay | [note 2] | Brazil | Sweden | [note 2] | Spain | . | 4 | 1954 Details | Switzerland | West Germany | 3 - 2 | Hungary | Austria | 3 - 1 | Uruguay | . This delivers the table neatly loaded into a pandas dataframe ready for further use. ",
    "url": "/Other/simple_web_scrape.html#python",
    
    "relUrl": "/Other/simple_web_scrape.html#python"
  },"686": {
    "doc": "Simple Web Scraping",
    "title": "R",
    "content": "The “rvest” package is a webscraping package in R that provides a tremendous amount of versatility, as well as being easy to use. For this specific task, of web scrapping pages on a website, we will be using read_html(), html_node(), html_table(), html_elements(), and html_text(). I will also make use of the selector gadget tool,(link for the download:Selector Gaget, as well as F12, to find the html paths. html_node and html_text . library(rvest) black_opals = read_html(\"https://blackopaldirect.com/product/black-opals/2-86-ct-black-opal-11-6x9-7x3-9mm/\") # Website of interest price = black_opals %&gt;% html_node(\"#product-103505 &gt; div.summary.entry-summary &gt; p.price &gt; span &gt; bdi\") %&gt;% # Find the exact element's node for the price html_text() # Convert it to text price # print the price ## [1] \"US$2,500.00\" . html_table . world_cup = read_html(\"https://simple.wikipedia.org/wiki/FIFA_World_Cup\") # Past_World_Cup_results cup_table = world_cup %&gt;% html_elements(xpath = \"/html/body/div[3]/div[3]/div[5]/div[1]/table[2]\") %&gt;% html_table() # Extract html elements cup_table = cup_table[[1]] # Assign the table from the lists cup_table %&gt;% head(5) # First 5 obs ## # A tibble: 5 x 8 ## Year Host Winner Score `Runners-up` `Third Place` Score `Fourth Place` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1930 D~ Uruguay Uruguay 4 - 2 Argentina United States [not~ Yugoslavia ## 2 1934 D~ Italy Italy 2 - 1~ Czechoslova~ Germany 3 - 2 Austria ## 3 1938 D~ France Italy 4 - 2 Hungary Brazil 4 - 2 Sweden ## 4 1950 D~ Brazil Uruguay [note~ Brazil Sweden [not~ Spain ## 5 1954 D~ Switze~ West G~ 3 - 2 Hungary Austria 3 - 1 Uruguay . Another good tool is html_element. Also here is the rvest website for more information. rvest . ",
    "url": "/Other/simple_web_scrape.html#r",
    
    "relUrl": "/Other/simple_web_scrape.html#r"
  },"687": {
    "doc": "Simple Web Scraping",
    "title": "Simple Web Scraping",
    "content": " ",
    "url": "/Other/simple_web_scrape.html",
    
    "relUrl": "/Other/simple_web_scrape.html"
  },"688": {
    "doc": "Spatial Joins",
    "title": "Spatial Joins",
    "content": "Spatial joins are crucial for merging different types of data in geospatial analysis. For example, if you want to know how many libraries (points) are in a city, county, or state (polygon). This skill allows you to take data from different types of spatial data (vector data like points, lines, and polygons, and raster data (with a little more work)) sets and merge them together using unique identifiers. Joins are typically interesections of objects, but can be expressed in different ways. These include: equals, covers, covered by, within, touches, near, crosses, and more. These are all functions within the sf function in R or the geopandas package in Python. For more on the different types of intersections in 2D projections, see the Wikipedia page on spatial relations. ",
    "url": "/Geo-Spatial/spatial_joins.html",
    
    "relUrl": "/Geo-Spatial/spatial_joins.html"
  },"689": {
    "doc": "Spatial Joins",
    "title": "Keep in Mind",
    "content": ". | Geospatial packages in R and Python tend to have a large number of complex dependencies, which can make installing them painful. Best practice is to install geospatial packages in a new virtual environment. | When it comes to the package we are using in R for the US boundaries, it is much easier to install via the devtools. This will save you the trouble of getting errors when installing the data packages for the boundaries. Otherwise, your mileage may vary. When I installed USAboundariesData via USAboundaries, I received errors. | . devtools::install_github(\"ropensci/USAboundaries\") devtools::install_github(\"ropensci/USAboundariesData\") . | Note: Even with the R installation via devtools, you may be prompted to install the “USAboundariesData” package and need to restart your session. | . ",
    "url": "/Geo-Spatial/spatial_joins.html#keep-in-mind",
    
    "relUrl": "/Geo-Spatial/spatial_joins.html#keep-in-mind"
  },"690": {
    "doc": "Spatial Joins",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/spatial_joins.html#implementations",
    
    "relUrl": "/Geo-Spatial/spatial_joins.html#implementations"
  },"691": {
    "doc": "Spatial Joins",
    "title": "Python",
    "content": "The geopandas package is the easiest way to start doing geo-spatial analysis in Python. This example of a spatial merge closely follows one from the documentation for geopandas. # Geospatial packages tend to have many elaborate dependencies. The quickest # way to get going is to use a clean virtual environment and then # 'conda install geopandas' followed by # 'conda install -c conda-forge descartes' # descartes is what allows geopandas to plot data. import geopandas as gpd # Grab a world map world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) # Plot the map of the world world.plot() # Grab data on cities cities = gpd.read_file(gpd.datasets.get_path('naturalearth_cities')) # We can plot the cities too - but they're just dots of lat/lon without any # context for now cities.plot() # The data don't actually need to be combined to be viewed on a map as long as # they are using the same 'crs', or coordinate reference system. # Force cities and world to share crs: cities = cities.to_crs(world.crs) # Combine them on a plot: base = world.plot(color='white', edgecolor='black') cities.plot(ax=base, marker='o', color='red', markersize=5) # We want to perform a spatial merge, but there are many kinds in 2D # projections, including withins, touches, crosses, and overlaps. We want to # use an intersects spatial join - ie we want to combine each city (a lat/lon # point) with the shapes of countries and determine which city goes in which # country (even if it's on the boundary). We use the 'sjoin' function: cities_with_country = gpd.sjoin(cities, world, how=\"inner\", op='intersects') cities_with_country.head() # name_left geometry pop_est continent \\ # Vatican City POINT (12.45339 41.90328) 62137802 Europe # San Marino POINT (12.44177 43.93610) 62137802 Europe # Rome POINT (12.48131 41.89790) 62137802 Europe # Vaduz POINT (9.51667 47.13372) 8754413 Europe # Vienna POINT (16.36469 48.20196) 8754413 Europe # name_right iso_a3 gdp_md_est # Italy ITA 2221000.0 # Italy ITA 2221000.0 # Italy ITA 2221000.0 # Austria AUT 416600.0 # Austria AUT 416600.0 . ",
    "url": "/Geo-Spatial/spatial_joins.html#python",
    
    "relUrl": "/Geo-Spatial/spatial_joins.html#python"
  },"692": {
    "doc": "Spatial Joins",
    "title": "R",
    "content": "Acknowledgments to Ryan A. Peek for his guide that I am reimagining for LOST. We will need a few packages to do our analysis. If you need to install any packages, do so with install.packages(‘name_of_package’), then load it if necessary. library(sf) library(dplyr) library(viridis) library(ggplot2) library(USAboundaries) library(GSODR) . | We will work with polygon data from the USA boundaries initially, then move on to climate data point data via the Global Surface Summary of the Day (gsodr) package and join them together. | We start with the boundaries of the United States to get desirable polygons to work with for our analysis. To pay homage to the states of my alma maters, we will do some analysis with Oregon, Ohio, and Michigan. | . #Selecting the United States Boundaries, but omitting Alaska, Hawaii, and Puerto Rico for it to be scaled better usa &lt;- us_boundaries(type=\"state\", resolution = \"low\") %&gt;% filter(!state_abbr %in% c(\"PR\", \"AK\", \"HI\")) #Ohio with high resolution oh &lt;- USAboundaries::us_states(resolution = \"high\", states = \"OH\") #Oregon with high resolution or &lt;- USAboundaries::us_states(resolution = \"high\", states = \"OR\") #Michigan with high resolution mi &lt;- USAboundaries::us_states(resolution = \"high\", states = \"MI\") #Insets for the identified states #Oregon or_box &lt;- st_make_grid(or, n = 1) #Ohio oh_box &lt;- st_make_grid(oh, n = 1) #Michigan mi_box &lt;- st_make_grid(mi, n = 1) #We can also include the counties boundaries within the state too! #Oregon or_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"OR\") #Ohio oh_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"OH\") #Michigan mi_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"MI\") . Now we can plot it out. Oregon highlighted . plot(usa$geometry) plot(or$geometry, add=T, col=\"gray50\", border=\"black\") plot(or_co$geometry, add=T, border=\"green\", col=NA) plot(or_box, add=T, border=\"yellow\", col=NA, lwd=2) . Ohio highlighted . plot(usa$geometry) plot(oh$geometry, add=T, col=\"gray50\", border=\"black\") plot(oh_co$geometry, add=T, border=\"yellow\", col=NA) plot(oh_box, add=T, border=\"blue\", col=NA, lwd=2) . Michigan highlighted . plot(usa$geometry) plot(mi$geometry, add=T, col=\"gray50\", border=\"black\") plot(mi_co$geometry, add=T, border=\"gray\", col=NA) plot(mi_box, add=T, border=\"green\", col=NA, lwd=2) . All three highlighted at once. plot(usa$geometry) plot(mi$geometry, add=T, col=\"gray50\", border=\"black\") plot(mi_co$geometry, add=T, border=\"gray\", col=NA) plot(mi_box, add=T, border=\"green\", col=NA, lwd=2) plot(oh$geometry, add=T, col=\"gray50\", border=\"black\") plot(oh_co$geometry, add=T, border=\"yellow\", col=NA) plot(oh_box, add=T, border=\"blue\", col=NA, lwd=2) plot(or$geometry, add=T, col=\"gray50\", border=\"black\") plot(or_co$geometry, add=T, border=\"green\", col=NA) plot(or_box, add=T, border=\"yellow\", col=NA, lwd=2) . Now that there are polygons established and identified, we can add in some point data to join to our currently existing polygon data and do some analysis with it. To do this we will use the Global Surface Summary of the Day (gsodr) package for climate data. We will take the metadata from the GSODR package via ‘isd_history’, make it spatial data, then filter out only those observations in our candidate states of Oregon, Ohio, and Michigan. load(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\")) #We want this to be spatial data isd_history &lt;- as.data.frame(isd_history) %&gt;% st_as_sf(coords=c(\"LON\",\"LAT\"), crs=4326, remove=FALSE) #There are many observations, so we want to narrow it to our three candidate states isd_history_or &lt;- dplyr::filter(isd_history, CTRY==\"US\", STATE==\"OR\") isd_history_oh &lt;- dplyr::filter(isd_history, CTRY==\"US\", STATE==\"OH\") isd_history_mi &lt;- dplyr::filter(isd_history, CTRY==\"US\", STATE==\"MI\") . This filtering should take you from around 26,700 observation sites around the world to approximately 200 in Michigan, 85 in Ohio, and 100 in Oregon. These numbers may vary based on when you independently do your analysis. Let’s see these stations plotted in each state individually: . Note: the codes in the ‘border’ and ‘bg’ identifiers are from the viridis package. You can get some awesome color scales using that package. You can also use standard names. Oregon . plot(isd_history_or$geometry, cex=0.5) plot(or$geometry, col=alpha(\"gray\", 0.5), border=\"#1F968BFF\", lwd=1.5, add=TRUE) plot(isd_history_or$geometry, add=T, pch=21, bg=\"#FDE725FF\", cex=0.7, col=\"black\") title(\"Oregon GSOD Climate Stations\") . Ohio . plot(isd_history_oh$geometry, cex=0.5) plot(oh$geometry, col=alpha(\"red\", 0.5), border=\"gray\", lwd=1.5, add=TRUE) plot(isd_history_oh$geometry, add=T, pch=21, bg=\"black\", cex=0.7, col=\"black\") title(\"Ohio GSOD Climate Stations\") . Michigan . plot(isd_history_mi$geometry, cex=0.5) plot(mi$geometry, col=alpha(\"green\", 0.5), border=\"blue\", lwd=1.5, add=TRUE) plot(isd_history_mi$geometry, add=T, pch=21, bg=\"white\", cex=0.7, col=\"black\") title(\"Michigan GSOD Climate Stations\") . ",
    "url": "/Geo-Spatial/spatial_joins.html#r",
    
    "relUrl": "/Geo-Spatial/spatial_joins.html#r"
  },"693": {
    "doc": "Spatial Joins",
    "title": "Now, for the magic:",
    "content": "We are going to start with selecting polygons from points. This is not necessarily merging the data together, but using a spatial join to filter out polygons (counties, states, etc.) from points (climate data stations) . We will start by selecting the Oregon counties that have climate data stations within their boundaries: . or_co_isd_poly &lt;- or_co[isd_history, ] plot(or_co_isd_poly$geometry, col=alpha(\"green\",0.7)) title(\"Oregon Counties with GSOD Climate Stations\") . Now for all of our three candidate states: . cand_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = c(\"OR\", \"OH\", \"MI\")) cand_co_isd_poly &lt;- cand_co[isd_history, ] plot(cand_co_isd_poly$geometry, col=alpha(\"blue\",0.7)) title(\"Counties in Candidate States with GSOD Climate Stations\") . We see how we can filter out polygons from attributes or intersecting relationships with points, but what if we want to merge data from the points into the polygon or vice versa? . We will use the data set for Oregon for the join example. Notice in our point dataset that there are no county names. Only station/city names. Let us join the county polygons with the climate station points and add the county names to the station data. We do this using the st_join function, which comes from the sf package. isd_or_co_pts &lt;- st_join(isd_history, left = FALSE, or_co[\"name\"]) #Rename the county name variable county instead of name, since we already have NAME for the station location colnames(isd_or_co_pts)[which(names(isd_or_co_pts) == \"name\")] &lt;- \"county\" plot(isd_or_co_pts$geometry, pch=21, cex=0.7, col=\"black\", bg=\"orange\") plot(or_co$geometry, border=\"gray\", col=NA, add=T) . You now have successfully joined the county name data into your new point data set! Those points in the plot now contain the county information for data analysis purposes. You can join in any attribute you would like, or by leaving it as: . isd_or_co_pts &lt;- st_join(isd_history, left = FALSE, or_co) . You add all attributes from the polygon into the point data frame! . Also note that st_join is the default function that joins any type of intersection. You can be more precise our particular about your conditions with the other spatial joins: . st_within only joins elements that are completely within the defined area . st_equal only joins elements that are spatially equal. Meaning that A is within B and B is within A. You can use these to pare down your selections and joins to specific relationships. Good luck with your geospatial analysis! . ",
    "url": "/Geo-Spatial/spatial_joins.html#now-for-the-magic",
    
    "relUrl": "/Geo-Spatial/spatial_joins.html#now-for-the-magic"
  },"694": {
    "doc": "Spatial Lag Model",
    "title": "Spatial Lag Model",
    "content": "Data that is to some extent geographical in nature often displays spatial autocorrelation. Outcome variables and explanatory variables both tend to be clustered geographically, which can drive spurious correlations, or upward-biased treatment effect estimates (Ploton et al. 2020). One way to account for this spatial dependence is to model the autocorrelation directly, as would be done with autocorrated time-series data. One such model is the spatial lag model, in which a dependent variable is predicted using the value of the dependent variable of an observation’s “neighbors.” . \\[Y_i = \\rho W Y_j + \\beta X_i + \\varepsilon_i\\] Where $Y_j$ is the set of $Y$ values from observations other than $i$, and $W$ is a matrix of spatial weights, which are higher for $j$s that are spatially closer to $i$. This process requires estimation of which observations constitute neighbors, and generally the estimation of $\\rho$ is performed using a separate process from how $\\beta$ is estimated. More estimation details are in Darmofal (2015). ",
    "url": "/Geo-Spatial/spatial_lag_model.html",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html"
  },"695": {
    "doc": "Spatial Lag Model",
    "title": "Keep in Mind",
    "content": ". | There is more than one way to create the weighting matrix, and also more than one way to estimate the spatial lag model. Be sure to read the documentation to see what model and method your command is estimating, and that it’s the one you want. | Some approaches select a list of “neighbor” observations, such that each observation $j$ either is or is not a neighbor of $i$ (note that non-neighbors can still affect $i$ if they are neighbors-of-neighbors, and so on) | The effect of a given predictor in a spatial lag model is not just given by its coefficient, but should also include its spillover effects via $\\rho$. | . ",
    "url": "/Geo-Spatial/spatial_lag_model.html#keep-in-mind",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html#keep-in-mind"
  },"696": {
    "doc": "Spatial Lag Model",
    "title": "Also Consider",
    "content": ". | There are other ways of modeling spatial dependence, such as the Spatial Moving-Average Model | A common test to determine whether there is spatial dependence that needs to be modeled is the Moran Test | . ",
    "url": "/Geo-Spatial/spatial_lag_model.html#also-consider",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html#also-consider"
  },"697": {
    "doc": "Spatial Lag Model",
    "title": "Implementations",
    "content": "These examples will use some data on US colleges from IPEDS, including their latitude, longitude, and the extent of distance learning they offered in 2018. It will then see if this distance learning predicts (and perhaps reduces?) the prevalence of COVID in the college’s county by July 2020. ",
    "url": "/Geo-Spatial/spatial_lag_model.html#implementations",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html#implementations"
  },"698": {
    "doc": "Spatial Lag Model",
    "title": "Python",
    "content": "import pandas as pd from libpysal.cg import KDTree, RADIUS_EARTH_MILES from libpysal.weights import KNN from spreg import ML_Lag url = ('https://github.com/LOST-STATS/lost-stats.github.io/raw/source' '/Geo-Spatial/Data/Merging_Shape_Files/colleges_covid.csv') # specify index cols we need only for identification -- not modeling df = pd.read_csv(url, index_col=['unitid', 'instnm']) # we'll `pop` renaming columns so they're no longer in our dataframe x = df.copy().dropna(how='any') # tree object is the main input to nearest neighbors tree = KDTree( data=zip(x.pop('longitude'), x.pop('latitude')), # default is euclidean, but we want to use arc or haversine distance distance_metric='arc', radius=RADIUS_EARTH_MILES ) nn = KNN(tree, k=5) y = x.pop('covid_cases_per_cap_jul312020') # spreg only accepts numpy arrays or lists as arguments mod = ML_Lag( y=y.to_numpy(), x=x.to_numpy(), w=nn, name_y=y.name, name_x=x.columns.tolist() ) # results print(mod.summary) . ",
    "url": "/Geo-Spatial/spatial_lag_model.html#python",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html#python"
  },"699": {
    "doc": "Spatial Lag Model",
    "title": "R",
    "content": "# if necessary # install.packages(c('spatialreg', 'spdep')) # Library for calculating neighbors library(spdep) # And for the spatial lag model library(spatialreg) # Load data df &lt;- read.csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Geo-Spatial/Data/Merging_Shape_Files/colleges_covid.csv') # Use latitude and longitude to determine the list of neighbors # Here we're using K-nearest-neighbors to find 5 neighbors for each college # But there are othe rmethods available # Get latitude and longitude into a matrix # Make sure longitude comes first loc_matrix &lt;- as.matrix(df[, c('longitude','latitude')]) # Get 5 nearest neighbors kn &lt;- knearneigh(loc_matrix, 5) # Turn the k-nearest-neighbors object into a neighbors object nb &lt;- knn2nb(kn) # Turn the nb object into a listw object # Which is a list of spatial weights for the neighbors listw &lt;- nb2listw(nb) # Use a spatial regression # This uses the method from Bivand &amp; Piras (2015) https://www.jstatsoft.org/v63/i18/. m &lt;- lagsarlm(covid_cases_per_cap_jul312020 ~ pctdesom + pctdenon, data = df, listw = listw) # Note that, whlie summary(m) will show rho below the regression results, # most regression-table functions like modelsummary::msummary() or jtools::export_summs() # will include it as a coefficient along with the others and report its standard error summary(m) . ",
    "url": "/Geo-Spatial/spatial_lag_model.html#r",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html#r"
  },"700": {
    "doc": "Spatial Lag Model",
    "title": "Stata",
    "content": "Stata has a suite of built-in spatial analysis commands, which we will be using here. A more thorough description of using Stata for spatial autocorrelation models (and perhaps using shapefiles to start with) can be found here. * Import data import delimited using \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Geo-Spatial/Data/Merging_Shape_Files/colleges_covid.csv\", clear * This process requires full data drop if missing(pctdesom) | missing(pctdenon) * Get Stata to recognize this is a spatial dataset * with longitude and latitude spset unitid, coord(longitude latitude) * Create matrix of inverse distance weights spmatrix create idistance M * Note that Stata doesn't have an automatic process for selecting a set of neighbors * Unless you are working with a shapefile * Run spatial regression model * This uses a maximum likelihood estimator, but GS2SLS is also available spregress covid_cases_per_cap_jul312020 pctdesom pctdenon, ml dvarlag(M) * Get impact of each predictor, including spillovers, with estat impact estat impact . ",
    "url": "/Geo-Spatial/spatial_lag_model.html#stata",
    
    "relUrl": "/Geo-Spatial/spatial_lag_model.html#stata"
  },"701": {
    "doc": "Stepwise Regression",
    "title": "Stepwise Regression",
    "content": "When we use multiple explanatory variables to perform regression analysis on a dependent variable, there is a possibility that the problem of multicollinearity will occur. However, multiple linear regression requires that the correlation between the independent variables is not too high, so there is value in a method to eliminate multicollinearity and select the “optimal” regression equation. Stepwise regression is one approach to this. It can automatically help us retain the most important explanatory variables and remove relatively unimportant variables from the model. The idea of stepwise regression is to introduce independent variables one by one, and after each independent variable is introduced, the selected variables are tested one by one. If the originally introduced variable is no longer significant due to the introduction of subsequent variables, then delete it. Repeat this process until the regression equation does not introduce insignificant independent variables and does not remove significant independent variables, then the optimal regression equation can be obtained. ",
    "url": "/Model_Estimation/OLS/stepwise_regression.html",
    
    "relUrl": "/Model_Estimation/OLS/stepwise_regression.html"
  },"702": {
    "doc": "Stepwise Regression",
    "title": "Keep in Mind",
    "content": ". | The purpose of stepwise regression is to find which combination of variables can explain more changes in dependent variables. | Stepwise regression uses statistical measures such as R-square, t-stats, and AIC indicators to identify important variables. | There are three methods of stepwise regression: Forward Selection, Backward Elimination and Stepwise Selection. | Forward selection starts from the most important independent variable in the model, and then increases the variable in each step. | Backward elimination starts with all the independent variables of the model, and then removes the least significant variable at each step. | The standard stepwise selection combines the above two methods, adding or removing independent variables in each step. | Standard stepwise regression approaches use statistical significance to make decisions about model design, which is not the typical purpose of statistical significance | . ",
    "url": "/Model_Estimation/OLS/stepwise_regression.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/OLS/stepwise_regression.html#keep-in-mind"
  },"703": {
    "doc": "Stepwise Regression",
    "title": "Also Consider",
    "content": ". | Penalized regression, specifically the LASSO approach to model selection. | . ",
    "url": "/Model_Estimation/OLS/stepwise_regression.html#also-consider",
    
    "relUrl": "/Model_Estimation/OLS/stepwise_regression.html#also-consider"
  },"704": {
    "doc": "Stepwise Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/stepwise_regression.html#implementations",
    
    "relUrl": "/Model_Estimation/OLS/stepwise_regression.html#implementations"
  },"705": {
    "doc": "Stepwise Regression",
    "title": "R",
    "content": "We will use the built-in mtcars dataset. The step() function in package stats can perform the stepwise regression. Set up . # Load package library(stats) library(broom) # Load data and take a look at this dataset data(mtcars) head(mtcars) # mpg cyl disp hp drat wt qsec vs am gear carb # Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 # Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 # Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 # Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 # Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 # Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Define a regression model mpg ~ all other independent variables. reg_mpg &lt;- lm(mpg ~ ., data=mtcars) # Define intercept model intercept &lt;- lm(mpg ~ 1, data=mtcars) . Stepwise Selection . # Stepwise selection # The direction argument can be changed to perform forwards or backwards selection stepwise &lt;- step(intercept, direction = c(\"both\"), scope=formula(reg_mpg)) # Start: AIC=115.94 # mpg ~ 1 # Df Sum of Sq RSS AIC # + wt 1 847.73 278.32 73.217 # + cyl 1 817.71 308.33 76.494 # + disp 1 808.89 317.16 77.397 # + hp 1 678.37 447.67 88.427 # + drat 1 522.48 603.57 97.988 # + vs 1 496.53 629.52 99.335 # + am 1 405.15 720.90 103.672 # + carb 1 341.78 784.27 106.369 # + gear 1 259.75 866.30 109.552 # + qsec 1 197.39 928.66 111.776 # &lt;none&gt; 1126.05 115.943 # Omit the filter in the middle... # Step: AIC=62.66 # mpg ~ wt + cyl + hp # Df Sum of Sq RSS AIC # &lt;none&gt; 176.62 62.665 # - hp 1 14.551 191.17 63.198 # + am 1 6.623 170.00 63.442 # + disp 1 6.176 170.44 63.526 # - cyl 1 18.427 195.05 63.840 # + carb 1 2.519 174.10 64.205 # + drat 1 2.245 174.38 64.255 # + qsec 1 1.401 175.22 64.410 # + gear 1 0.856 175.76 64.509 # + vs 1 0.060 176.56 64.654 # - wt 1 115.354 291.98 76.750 . # Result tidy(stepwise) # A tibble: 4 x 5 # term estimate std.error statistic p.value # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 (Intercept) 38.8 1.79 21.7 4.80e-19 # 2 wt -3.17 0.741 -4.28 1.99e- 4 # 3 cyl -0.942 0.551 -1.71 9.85e- 2 # 4 hp -0.0180 0.0119 -1.52 1.40e- 1 . The optimal equation we get from stepwise selection is . \\[mpg = 38.752 - 3.167*wt - 0.942*cyl - 0.018*hyp\\] ",
    "url": "/Model_Estimation/OLS/stepwise_regression.html#r",
    
    "relUrl": "/Model_Estimation/OLS/stepwise_regression.html#r"
  },"706": {
    "doc": "Styling Line Graphs",
    "title": "Styling Line Graphs",
    "content": "There are several ways of styling line graphs. The following examples demonstrate how to modify the appearances of the lines (type and sizes), as well chart titles and axes labels. ",
    "url": "/Presentation/Figures/styling_line_graphs.html",
    
    "relUrl": "/Presentation/Figures/styling_line_graphs.html"
  },"707": {
    "doc": "Styling Line Graphs",
    "title": "Keep in Mind",
    "content": ". | To get started on how to plot line graphs, see here. | Elements for customization include line thickness, line type (solid, dashed, etc.), shade, transparency, and color. | Color is one of the easiest ways to distinguish a large number of line graphs. If you have many line graphs overlaid and have to use black-and-white, consider different shades of black/gray. | . ",
    "url": "/Presentation/Figures/styling_line_graphs.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#keep-in-mind"
  },"708": {
    "doc": "Styling Line Graphs",
    "title": "Implementation",
    "content": " ",
    "url": "/Presentation/Figures/styling_line_graphs.html#implementation",
    
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#implementation"
  },"709": {
    "doc": "Styling Line Graphs",
    "title": "Python",
    "content": "import pandas as pd import seaborn.objects as so import numpy as np import matplotlib.pyplot as plt from seaborn import axes_style # Download the economics dataset (from ggplot2 so comparison is apples-to-apples) url = \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/economics.csv\" economics = pd.read_csv(url) # Quick manipulation of dataframe to convert column to datetime df = ( economics .assign( date = lambda df: pd.to_datetime(df['date']) ) ) # Default plots (Notice the xaxis only has 2 years! We'll fix this in p2) p1 = ( so.Plot(data=df, x='date', y='uempmed') .add(so.Line()) ) p1 ## Change line color and chart labels, and fix xaxis ## Note here that color is inside of the Line call, so this would color the line. ## If color were instead *inside* the so.Plot() object, SO would assign it ## a different line for each value of the factor variable (column), colored differently. (Commonly referred to as hue in seaborn) # However, in our case, we can pass a color directly. p2 = ( so.Plot(data=df, x='date', y='uempmed') .add(so.Line(color='purple')) .label(title='Median Duration of Unemploymeny', x='Date', y='') .scale(x=so.Temporal().tick(upto=10)) #Needed for current configuration of seaborn.objects so xaxis prints more than 2 ticks .theme(axes_style(\"whitegrid\")) #use a function from parent seaborn library, that will pass a prebuilt selection based on what you pass ) p2 ## plotting multiple charts (of different line types and sizes) p3 = ( so.Plot(data=df) .add(so.Line(color='darkblue', linewidth=5), x='date', y='uempmed') .add(so.Line(color='red', linewidth=2, linestyle='dotted'), x='date', y='psavert') .label(title='Unemployment Duration (Blue)\\n &amp; Savings Rate (Red)', x='Date', y='') .scale(x=so.Temporal().tick(upto=10)) #Needed for current configuration of seaborn.objects so xaxis prints more than 2 ticks .theme(axes_style(\"whitegrid\")) #use a function from parent seaborn library, that will pass a prebuilt selection based on what you pass ) p3 ## Plotting a different line type for each group ## There isn't a natural factor in this data so let's just duplicate the data and make one up df['fac'] = 1 df2 = df.copy() df2['fac'] = 2 df2['uempmed'] = df2['uempmed'] - 2 + np.random.normal(size=len(df2)) df_final = pd.concat([df, df2], ignore_index=True).astype({'fac':'category'}) p4 = ( so.Plot(data=df_final, x='date', y='uempmed', color='fac') .add(so.Line()) .label(title = \"Median Duration of Unemployment\", x = \"Date\", y = \"\", color='Random Factor') .scale(x=so.Temporal().tick(upto=10)) #Needed for current configuration of seaborn.objects so xaxis prints more than 2 ticks .theme(axes_style(\"whitegrid\")) #use a function from parent seaborn library, that will pass a prebuilt selection based on what you pass ) p4 # Plot all 4 plots fig, axs = plt.subplots(2, 2, figsize=(10, 8)) # Draw each plot in the corresponding subplot p1.on(axs[0, 0]).plot() p2.on(axs[0, 1]).plot() p3.on(axs[1, 0]).plot() p4.on(axs[1, 1]).plot() # Adjust layout to avoid overlap plt.tight_layout() # Show the combined plot plt.show() . The four plots generated by the code are (in order p1, p2, then p3 and p4): . ",
    "url": "/Presentation/Figures/styling_line_graphs.html#python",
    
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#python"
  },"710": {
    "doc": "Styling Line Graphs",
    "title": "R",
    "content": "## If necessary ## install.packages(c('ggplot2','cowplot')) ## load packages library(ggplot2) ## Cowplot is just to join together the four graphs at the end library(cowplot) ## load data (the Economics dataset comes with ggplot2) eco_df &lt;- economics ## basic plot p1 &lt;- ggplot() + geom_line(aes(x=date, y = uempmed), data = eco_df) p1 ## Change line color and chart labels ## Note here that color is *outside* of the aes() argument, and so this will color the line ## If color were instead *inside* aes() and set to a factor variable, ggplot would create ## a different line for each value of the factor variable, colored differently. p2 &lt;- ggplot() + ## choose a color of preference geom_line(aes(x=date, y = uempmed), color = \"navyblue\", data = eco_df) + ## add chart title and change axes labels labs( title = \"Median Duration of Unemployment\", x = \"Date\", y = \"\") + ## Add a ggplot theme theme_light() ## center the chart title theme(plot.title = element_text(hjust = 0.5)) + p2 ## plotting multiple charts (of different line types and sizes) p3 &lt;-ggplot() + geom_line(aes(x=date, y = uempmed), color = \"navyblue\", size = 1.5, data = eco_df) + geom_line(aes(x=date, y = psavert), color = \"red2\", linetype = \"dotted\", size = 0.8, data = eco_df) + labs( title = \"Unemployment Duration (Blue) and Savings Rate (Red)\", x = \"Date\", y = \"\") + theme_light() + theme(plot.title = element_text(hjust = 0.5)) p3 ## Plotting a different line type for each group ## There isn't a natural factor in this data so let's just duplicate the data and make one up eco_df$fac &lt;- factor(1, levels = c(1,2)) eco_df2 &lt;- eco_df eco_df2$fac &lt;- 2 eco_df2$uempmed &lt;- eco_df2$uempmed - 2 + rnorm(nrow(eco_df2)) eco_df &lt;- rbind(eco_df, eco_df2) p4 &lt;- ggplot() + ## This time, color goes inside aes geom_line(aes(x=date, y = uempmed, color = fac), data = eco_df) + ## add chart title and change axes labels labs( title = \"Median Duration of Unemployment\", x = \"Date\", y = \"\") + ## Add a ggplot theme theme_light() + ## center the chart title theme(plot.title = element_text(hjust = 0.5), ## Move the legend onto some blank space on the diagram legend.position = c(.25,.8), ## And put a box around it legend.background = element_rect(color=\"black\")) + ## Retitle the legend that pops up to explain the discrete (factor) difference in colors ## (note if we just want a name change we could do guides(color = guide_legend(title = 'Random Factor')) instead) scale_color_manual(name = \"Random Factor\", # And specify the colors for the factor levels (1 and 2) by hand if we like values = c(\"1\" = \"red\", \"2\" = \"blue\")) p4 # Put them all together with cowplot for LOST upload plot_grid(p1,p2,p3,p4, nrow=2) . The four plots generated by the code are (in order p1, p2, then p3 and p4): . ",
    "url": "/Presentation/Figures/styling_line_graphs.html#r",
    
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#r"
  },"711": {
    "doc": "Styling Line Graphs",
    "title": "Stata",
    "content": "In Stata, one can create plot lines using the command line, which in combination with twoway allows you to modify components of sub-plots individually. In this demonstration, I will use minimal formatting, but will apply minimal modifications using Ben Jann’s grstyle. ** Setup: Install grstyle ssc install grstyle grstyle init grstyle color background white grstyle set legend, nobox . Setup . First, you need to load the data into Stata. The data is a copy from the data economics available within ggplot package, and translated using foreign. use https://friosavila.github.io/playingwithstata/rnd_dta/economics, clear ** Since this was taken directly from R, the date variable will not be formatted. ** We can format the date using the following. format date %tdCCYY ** This indicates to create a _mask_, to put on top of \"data\" ** but only display the \"year\" . Simple line plot . Now, For a simple plot, we could use the following syntax: . line yvar1 [yvar2 yvar3 ...] xvar1 . This requests plotting all variables yvarX against xvar1 (horizontal axis). Internally, the command connects every pair of data [yvar1,xvar1] sequentially, based on the order they appear in the dataset. Below, we can do that, plotting unemployment duration uempmed vs date. line uempmed date . Something to keep in mind. If the dataset is not sorted by date, you may end up with a lineplot that is all over the place. For example: . sort uempmed line uempmed date . To avoid this, it is recommended to use the option sort. line uempmed date, sort . Adding titles, and axis titles . The next thing you may want to do is add information to the plot, so its easier to understand what the figure is showing. Specifically, we can add information on the vertical axis using ytitle(). I will also use xtitle() to drop the horizontal axis information, and add a title title(). line uempmed date, sort /// ytitle(\"# of weeks\") xtitle(\"\") /// title(Unemployment Duration) . Changing Line characteristics. It is also possible to modify the line width lwidth(), line color lcolor(), and line pattern lpattern(). To show how this can affect the plot, below 4 examples are provided. Notice that each plot is saved in memory using name(), and all are combined using graph combine. line uempmed date, sort /// ytitle(\"# of weeks\") xtitle(\"\") /// title(Unemployment Duration 1) /// lwidth(.5) lcolor(red) lpattern(solid) name(m1,replace) line uempmed date, sort /// ytitle(\"# of weeks\") xtitle(\"\") /// title(Unemployment Duration 2) /// lwidth(.25) lcolor(gold) lpattern(dash) name(m2,replace) line uempmed date, sort /// ytitle(\"# of weeks\") xtitle(\"\") /// title(Unemployment Duration 3) /// lwidth(1) lcolor(\"68 170 153\") lpattern(dot) name(m3,replace) line uempmed date, sort /// ytitle(\"# of weeks\") xtitle(\"\") /// title(Unemployment Duration 4) /// lwidth(.5) lcolor(navy%50) lpattern(dash_dot) name(m4,replace) graph combine m1 m2 m3 m4 . Ploting Multiple Lines, and different axis . You may also want to plot multiple variables in the same figure. There are two ways to do this: . twoway (line uempmed date, sort lwidth(.75) lpattern(solid) ) /// (line psavert date, sort lwidth(.25) lpattern(dash) ), /// legend (order(1 \"Unemployment duration\" 2 \"Saving rate\")) line uempmed psavert date, sort lwidth(0.75 .25) lpattern(solid dash) /// legend(order(1 \"Unemployment duration\" 2 \"Saving rate\")) . Both options provide the same figure, however, I prefer the first option since that allows for more flexibility. You can also choose to plot each variable in a different axis. Each axis can have its own title. twoway (line uempmed date, sort lwidth(.75) lpattern(solid) yaxis(1)) /// (line psavert date, sort lwidth(.25) lpattern(dash) yaxis(2)), /// legend(order(1 \"Unemployment duration\" 2 \"Saving rate\")) /// ytitle(Weeks ,axis(1) ) ytitle(Interest rate,axis(2) ) . Adding informative Vertical lines. Finally, it is possible to add vertical lines. This may be useful, for example, to differentiate the great recession period. Additionally, in this plot, I add a note. twoway (line uempmed date, sort lwidth(.75) lpattern(solid) yaxis(1)) /// (line psavert date, sort lwidth(.25) lpattern(dash) yaxis(2)), /// legend(order(1 \"Unemployment duration\" 2 \"Saving rate\")) /// ytitle(Weeks ,axis(1) ) ytitle(Interest rate,axis(2) ) /// xline(`=td(1dec2007)'/`=td(30jun2008)', lcolor(gs8)) /// note(\"Note:Grey Area marks the Great recession Period\") /// title(\"Unemployement Duration and\" \"Saving Rate\") . ",
    "url": "/Presentation/Figures/styling_line_graphs.html#stata",
    
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#stata"
  },"712": {
    "doc": "Graphing a By-Group or Over-Time Summary Statistic",
    "title": "Graphing a By-Group or Over-Time Summary Statistic",
    "content": "A common task in exploring or presenting data is looking at by-group summary statistics. This commonly takes the form of a graph where the group is along the x-axis and the summary statistic is on the y-axis. Often this group might be a time period so as to look at changes over time. Producing such a graph requires three things: . | A decision of what kind of graph will be produced (line graph, bar graph, scatterplot, etc.) | The creation of the grouped summary statistic | The creation of the graph itself | . ",
    "url": "/Presentation/Figures/summary_graphs.html",
    
    "relUrl": "/Presentation/Figures/summary_graphs.html"
  },"713": {
    "doc": "Graphing a By-Group or Over-Time Summary Statistic",
    "title": "Keep in Mind",
    "content": ". | Line graphs are only intended for use in cases where the x-axis variable (1) is ordinal (one value is “more” than another), and (2) takes consistently-sized jumps from one observation to the next. A time-based x-axis is a good candidate for use with line graphs. If your group is categorical and doesn’t follow a natural ordering, then do not use a line graph. Consider a bar graph or some other kind of graph instead. | If you are making a graph for presentation rather than exploration, and your x-axis variable is categorical and doesn’t have a natural ordering, your graph will often be easier to read if the x-axis is sorted by the height of the y-axis. The way to do this will be demonstrated in the code examples below. | . ",
    "url": "/Presentation/Figures/summary_graphs.html#keep-in-mind",
    
    "relUrl": "/Presentation/Figures/summary_graphs.html#keep-in-mind"
  },"714": {
    "doc": "Graphing a By-Group or Over-Time Summary Statistic",
    "title": "Also Consider",
    "content": ". | This page will cover how to calculate the summary statistic in the graph code itself. However, an alternate approach that provides a bit more control and flexibility is to calculate the by-group summary statistic by collapsing the data set so there is only one observation per group in the data. Then, just make a regular graph of whatever kind you like, with the group along the x-axis, and the summary statistic on the y-axis. See Line Graphs or Bar Graphs. | If you want a version of these graphs that has two groupings - one group along the x-axis and with different bars or lines for another group, see how to graph multiple lines on Line Graphs or multiple bars per x-axis point on Bar Graphs. | . ",
    "url": "/Presentation/Figures/summary_graphs.html#also-consider",
    
    "relUrl": "/Presentation/Figures/summary_graphs.html#also-consider"
  },"715": {
    "doc": "Graphing a By-Group or Over-Time Summary Statistic",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/summary_graphs.html#implementations",
    
    "relUrl": "/Presentation/Figures/summary_graphs.html#implementations"
  },"716": {
    "doc": "Graphing a By-Group or Over-Time Summary Statistic",
    "title": "R",
    "content": "# We want ggplot2 for graphing and dplyr for the storms data library(tidyverse) data(storms) # First, a line graph with time on the x-axis # This uses stat_summary # Note that stat_summary_bin is also available, # which first bins the x-axis, if desired # Put the time variable in the x aesthetic, and the # variable to be summarized in y ggplot(storms, aes(x = year, y = wind)) + stat_summary(geom = 'line', # Do we want a line graph? Point? fun = mean) + # What function should be used to summarize? # Note another good option for geom is 'pointrange', the default # which you can get from just stat_summary(), # which also shows the range of data # Just decoration: labs(x = 'Year', y = 'Average Wind Speed', title = 'Average Wind Speed of Storms by Year') + theme_minimal() # Second, a bar graph with a category on the x-axis # Use reorder() to sort by which status has the most wind ggplot(storms, aes(x = reorder(status,-wind), y = wind)) + stat_summary(geom = 'bar', # Do we want a line graph? Point? fun = mean) + # Decoration: scale_x_discrete(labels = c('Hurricane','Tropical Storm','Tropical Depression')) + # make the labels more presentable # Decoration: labs(x = NULL, y = 'Average Wind Speed', title = 'Average Wind Speed by Storm Type') + theme_minimal() . This code produces: . ",
    "url": "/Presentation/Figures/summary_graphs.html#r",
    
    "relUrl": "/Presentation/Figures/summary_graphs.html#r"
  },"717": {
    "doc": "Graphing a By-Group or Over-Time Summary Statistic",
    "title": "Stata",
    "content": "In Stata there is not a single graph command that will graph a summary statistic line graph for us (although there is for bar graphs). Instead, for line graphs, we must collapse the data set and graph the result. You could avoid collapsing by instead using bysort group: egen newvar = mean(oldvar) (or some egen function from help egen other than mean) to create by-group statistics in the original data, use egen tag = tag(group) to select only one observation per group, and then do the below graphing commands while adding if tag == 1 to them. ** Read in the data import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", clear * Keep the original data to return to after collapsing preserve * First, a line graph with time on the x-axis and average wind on y collapse (mean) wind, by(year) * Then, a line graph tw line wind year, xtitle(\"Year\") ytitle(\"Average Wind Speed\") restore * Now, a bar graph with a category on the x-axis graph bar (mean) wind, over(status, relabel(1 \"Hurricane\" 2 \"Tropical Depression\" 3 \"Tropical Storm\") /// Relabel the statuses to capitalize sort((mean) wind)) /// Put in height order automatically ytitle(\"Average Wind Speed\") . This code produces: . ",
    "url": "/Presentation/Figures/summary_graphs.html#stata",
    
    "relUrl": "/Presentation/Figures/summary_graphs.html#stata"
  },"718": {
    "doc": "Support Vector Machine",
    "title": "Support Vector Machine",
    "content": "A support vector machine (hereinafter, SVM) is a supervised machine learning algorithm in that it is trained by a set of data and then classifies any new input data depending on what it learned during the training phase. SVM can be used both for classification and regression problems but here we focus on its use for classification. The idea is to separate two distinct groups by maximizing the distance between those points that are most hard to classify. To put it more formally, it maximizes the distance or margin between support vectors around the separating hyperplane. Support vectors here imply the data points that lie closest to the hyperplane. Hyperplanes are decision boundaries that are represented by a line (in two dimensional space) or a plane (in three dimensional space) that separate the two groups. Suppose a hypothetical problem of classifying apples from lemons. Support vectors in this case are apples that look closest to lemons and lemons that look closest to apples. They are the most difficult ones to classify. SVM draws a separating line or hyperplane that maximizes the distance or margin between support vectors, in this case the apples that look closest to the lemons and lemons that look closest to apples. Therefore support vectors are critical in determining the position as well as the slope of the hyperplane. For additional information about the support vector regression or support vector machine, refer to Wikipedia: Support-vector machine. ",
    "url": "/Machine_Learning/support_vector_machine.html",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html"
  },"719": {
    "doc": "Support Vector Machine",
    "title": "Keep in Mind",
    "content": ". | Note that optimization problem to solve for a linear separator is maximizing the margin which could be calculated as \\(\\frac{2}{\\lVert w \\rVert}\\). This could then be rewritten as minimizing \\(\\lVert w \\rVert\\), or minimizing a monotonic transformation version of it expressed as \\(\\frac{1}{2}\\lVert w \\rVert^2\\). Additional constraint of \\(y_i(w^T x_i + b) \\geq 1\\) needs to be imposed to ensure that the data points are still correctly classified. As such, the constrained optimization problem for SVM looks as the following: | . \\[\\text{min} \\frac{\\lVert w \\rVert ^2}{2}\\] s.t. \\(y_i(w^T x_i + b) \\geq 1\\), . where \\(w\\) is a weight vector, \\(x_i\\) is each data point, \\(b\\) is bias, and \\(y_i\\) is each data point’s corresponding label that takes the value of either \\(\\{-1, 1\\}\\). For detailed information about derivation of the optimization problem, refer to MIT presentation slides, The Math Behind Support Vector Machines, and Demystifying Maths of SVM - Part1. | If data points are not linearly separable, non-linear SVM introduces higher dimensional space that projects data points from original finite-dimensional space to gain linearly separation. Such process of mapping data points into a higher dimensional space is known as the Kernel Trick. There are numerous types of Kernels that can be used to create higher dimensional space including linear, polynomial, Sigmoid, and Radial Basis Function. | Setting the right form of Kernel is important as it determines the structure of the separator or hyperplane. | . ",
    "url": "/Machine_Learning/support_vector_machine.html#keep-in-mind",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html#keep-in-mind"
  },"720": {
    "doc": "Support Vector Machine",
    "title": "Also Consider",
    "content": ". | See the alternative classification method described on the K-Nearest Neighbor Matching. | . ",
    "url": "/Machine_Learning/support_vector_machine.html#also-consider",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html#also-consider"
  },"721": {
    "doc": "Support Vector Machine",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/support_vector_machine.html#implementations",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html#implementations"
  },"722": {
    "doc": "Support Vector Machine",
    "title": "Python",
    "content": "In this example, we will use scikit-learn, which is a very popular Python library for machine learning. We will look at two support vector machine models: LinearSVC, which performs linear support vector classification (example 1); and SVC, which can accept several different kernels (including non-linear ones). For the latter case, we’ll use the non-linear radial basis function kernel (example 2 below). The last part of the code example plots the decision boundary, ie the support vectors, for the second example. from sklearn.datasets import make_classification, make_gaussian_quantiles from sklearn.svm import LinearSVC, SVC from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np ########################### # Example 1: Linear SVM ### ########################### # Generate linearly separable data: X, y = make_classification(n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2) # Train linear SVM model svm = LinearSVC(tol=1e-5) svm.fit(X_train, y_train) # Test model test_score = svm.score(X_test, y_test) print(f'The test score is {test_score}') ############################### # Example 2: Non-linear SVM ### ############################### # Generate non-linearly separable data X, y = make_gaussian_quantiles(n_features=2, n_classes=2) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2) # Train non-linear SVM model nl_svm = SVC(kernel='rbf', C=50) nl_svm.fit(X_train, y_train) # Test model test_score = nl_svm.score(X_test, y_test) print(f'The non-linear test score is {test_score}') #################################### # Plot non-linear SVM boundaries ### #################################### plt.figure() decision_function = nl_svm.decision_function(X) support_vector_indices = np.where( np.abs(decision_function) &lt;= 1 + 1e-15)[0] support_vectors = X[support_vector_indices] plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired) ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)) Z = nl_svm.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--']) plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k') plt.tight_layout() plt.show() . ",
    "url": "/Machine_Learning/support_vector_machine.html#python",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html#python"
  },"723": {
    "doc": "Support Vector Machine",
    "title": "R",
    "content": "There are a couple of ways to implement SVM in R. Here we’ll demonstrate using the e1071 package. To learn more about the package, check out its CRAN page, as well as this vignette. Note that we’ll also load the tidyverse to help with some data wrangling and plotting. Two examples are shown below that use linear SVM and non-linear SVM respectively. The first example shows how to implement linear SVM. We start by constructing data, separating them into training and test set. Using the training set, we fit the data using the svm() function. Notice that kernel argument for svm() function is specified as linear for our first example. Next, we predict the test data based on the model estimates using the predict() function. The first example result suggests that only one out of 59 data points is incorrectly classified. The second example shows how to implement non-linear SVM. The data in example two is generated in a way to have data points of one class centered around the middle whereas data points of the other class spread on two sides. Notice that kernel argument for the svm() function is specified as radial for our second example, based on the shape of the data. The second example result suggests that only two out of 58 data points are incorrectly classified. # Install and load the packages if (!require(\"tidyverse\")) install.packages(\"tidyverse\") if (!require(\"e1071\")) install.packages(\"e1071\") library(tidyverse) # package for data manipulation library(e1071) # package for SVM ########################### # Example 1: Linear SVM ### ########################### # Construct a completely separable data set ## Set seed for replication set.seed(0715) ## Make variable x x = matrix(rnorm(200, mean = 0, sd = 1), nrow = 100, ncol = 2) ## Make variable y that labels x by either -1 or 1 y = rep(c(-1, 1), c(50, 50)) ## Make x to have unilaterally higher value when y equals 1 x[y == 1,] = x[y == 1,] + 3.5 ## Construct data set d1 = data.frame(x1 = x[,1], x2 = x[,2], y = as.factor(y)) ## Split it into training and test data flag = sample(c(0,1), size = nrow(d1), prob=c(0.5,0.5), replace = TRUE) d1 = setNames(split(d1, flag), c(\"train\", \"test\")) # Plot ggplot(data = d1$train, aes(x = x1, y = x2, color = y, shape = y)) + geom_point(size = 2) + scale_color_manual(values = c(\"darkred\", \"steelblue\")) # SVM classification svmfit1 = svm(y ~ ., data = d1$train, kernel = \"linear\", cost = 10, scale = FALSE) print(svmfit1) plot(svmfit1, d1$train) # Predictability pred.d1 = predict(svmfit1, newdata = d1$test) table(pred.d1, d1$test$y) ############################### # Example 2: Non Linear SVM ### ############################### # Construct less separable data set ## Make variable x x = matrix(rnorm(200, mean = 0, sd = 1), nrow = 100, ncol = 2) ## Make variable y that labels x by either -1 or 1 y &lt;- rep(c(-1, 1) , c(50, 50)) ## Make x to have extreme values when y equals 1 x[y == 1, ][1:25,] = x[y==1,][1:25,] + 3.5 x[y == 1, ][26:50,] = x[y==1,][26:50,] - 3.5 ## Construct data set d2 = data.frame(x1 = x[,1], x2 = x[,2], y = as.factor(y)) ## Split it into training and test data d2 = setNames(split(d2, flag), c(\"train\", \"test\")) # Plot data ggplot(data = d2$train, aes(x = x1, y = x2, color = y, shape = y)) + geom_point(size = 2) + scale_color_manual(values = c(\"darkred\", \"steelblue\")) # SVM classification svmfit2 = svm(y ~ ., data = d2$train, kernel = \"radial\", cost = 10, scale = FALSE) print(svmfit2) plot(svmfit2, d2$train) # Predictability pred.d2 = predict(svmfit2, newdata = d2$test) table(pred.d2, d2$test$y) . ",
    "url": "/Machine_Learning/support_vector_machine.html#r",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html#r"
  },"724": {
    "doc": "Support Vector Machine",
    "title": "Stata",
    "content": "The below code shows how to implement support vector machines in Stata using the svmachines command. To learn more about this community contriuted command, you can read this Stata Journal article. clear all set more off *Install svmachines ssc install svmachines *Import Data with a binary outcome for classification use http://www.stata-press.com/data/r16/fvex.dta, clear *First try logistic regression to benchmark the prediction quality of SVM against logit outcome group sex arm age distance y // Run the regression predict outcome_predicted // Generate predictions from the regression *Calculate the log loss - see https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html for more info gen log_loss = outcome*log(outcome_predicted)+(1-outcome)*log(1-outcome_predicted) *Run SVM svmachines outcome group sex arm age distance y, prob // Specifiying the prob option to generate predicted probabilities in the next line predict sv_outcome_predicted, probability . Next we will Calculate the log loss (or cross-entropy loss) for SVM. Note: Predictions following svmachines generate three variables from the stub you provide in the predict command (in this case sv_outcome_predicted). The first is just the same as the stub and stores the best-guess classification (the group with the highest probability out of the possible options). The next n variables store the probability that the given observation will fall into each of the possible classes (in the binary case, this is just n=2 possible classes). These new variables are the stub + the value of each class. In the case below, the suffixes are _0 and _1. We use sv_outcome_predicted_1 because it produces probabilities that are equivalent in their intepretation (probability of having a class of 1) to the probabilities produced by the logit model and that can be used in calculating the log loss. Calculating loss functions for multi-class classifiers is more complicated, and you can read more about that at the link above. gen log_loss_svm = outcome*log(sv_outcome_predicted_1)+(1-outcome)*log(1-sv_outcome_predicted_1) *Show log loss for both logit and SVM, remember lower is better sum log_loss log_loss_svm . ",
    "url": "/Machine_Learning/support_vector_machine.html#stata",
    
    "relUrl": "/Machine_Learning/support_vector_machine.html#stata"
  },"725": {
    "doc": "Synthetic Control",
    "title": "Synthetic Control Method (SCM)",
    "content": "Synthetic Control Method is a way of estimating the causal effect of an intervention in comparative case studies. It is typically used with a small number of large units (e.g. countries, states, counties) to estimate the effects of aggregate interventions. The idea is to construct a convex combination of similar untreated units (often referred to as the “donor pool”) to create a synthetic control that closely resembles the treatment subject and conduct counterfactual analysis with it. We have \\(j = 1, 2, ..., J+1\\) units, assuming without loss of generality that the first unit is the treated unit, \\(Y_{1t}\\). Denoting the potential outcome without intervention as \\(Y_{1t}^N\\), our goal is to estimate the treatment effect: . \\[\\tau_{1t} = Y_{1t} - Y_{1t}^N\\] We won’t have data for \\(Y_{1t}^N\\) but we can use synthetic controls to estimate it. Let the \\(k\\) x \\(J\\) matrix \\(X_0 = [X_2 ... X_{J+1}]\\) represent characteristics for the untreated units and the \\(k\\)-length vector \\(X_1\\) represent characteristics for the treatment unit. Last, define our \\(J\\times 1\\) vector of weights as \\(W = (w_2, ..., w_{J+1})'\\). Recall, these weights are used to form a convex combination of the untreated units. Now we have our estimate for the treatment effect: . \\[\\hat{\\tau_{1t}} = Y_{1t} - \\hat{Y_{1t}^N}\\] where \\(\\hat{Y_{1t}^N} = \\sum_{j=2}^{J+1} w_j Y_{jt}\\). The matrix of weights is found by choosing \\(W*\\) to minimize \\(\\|X_1 - X_0W\\|\\) such that \\(W &gt;&gt; 0\\) and \\(\\sum_2^{J+2} w_j = 1\\). Once you’ve found the \\(W*\\), you can put together an estimated \\(\\hat{Y_{1t}}\\) (synthetic control) for all time periods \\(t\\). Because our synthetic control was constructed from untreated units, when the intervention occurs at time \\(T_0\\), the difference between the synthetic control and the treated unit gives us our estimated treatment effect. As a last bit of intuition, below is a graph depicting the upshot of the method. The synthetic control follows a very similar path to the treated unit pre-intervention. The difference between the two curves, post-intervention, gives us our estimated treatment effect. Here is an excellent resource by Alberto Abadie (the economist who developed the method) if you’re interested in getting a more comprehensive overview of synthetic controls. ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html#synthetic-control-method-scm",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html#synthetic-control-method-scm"
  },"726": {
    "doc": "Synthetic Control",
    "title": "Keep in Mind",
    "content": ". | Unlike the difference-in-difference method, parallel trends aren’t a necessary assumption. However, the donor pool must still share similar characteristics to the treatment unit in order to construct an accurate estimate. | Panel data is necessary for the synthetic control method and, typically, requires observations over many time periods. Specifically, the pre-intervention time frame ought to be large enough to form an accurate estimate. | Aggregate data is required for this method. Examples include state-level per-capita GDP, country-level crime rates, and state-level alcohol consumption statistics. Additionally, if aggregate data doesn’t exist, you can sometimes aggregate micro-level data to estimate aggregate values. | As a caveat to the previous bullet point, be wary of structural breaks when using large pre-intervention periods. | Abadie and L’Hour (2020) also proposes a penalization method for performing the synthetic control method on disaggregated data. | . ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html#keep-in-mind"
  },"727": {
    "doc": "Synthetic Control",
    "title": "Also Consider",
    "content": ". | As stated before, this technique can be compared to difference-in-difference. If you don’t have aggregate data or don’t have sufficient data for the pre-intervention window and you have a control that you can confidently assume has a parallel trend to the treatment unit, then diff-in-diff might be better suited than SCM. | . ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html#also-consider",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html#also-consider"
  },"728": {
    "doc": "Synthetic Control",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html#implementations",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html#implementations"
  },"729": {
    "doc": "Synthetic Control",
    "title": "R",
    "content": "To implement the synthetic control method in R, we will be using the package Synth. While not used here, the SynthTools package also has a number of functions for making it easier to work with the Synth package. As stated above, the key part of the synthetic control method is to estimate the weight matrix \\(W*\\) in order to form accurate estimates of the treatment unit. The Synth package provides you with the tools to find the weight matrix. From there, you can construct the synthetic control by interacting the \\(W*\\) and the \\(Y\\) values from the donor pool. # First we will load Synth and dplyr. # If you haven't already installed Synth, now would be the time to do so library(dplyr) library(Synth) # We're going to use simulated data included in the Synth package for our example. # This dataframe consists of panel data including 1 outcome variable and 3 predictor variables for 1 treatment unit and 7 control units (donor pool) over 21 years data(\"synth.data\") # The primary function that we will use is the synth() function. # However, this function needs four particularly formed matrices as inputs, so it is highly recommended that you use the dataprep() function to generate the inputs. # Once we've gathered our dataprep() output, we can just use that as our sole input for synth() and we'll be good to go. # One important note is that your data must be in long format with id variables (integers) and name variables (character) for each unit. dataprep_out = dataprep( foo = synth.data, # first input is our data predictors = c(\"X1\", \"X2\", \"X3\"), # identify our predictor variables predictors.op = \"mean\", # operation to be performed on the predictor variables for when we form our X_1 and X_0 matrices. time.predictors.prior = c(1984:1989), # pre-intervention window dependent = \"Y\", # outcome variable unit.variable = \"unit.num\", # identify our id variable unit.names.variable = \"name\", # identify our name variable time.variable = \"year\", # identify our time period variable treatment.identifier = 7, # integer that indicates the id variable value for our treatment unit controls.identifier = c(2, 13, 17, 29, 32, 36, 38), # vector that indicates the id variable values for the donor pool time.optimize.ssr = c(1984:1990), # identify the time period you want to optimize over to find the W*. Includes pre-treatment period and the treatment year. time.plot = c(1984:1996) # periods over which results are to be plotted with Synth's plot functions ) # Now we have our data ready in the form of a list. We have all the matrices we need to run synth() # Our output from the synth() function will be a list that includes our optimal weight matrix W* synth_out = dataprep_out %&gt;% synth() # From here, we can plot the treatment variable and the synthetic control using Synth's plot function. # The variable tr.intake is an optional variable if you want a dashed vertical line where the intervention takes place. synth_out %&gt;% path.plot(dataprep.res = dataprep_out, tr.intake = 1990) # Finally, we can construct our synthetic control variable if we wanted to conduct difference-in-difference analysis on it to estimate the treatment effect. synth_control = dataprep_out$Y0plot %*% synth_out$solution.w . ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html#r",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html#r"
  },"730": {
    "doc": "Synthetic Control",
    "title": "Stata",
    "content": "To implement the synthetic control method in Stata, we will be using the synth and synth_runner packages. For a short tutorial on how to carry out the synthetic control method in Stata by Jens Hainmueller, there is a useful video here. *Install plottig graph scheme used below ssc install blindschemes *Install synth and synth_runner if they're not already installed (uncomment these to install) * ssc install synth, all * cap ado uninstall synth_runner //in-case already installed * net install synth_runner, from(https://raw.github.com/bquistorff/synth_runner/master/) replace *Import Dataset sysuse synth_smoking.dta, clear *Need to set the data as time series, using tsset tsset state year . Next we will run the synthetic control analysis using synth_runner, which adds some useful options for estimation. Note that this example uses the pre-treatment outcome for just three years (1988, 1980, and 1975), but any combination of pre-treatment outcome years can be specified. The nested option specifies a more computationally intensive but comprehensive method for estimating the synthetic control. The trunit() option specifies the ID of the treated entity (in this case, the state of California has an ID of 3). synth cigsale beer lnincome retprice age15to24 cigsale(1988) /// cigsale(1980) cigsale(1975), trunit(3) trperiod(1989) fig /// nested keep(synth_results_data.dta) replace /*Keeping the synth_results_data.dta stores a dataset of all the time series values of cigsale for each year for California (observed) and synthetic California (constructed using a weighted average of observed data from donor states). We can then import this dataset to create a synth plot whose attributes we can control. */ use synth_results_data.dta, clear drop _Co_Number _W_Weight // Drops the columns of the data that store the donor state weights twoway line (_Y_treated _Y_synthetic _time), scheme(plottig) xline(1989) /// xtitle(Year) ytitle(Cigarette Sales) legend(pos(6) rows(1)) ** Run the analysis using synth_runner *Import Dataset sysuse synth_smoking.dta, clear *Need to set the data as time series, using tsset tsset state year *Estimate Synthetic Control using synth_runner synth_runner cigsale beer(1984(1)1988) lnincome(1972(1)1988) retprice age15to24 cigsale(1988) cigsale(1980) /// cigsale(1975), trunit(3) trperiod(1989) gen_vars . We can plot the effects in two ways: displaying both the treated and synthetic time series together and displaying the difference between the two over the time series. The first plot is equivalent to the plot produced by specifying the fig option for synth, except you can control aspects of the figure. For both plots you can control the plot appearence by specifying effect_options() or tc_options(), depending on which plot you would like to control. effect_graphs, trlinediff(-1) effect_gname(cigsale1_effect) tc_gname(cigsale1_tc) /// effect_options(scheme(plottig)) tc_options(scheme(plottig)) /*Graph the outcome paths of all units and (if there is only one treated unit) a second graph that shows prediction differences for all units */ single_treatment_graphs, trlinediff(-1) raw_gname(cigsale1_raw) /// effects_gname(cigsale1_effects) effects_ylabels(-30(10)30) /// effects_ymax(35) effects_ymin(-35) . ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html#stata",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html#stata"
  },"731": {
    "doc": "Synthetic Control",
    "title": "Synthetic Control",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/synthetic_control_method.html",
    
    "relUrl": "/Model_Estimation/Research_Design/synthetic_control_method.html"
  },"732": {
    "doc": "Task Scheduling with Github Actions",
    "title": "The Problem We’ll Solve",
    "content": "The United States Substance Abuse and Mental Health Services Administration (SAMHSA) is an agency inside the U.S. Department of Health and Human Services tasked with overseeing the country’s substance abuse and mental health initiatives. A major one of these initiatives is maintaining the list of “waived providers” who can prescribe opioids, something that is typically prohibited under the federal Controlled Substances Act. SAMHSA makes available a list of currently waived providers, but does not publish (at least easily) historical lists of providers. As such, we’ll write a small web scraper that pulls all the data from their website and writes it out to a CSV. This article, however, is not about web scrapers. Instead, our problem is that SAMHSA seems to update the list without fanfare at irregular intervals. So we would like to scrape their website every day. This article demonstrates how set up a Github repo to do just that. ",
    "url": "/Other/task_scheduling_with_github_actions.html#the-problem-well-solve",
    
    "relUrl": "/Other/task_scheduling_with_github_actions.html#the-problem-well-solve"
  },"733": {
    "doc": "Task Scheduling with Github Actions",
    "title": "Requirements",
    "content": "You’ll need: . | A Github account and some familiarity with git | A program that can be run on the command line that accomplishes your data gathering task | The requirements for that program enumerated in one of several standard ways | . For the rest of this section, we’ll focus a bit on requirements (2) and (3). Requirement (2): A command line program . What you’ll be able to tell Github to do is run a series of commands. It is best to package these up into one command that will do everything for you. For instance, if you’re using python, you will probably want to have a file called main.py that looks something like this: . import csv import sys from datetime import datetime from typing import List, Union import requests URL = \"https://whereveryourdatais.com/\" def process_page(html: str) -&gt; List[List[Union[int, str]]]: \"\"\" This is the meat of your web scraper: Pulling out the data you want from the HTML of the web page \"\"\" def pull_data(url: str) -&gt; List[List[Union[int, str]]]: resp = requests.get(url) resp.raise_for_status() content = resp.content.decode('utf8') return process_page(content) def main(): # The program takes 1 optional argument: an output filename. If not present, # we will write the output a default filename, which is: filename = f\"data/output-{datetime.utcnow().strftime('%Y-%m-%d').csv\" if len(sys.argv) &gt; 1: filename = sys.argv[1] print(f\"Will write data to {filename}\") print(f\"Pulling data from {URL}...\") data = pull_data(URL) print(f\"Done pulling data.\") print(\"Writing data...\") with open(filename, 'wt') as outfile: writer = csv.writer(outfile) writer.writerows(data) print(\"Done writing data.\") if __name__ == \"__main__\": main() . Here the meat of your web scraper goes into the pull_data and the process_page functions. These are then wrapped into the main function which you can call on the command line as: . python3 main.py . Similarly, if you’re using R, you’ll want to create a main.R file to similar effect. For instance, it might look something like: . library(readr) library(httr) URL &lt;- \"https://whereveryourdatais.com/\" #' This hte meat of your web scraper: #' Pulling out the data you want from the HTML of the web page process_page &lt;- function(html) { # Process html } #' Pull data from a single URL and return a tibble with it nice and ordered pull_data &lt;- function(url) { resp &lt;- GET(url) if (resp$status_code &gt;= 400) { stop(paste0(\"Something bad occurred in trying to pull \", URL)) } return(process_page(content(resp))) } main &lt;- function() { # The program takes 1 optional argument: an output filename. If not present, # we will write the output a default filename, which is: date &lt;- Sys.time() attr(date, \"tzone\") &lt;- \"UTC\" filename &lt;- paste0(\"data/output-\", as.Date(date, format = \"%Y-%m-%d\")) args &lt;- commandArgs(trailingOnly = TRUE) if (length(args) &gt; 0) { filename &lt;- args[1] } print(paste0(\"Will write data to \", filename)) print(paste0(\"Pulling data from \", URL)) data &lt;- pull_data(URL) print(\"Done pulling data\") print(\"Writing data...\") write_csv(data, filename) print(\"Done writing data.\") } . Here the meat of your web scraper goes into the pull_data and the process_page functions. These are then wrapped into the main function which you can call on the command line as (note the --vanilla): . Rscript --vanilla main.R . Requirement (3): Enumerated lists of requirements . In order for Github to run your command, it will need to know what dependencies it needs to install. For experts, using a tool like poetry in Python or renv in R is probably what you actually want to do. However, for the purposes of this article, we’ll stick to a simple list. As such, you should create a file entitled requirements.txt in your project’s main folder. In this you should list, one requirement per line, the requirements of your script. For instance, in the python example above, your requirements.txt should look like . requests . The R example should have . httr readr . If you’re using R, you’ll also need to add the following script in a file called install.R to your project: . CRAN &lt;- \"https://mirror.las.iastate.edu/CRAN/\" process_file &lt;- function(filepath) { con &lt;- file(filepath, \"r\") while (TRUE) { line &lt;- trimws(readLines(con, n = 1)) if (length(line) == 0) { break } install.packages(line, repos = CRAN) } close(con) } process_file(\"requirements.txt\") . ",
    "url": "/Other/task_scheduling_with_github_actions.html#requirements",
    
    "relUrl": "/Other/task_scheduling_with_github_actions.html#requirements"
  },"734": {
    "doc": "Task Scheduling with Github Actions",
    "title": "Setting up the Action",
    "content": "With all of the above accomplished, you should have a main.py or a main.R file and a requirements.txt file setup in your repository. If you’re using R, you’ll also have an install.R script present. With that, we move to setting up the Github Action! . In this section, we assume that your repository is already on Github. Throughout, we’ll assume that the repository is hosted at USERNAME/REPO, e.g., lost-stats/lost-stats.github.io. Telling it to run . Now you just need to add a file called .github/workflows/schedule.yml to your repo. Its contents should look like this: . name: Run scheduled action on: schedule: # You need to set your schedule here - cron: CRON_SCHEDULE jobs: pull_data: runs-on: ubuntu-20.04 steps: - name: Checkout code uses: actions/checkout@v2 with: persist-credentials: false fetch-depth: 0 # If using Python: - name: Set up Python 3.8 uses: actions/setup-python@v2 with: python-version: \"3.8\" # If using R: - name: Set up R 4.0.3 uses: r-lib/actions/setup-r@v1 with: r-version: \"4.0.3\" # If using Python: - name: Install dependencies run: pip install -r requirements.txt # If using R: - name: Install dependencies run: Rscript --vanilla install.R # If using Python: - name: Pull data run: python3 main.py # If using R: - name: Pull data run: Rscript --vanilla main.R # NOTE: This commits everything in the `data` directory. Make sure this matches your needs - name: Git commit run: | git add data git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git commit -m \"Commiting data\" # NOTE: Check that your branch name is correct here - name: Git push run: | git push \"https://${GITHUB_ACTOR}:${TOKEN}@github.com/${GITHUB_REPOSITORY}.git\" HEAD:main env: TOKEN: ${{ secrets.GITHUB_TOKEN }} . You’ll need to edit this file and retain only the stanzas that pertain to whether you’re using Python or R. However, you’ll need to make a few adjustments. Let’s go through the file stanza by stanza to explain what it is doing: . name: Run scheduled action . This is just a descriptive name. Everything after the : is decorative. Name it whatever you like! . on: . This section describes when the action should run. Github actions supports several potential events, including push, pull_request, and repository_dispatch. However, since this is a scheduled action, we’re going to use the schedule event. The next line - cron: CRON_SCHEDULE tells Github how frequently to run the action. You need to replace CRON_SCHEDULE with your preferred frequency. You need to write this in “cron syntax,” which is an arcane but pretty universally recognized format for specifying event schedules. I recommend using a helper like this one to write this expression. For instance, let’s say we want to run this job at noon UTC every day. Then this line should become - cron: \"0 12 * * *\". jobs: . This tells us that we’re about to begin specifying the list of jobs to be run on the schedule described above. pull_data: . This is also just a descriptive name. It is best that it follow snake_casing, in particular, it should have no spaces or strange characters. runs-on: ubuntu-20.04 . This specifies which operating system to run your code on. Github supports a lot of choices, but generally, ubuntu-20.04 or ubuntu-latest is what you’ll want. steps: . In what follows, we list out the individual steps Github should take. Each step consists of several components: . | name: A descriptive name. Can be anything you’d like. It’s also optional, but I find it useful. | uses: Optionally reference an series of steps somebody else has already specified. | with: If using uses:, specificy any variables in calling that action. | run: Just simply run a (series of) commands in the shell, one per line. | env: Specify envrionment variables for use in the shell. | . We’ll see several examples of this below. Checkout code . This stanza tells the action to checkout this repository’s code. This will begin basically every Github action you build. Note that it uses: a standard action that is maintained by Github itself. Setup Python or R . These are actions that tell Github to make a specific version of Python or R available in your envrionment. You probably only need one, but you can use both if you need. Specify the exact version you want in the with: section. Install dependencies . This runs a script that installs all the dependencies you enumerated earlier in requirements.txt. Python comes with a built in dependency manager called pip, so we just point it to our list of dependencies. On the other hand, we tell R to execute our dependency installation script install.R. In either case, we’re using run: as we’re telling Github to execute a command in its own shell. Pull data . This is the task we’re actually going to run! Note that we’re calling either the main.py or main.R file we built before. After this is done, we assume there will be a new file in the data/ directory. Git commit . This stanza commits the new data to this repository and sets up the required git variables. Note that here we’re using run: |. In YAML, ending a line with | indicates that all the following lines that are at the same tab depth should be used as a single value. So here, we’re telling Github to run the commands, git add data, git config --local user.email \"action@github.com\", etc in order. Git push . This pushes the commit back up to the repository using git push. Note that if the name of your main branch is not main (for instance, it may be master), you will need to change HEAD:main to whatever your main branch is called (e.g., HEAD:master). Also note that we are setting an environment variable here. Specfically, in the env: section we’re setting the TOKEN environment variable to ${{ secrets.GITHUB_TOKEN }}. This is a a special value that Github generates for each run of your action that allows your action to manipulate its own repository. In this case, it’s allowing it to push a commit back to the central repository. ",
    "url": "/Other/task_scheduling_with_github_actions.html#setting-up-the-action",
    
    "relUrl": "/Other/task_scheduling_with_github_actions.html#setting-up-the-action"
  },"735": {
    "doc": "Task Scheduling with Github Actions",
    "title": "And that’s all!",
    "content": "And that’s it! With that file commited, you Github action should run every day at noon UTC. From here, there are a lot of simple extensions to be made and tried. Here are some challenges to make sure you know what’s going on above: . | Instead making the job run every day at noon UTC, make it run on Wednesdays at 4pm UTC. | Instead of returning at tibble, return a data.frame in R. Note that you’ll need to expand the collection of requirements! | Instead of returning a list of lists in Python, return a pandas data frame. Note that you’ll need to expand the collection of requirements! | . ",
    "url": "/Other/task_scheduling_with_github_actions.html#and-thats-all",
    
    "relUrl": "/Other/task_scheduling_with_github_actions.html#and-thats-all"
  },"736": {
    "doc": "Task Scheduling with Github Actions",
    "title": "One final note: API keys",
    "content": "A very common need to pull data is some sort of API key. Your cron job will need access to your API key. Conveniently, Github has provided a nice functionality to do exactly this: Secrets. To get your API key to your script, follow these steps: . | Setup your secret according to the above instructions. Let’s give it the name API_KEY for convenience. | Modify your main.py or main.R file to look for the API_KEY environemnt variable. For instance, in Python you might do: | . import os api_key = os.environ.get(\"API_KEY\", \"some_other_way\") . or in R you might do . api_key &lt;- Sys.getenv(\"API_KEY\", unset = \"some_other_way\") . | Amend the Pull data step in your action to set the API_KEY environment variable. For instance, it might look like: | . - name: Pull data run: python3 main.py env: API_KEY: ${{ secrets.API_KEY }} . ",
    "url": "/Other/task_scheduling_with_github_actions.html#one-final-note-api-keys",
    
    "relUrl": "/Other/task_scheduling_with_github_actions.html#one-final-note-api-keys"
  },"737": {
    "doc": "Task Scheduling with Github Actions",
    "title": "Task Scheduling with Github Actions",
    "content": "Typically when performing statistical analyses, we write code to be run approximately once. But software more generally is frequently run multiple times. Web servers run constantly, executing the same code over and over in response to user commands. A video game is rerun on demand, each time you turn it on. In statistical analyses, though, if code is to be run multiple times, it often needs to be run on a schedule. For instance, you may want to scrape weather data every hour to build an archive for later analysis. Or perhaps you want to perform the same statistical analyses each week on new data as it comes in. In our experience, this is the worst kind of tasks for humans to do: They have to reliably remember to run a piece of code at a specified time, aggregate the results in a consistent format, and then walk away. One mistimed meeting or baby feeding and it’s likely the reseaercher will forget to hit “go.” . Thankfully, in addition to doing things over and over or on demand, computers are also reasonably good at keeping time. In this article, we’ll describe the role of a task scheduler and demonstrate how to use Github Actions to run a simple data gathering task at regular intervals and commit that data to a repository. ",
    "url": "/Other/task_scheduling_with_github_actions.html",
    
    "relUrl": "/Other/task_scheduling_with_github_actions.html"
  },"738": {
    "doc": "Tobit Regression",
    "title": "Tobit Regression",
    "content": "If you have ever encountered data that is censored in some way, then the Tobit method is worth a detailed look. Perhaps the measurement tools only detect at a minimum threashold or up until some maximum threshold, or there’s a physical limitation or natural constraint that cuts off the range of outcomes. If the dependent variable has a limited range in any way, then an OLS regression will capture the relationship between variables with a cluster of zeros or maximums distorting the relationship. James Tobin’s big idea was essentially to modify the likelihood function to represent the unequal sampling probability of observations depending if a latent dependent variable is smaller than or larger than that range. The Tobit model is also called a Censored Regression Model for this reason, as it allows flexility to account of either left or right side censorship. There is flexibility in the mathematics depending on how the censoring occurs. To learn more to match the mathematics/functional form to your practical application, wikipedia has a great page here along with links to outside practical applications. Para Español, dale click en el siguiente enlance aqui. Estas notas tienen las lecciones importantes de esta pagina en Ingles. ",
    "url": "/Model_Estimation/GLS/tobit.html",
    
    "relUrl": "/Model_Estimation/GLS/tobit.html"
  },"739": {
    "doc": "Tobit Regression",
    "title": "Keep in Mind",
    "content": ". | Tobit is used with Censored Data, which IS NOT the same as Truncated Data (see next section) | Tobit can produce a kinked relationship after a zero cluster | Tobit can find the correct relationship underneath a maximum cluster | For non-parametric tobit, you will need a CLAD operator (see link in next section) | . ",
    "url": "/Model_Estimation/GLS/tobit.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/GLS/tobit.html#keep-in-mind"
  },"740": {
    "doc": "Tobit Regression",
    "title": "Also Consider",
    "content": ". | If you are new to the concept of limited dependent variables or OLS Regression, click these links. | Deciphering whether data is censored or truncated is important. If all observations are observed in “X” but the true value of “Y” isn’t known outside some range, then it is Censored. At the Chernobyl disaster the radioactive isotope meter only read up until a maximum threshold, all areas (“X”) are observed but the true value of the radioactive level (“Y”) is right censored at a maximum. When there is not a full set of “X” observed, then data is truncated, or in other words, a censored Y value does not get it’s input x observed thus the set {Y,X} is not complete. For more info try these UH slides from Bauer School of Business (they also have relatively easily digestable theory). | The Tobit model type I (the main one people are talking about without specification) is really a morphed maximum likelihood estimation of a probit, more background from those links. | If you find yourself needing non-parametric form, you will need to use a CLAD operator as well as new variance estimation techniques, I recommend Bruce Hansen’s from University of Wisconsin, notes here. | . ",
    "url": "/Model_Estimation/GLS/tobit.html#also-consider",
    
    "relUrl": "/Model_Estimation/GLS/tobit.html#also-consider"
  },"741": {
    "doc": "Tobit Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/tobit.html#implementations",
    
    "relUrl": "/Model_Estimation/GLS/tobit.html#implementations"
  },"742": {
    "doc": "Tobit Regression",
    "title": "R",
    "content": "We can use the AER package (link) to run a tobit model in R. # install.packages(\"AER\") # Install first if you don't have it yet library(AER) data(\"Affairs\") # Use the \"Affairs\" dataset provided with AER # Aside: this example replicates Table 22.4 in Greene (2003) tob_mod1 = tobit(affairs ~ age + yearsmarried + religiousness + occupation + rating, data = Affairs) summary(tob_mod1) # The default left- and right-hand side limts for the censored dependent variable # are 0 and Inf, respectively. You might want to change these after inspecting your # data. hist(Affairs$affairs tob_mod2 = tobit(affairs ~ age + yearsmarried + religiousness + occupation + rating, data = Affairs, right = 4) # RHS censored now at 4 summary(tob_mod2) . For another example check out M Clark’s Models by Example Page. ",
    "url": "/Model_Estimation/GLS/tobit.html#r",
    
    "relUrl": "/Model_Estimation/GLS/tobit.html#r"
  },"743": {
    "doc": "2x2 Difference in Difference",
    "title": "2X2 Difference-in-Differences",
    "content": "Causal inference with cross-sectional data is fundamentally tricky. | People, firms, etc. are different from one another in lots of ways. | Can only get a clean comparison when you have a (quasi-)experimental setup, such as an experiment or an regression discontinuity. | . Difference-in-difference makes use of a treatment that was applied to one group at a given time but not another group. It compares how each of those groups changed over time (comparing them to themselves to eliminate between-group differences) and then compares the treatment group difference to the control group difference (both of which contain the same time gaps, eliminating differences over time). ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#2x2-difference-in-differences",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#2x2-difference-in-differences"
  },"744": {
    "doc": "2x2 Difference in Difference",
    "title": "KEEP IN MIND",
    "content": ". | For Difference-in-differences to work, parallel trends must hold. That is, nothing else should be changing the gap between treated and control states at the same time as the treatment. While it is not a formal test of parallel trends, researchers often look at whether the gap between treated and control states is constant in pre-treatment years. | Suppose in \\(t = 0\\) (“Pre-period”), and \\(t = 1\\) (“Post-period”). We want to estimate \\(\\tau = Post - Pre\\), or \\(Y(post)-Y(pre)= Y(t=1)-Y(t=0)=\\tau\\). | . ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#keep-in-mind",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#keep-in-mind"
  },"745": {
    "doc": "2x2 Difference in Difference",
    "title": "ALSO CONSIDER",
    "content": ". | This page discusses “2x2” difference-in-difference design, meaning there are two groups, and treatment occurs at a single point in time. Many difference-in-difference applications instead use many groups, and treatments that are implemented at different times (a “rollout” design). Traditionally these models have been estimated using fixed effects for group and time period, i.e. “two-way” fixed effects. However, this approach with difference-in-difference can heavily bias results if treatment effects differ across groups, and alternate estimators are preferred. See Goodman-Bacon 2018 and Callaway and Sant’anna 2019. | . ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#also-consider",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#also-consider"
  },"746": {
    "doc": "2x2 Difference in Difference",
    "title": "IMPLEMENTATIONS",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#implementations",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#implementations"
  },"747": {
    "doc": "2x2 Difference in Difference",
    "title": "Python",
    "content": "# Step 1: Load libraries and import data import pandas as pd import statsmodels.api as sm # for certain versions of jupyter: # %matplotlib inline url = ( \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS\" \".github.io/master/Model_Estimation/Data/\" \"Two_by_Two_Difference_in_Difference/did_crime.xlsx\" ) df = pd.read_excel(url) # Step 2: indicator variables # whether treatment has occured at all df['after'] = df['year'] &gt;= 2014 # whether it has occurred to this entity df['treatafter'] = df['after'] * df['treat'] # Step 3: # use pandas basic built in plot functionality to get a visual # perspective of our parallel trends assumption ax = df.pivot(index='year', columns='treat', values='murder').plot( figsize=(20, 10), marker='.', markersize=20, title='Murder and Time', xlabel='Year', ylabel='Murder Rate', # to make sure each year is displayed on axis xticks=df['year'].drop_duplicates().sort_values().astype('int') ) # the function returns a matplotlib.pyplot.Axes object # we can use this axis to add additional decoration to our plot ax.axvline(x=2014, color='gray', linestyle='--') # treatment year ax.legend(loc='upper left', title='treat', prop={'size': 20}) # move and label legend # Step 4: # statsmodels has two separate APIs # the original API is more complete both in terms of functionality and documentation X = sm.add_constant(df[['treat', 'treatafter', 'after']].astype('float')) y = df['murder'] sm_fit = sm.OLS(y, X).fit() # the formula API is more familiar for R users # it can be accessed through an alternate constructor bound to each model class smff_fit = sm.OLS.from_formula('murder ~ 1 + treat + treatafter + after', data=df).fit() # it can also be accessed through a separate namespace import statsmodels.formula.api as smf smf_fit = smf.ols('murder ~ 1 + treat + treatafter + after', data=df).fit() # if using jupyter, rich output is displayed without the print function # we should see three identical outputs print(sm_fit.summary()) print(smff_fit.summary()) print(smf_fit.summary()) . ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#python",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#python"
  },"748": {
    "doc": "2x2 Difference in Difference",
    "title": "R",
    "content": "In this case, we need to discover whether legalized marijuana could change the murder rate. Some states legalized marijuana in 2014. So we measure the how the murder rate changes from before 2014 to after between legalized states and states without legalization. Step 1: . | First of all, we need to load Data and Package, we call this data set “DiD”. | . library(tidyverse) library(broom) library(readxl) library(httr) # Download the Excel file from a URL tf &lt;- tempfile(fileext = \".xlsx\") GET( \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Two_by_Two_Difference_in_Difference/did_crime.xlsx\", write_disk(tf) ) DiD &lt;- read_excel(tf) . Step 2: . Notice that the data has already been collapsed to the treated-year level. That is, there is one observation of the murder rate for each year for the treated states (all averaged together), and one observation of the murder rate for each year for the untreated states (all averaged together). We create the indicator variable called after to indicate whether it is in the treated period of being after the year of 2014 (1), or the before period of between 2000-2013 (0). The variable treat indicates that the state legalizes marijuana in 2014. Notice that treat = 1 in these states even before 2014. If the year is after 2014 and the state decided to legalize marijuana, the indicator variable “treatafter” is “1” . DiD &lt;- DiD %&gt;% mutate(after = year &gt;= 2014) %&gt;% mutate(treatafter = after*treat) . Step 3: . Then we need to plot the graph to visualize the impact of legalize marijuana on murder rate by using ggplot. mt &lt;- ggplot(DiD,aes(x=year, y=murder, color = treat)) + geom_point(size=3)+geom_line() + geom_vline(xintercept=2014,lty=4) + labs(title=\"Murder and Time\", x=\"Year\", y=\"Murder Rate\") mt . It looks like, before the legalization occurred, murder rates in treated and untreated states were very similar, lending plausibility to the parallel trends assumption. Step 4: . We need to measure the impact of impact of legalize marijuana. If we include treat, after, and treatafter in a regression, the coefficient on treatafter can be interpreted as “how much bigger was the before-after difference for the treated group?” which is the DiD estimate. reg&lt;-lm(murder ~ treat+treatafter+after, data = DiD) summary(reg) . After legalization, the murder rate dropped by 0.3% more in treated than untreated states, suggesting that legalization reduced the murder rate. ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#r",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#r"
  },"749": {
    "doc": "2x2 Difference in Difference",
    "title": "2x2 Difference in Difference",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html",
    
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html"
  },"750": {
    "doc": "Home",
    "title": "Home",
    "content": "# Home Welcome to the **Library of Statistical Techniques** (LOST)! LOST is a publicly-editable website with the goal of making it easy to execute statistical techniques in statistical software. Each page of the website contains a statistical technique &mdash; which may be an estimation method, a data manipulation or cleaning method, a method for presenting or visualizing results, or any of the other kinds of things that statistical software typically does. For each of those techniques, the LOST page will contain code for performing that method in a variety of packages and languages. It may also contain information (or links) with thorough descriptions of the method, but the focus here is on implementation. How can you do it in your language of choice? If there are multiple ways, how are those ways different? Is the way you used to do it outdated, or does it do something unexpected? What's the `R` equivalent of that command you know about in `Stata` or `SAS`, or vice versa? In short, LOST is a Rosetta Stone for statistical software. If you are interested in contributing to LOST, please see the [Contributing](https://lost-stats.github.io/Contributing/Contributing.html) page. LOST was originated in 2019 by Nick Huntington-Klein and is maintained by volunteer contributors. The project's GitHub page is [here](https://github.com/LOST-STATS/lost-stats.github.io). ",
    "url": "/",
    
    "relUrl": "/"
  }
}
